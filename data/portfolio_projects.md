# Wage Gap BC Minimum vs. Living Wage 2025

## URL
https://tinyurl.com/yjy6vdfr

## Purpose
Analyzes the economic gap between British Columbia's minimum wage and living wage to highlight the challenge faced by minimum wage workers. This project uses strategic data visualization to communicate a critical social and economic issue affecting workers in BC.

## Tech Stack
Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook

## Technical Highlights
Implements a slope chart visualization technique—a sophisticated data storytelling approach that effectively communicates the magnitude of wage disparity between two data points. The project demonstrates advanced matplotlib customization including custom axis formatters, multi-layer point plotting, strategic color palette selection, and high-resolution image export at 300 DPI. The visualization employs professional design principles such as visual hierarchy through point sizing, color differentiation to distinguish wage types, and supporting reference lines to enhance comparative analysis. Shows expertise in data manipulation using Pandas and NumPy for structured data preparation, combined with production-ready code practices that balance aesthetic appeal with analytical clarity.

## Skills Demonstrated
Advanced data visualization design and narrative communication, with proficiency in translating numerical data into compelling visual stories. Demonstrates expertise in choosing appropriate visualization types for specific analytical goals, implementing professional chart styling, and optimizing visual design for clarity and impact. Strong capability in creating data-driven narratives that communicate complex economic information to diverse audiences while maintaining analytical rigor.

## Result/Impact
Quantified the wage gap at $9.20 CAD per hour, representing a 51% shortfall where living wage exceeds minimum wage. Generated a professional, publication-quality slope chart visualization that effectively illustrates economic disparity and serves as a powerful communication tool for stakeholder engagement and data-driven discourse.

# Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales

## URL
https://tinyurl.com/2wxtb6af

## Purpose
This project analyzes grain production trends in Canada using government statistics data, demonstrating expertise in data visualization design principles. It compares different visualization approaches—grouped bar charts versus bubble charts—to determine the most effective way to communicate complex, multi-dimensional agricultural data.

## Tech Stack
Python, Pandas, Plotly, Matplotlib, NumPy

## Technical Highlights
The project implements a sophisticated data transformation pipeline that processes raw CSV data with custom cleaning logic, including comma removal, numeric type conversion, and categorical renaming to prepare data for analysis. It systematically evaluates multiple visualization strategies: grouped bar charts are tested for their ability to compare categories within groups, but are shown to become cluttered with many categories; bubble charts are then introduced as an alternative where size adds a visual dimension to enhance magnitude differentiation. The implementation further demonstrates advanced color theory application by comparing diverging color scales (RdBu, balance) against sequential scales (Reds) on identical bubble chart datasets, revealing how red-to-blue transitions create cognitive friction due to counterintuitive color-value associations, while red-based sequential scales combined with bubble size provide faster visual comprehension. Each visualization is professionally styled with consistent grid lines and optimized layouts to isolate design variable impacts.

## Skills Demonstrated
Exceptional expertise in data visualization design, color theory, and human-centered information design that directly impacts audience comprehension. Demonstrates analytical rigor by conducting comparative studies on visualization effectiveness and the ability to translate design principles into technical implementation for maximum audience impact.

## Result/Impact
The project provides empirical, visual evidence for selecting appropriate chart types and color palettes based on data complexity and cognitive load considerations. This comparative analysis framework enables more informed design decisions that measurably improve data comprehension and decision-making across technical and non-technical audiences.

# AI Model Performance Visualization: Grouped Bar vs. Bubble Charts

## URL
https://tinyurl.com/4xktcfsm

## Purpose
This project evaluates different charting approaches for communicating multi-dimensional AI model performance data. It compares grouped bar charts, and bubble charts to demonstrate how appropriate chart selection significantly improves the clarity and accessibility of model-to-model and benchmark-to-benchmark performance comparisons.

## Tech Stack
Python, Pandas, Plotly, Matplotlib

## Technical Highlights
The project implements a comparative analysis of three visualization approaches to handle three-dimensional benchmark data (models, benchmarks, and performance scores). Grouped bar charts segment models side-by-side within each benchmark but struggle with visual clutter and comparison difficulty across categories. Grouped bar charts compound this problem by making cross-category comparisons difficult and misrepresenting relative performance values. In contrast, bubble charts leverage color gradients and marker sizing to encode all three dimensions simultaneously, enabling direct visual comparison of model performance across benchmarks without cognitive overhead. The implementation applies advanced Plotly configurations including sequential color scales (RdBu_r and RdYlBu_r), interactive hover templates with contextual details, and heatmap visualizations that further enhance pattern recognition across benchmark-model matrices.

## Skills Demonstrated
Demonstrates expertise in data visualization design principles, information architecture, and the ability to evaluate trade-offs between different chart typologies. Shows proficiency in selecting visualization techniques based on data structure and analytical requirements, with a focus on reducing cognitive load and improving stakeholder comprehension of complex comparative datasets.

## Result/Impact
Establishes bubble charts as the superior visualization method for multi-dimensional benchmark analysis, enabling rapid identification of model performance patterns and competitive advantages without requiring multiple chart interpretations. The visual clarity of encoded bubble size and color produces more actionable insights than traditional grouped bar approaches.

# CERN Career Preferences Analysis: A Data Visualization

## URL
https://tinyurl.com/hnexp2r8

## Purpose
This project transforms observational data from a CERN interactive activity into a visual analysis of visitor career preferences at Europe's largest research organization. The visualization reveals which scientific and engineering roles attracted the most interest among facility visitors.

## Tech Stack
Python, Pandas, Seaborn, Matplotlib

## Technical Highlights
The project demonstrates proficiency in data transformation and visualization with pandas for dataset organization and sorting. Custom styling techniques are applied through matplotlib and seaborn to create a publication-quality visualization, including custom color palettes, grid configuration, and typography management. The code implements responsive figure sizing with DPI adjustment and systematic removal of chart clutter through spine and border removal for improved data-ink ratio. Data is presented as a horizontally-oriented bar chart that prioritizes readability for categorical comparisons.

## Skills Demonstrated
The project showcases data cleaning and preparation capabilities, along with expertise in creating professional data visualizations for communication. It demonstrates attention to detail in visual design, including color theory application and accessibility-focused chart design conventions.

## Result/Impact
The visualization successfully aggregated and presented survey responses from 12 career preference categories, with particle physics research emerging as the most popular choice among respondents. This provides clear insights into visitor interests at a major scientific institution.

# Alternative Protein Industry Market Analysis and Segmentation

## URL
https://tinyurl.com/44ehkutz

## Purpose
Analyzes the alternative protein company database from the Good Food Institute to understand the current state of the alternative protein market, focusing on industry trends, market segments, and regional distribution. This analysis reveals key insights into why the industry experienced a 21% sales decrease in 2023 despite earlier growth signals.

## Tech Stack
Python, Pandas, NumPy, Scikit-Learn, Seaborn, Matplotlib, Plotly, GeoPandas, KModes, Data Storytelling

## Technical Highlights
Implemented comprehensive exploratory data analysis including automated profiling and multi-step data transformation pipelines with column denormalization and explosion of nested categorical values. Applied unsupervised machine learning using K-Modes clustering to identify three distinct market segments based on protein technology, industry focus, and geographic region. Developed time-series linear regression analysis across three distinct growth periods (1970-2014, 2015-2020, 2021-2023) to quantify changes in company formation rates and market momentum. Created interactive geospatial visualizations using Plotly and GeoPandas to map regional investment patterns and state-level distribution across the United States.

## Skills Demonstrated
Proficiency in exploratory data analysis, statistical modeling, and unsupervised machine learning for market segmentation and business intelligence. Strong capabilities in data cleaning, normalization, and transformation for nested and categorical data structures, combined with advanced data visualization techniques for communicating complex insights to stakeholders.

## Result/Impact
Linear regression analysis revealed that alternative protein company formation peaked at 23 new companies per year during 2015-2020, but declined to a net loss of 76 companies annually between 2021-2023. Geographic analysis identified the USA and California as the epicenter of the industry, while segmentation analysis uncovered three distinct market segments: plant-based meat substitutes in Asia Pacific, biomass fermentation dairy alternatives in North America, and plant-based dairy alternatives in Europe.

# Air Quality Index ETL to ML Prediction Pipeline

## URL
https://tinyurl.com/3xdwhxes

## Purpose
This project develops an end-to-end ETL pipeline that automatically collects air quality measurements from the OpenAQ API and applies time-series forecasting to predict the Air Quality Index. The solution addresses the need for reliable air quality predictions by combining data engineering with statistical machine learning techniques.

## Tech Stack
Python, Ploomber, DuckDB, MotherDuck, OpenAQ API, ARIMA, Pandas, Streamlit, Statsmodels, Jupyter Notebooks, Docker, GitHub Actions

## Technical Highlights
The project implements a production-grade data pipeline with automated hourly data ingestion via GitHub Actions, storing air quality measurements from real-world sensors in a cloud-managed data warehouse. The core machine learning component uses ARIMA (AutoRegressive Integrated Moving Average) for time-series forecasting, which performs stationarity testing and differencing to capture temporal dependencies in seasonal air quality patterns. The system integrates multiple layers: data extraction with error handling and validation, transformation with timezone conversion and outlier filtering, cloud data persistence via MotherDuck, and a user-facing Streamlit application deployed on Ploomber Cloud that enables interactive predictions with AQI classification bucketing (Good, Satisfactory, Moderate, Poor, Very Poor, Severe).

## Skills Demonstrated
Demonstrates expertise in orchestrating complex ETL workflows using modern workflow management tools, API integration with robust error handling, and time-series analysis with statistical methods. Additionally showcases cloud database architecture, containerization for deployment, and full-stack data product development from pipeline to production-ready web interface.

## Result/Impact
The system successfully processes real-time air quality data with automated scheduling, enabling stakeholders to access predictive AQI forecasts through an interactive web application. The ARIMA model captures temporal patterns in multi-parameter sensor data for accurate short-term air quality predictions.

# NOC Wage Finder: Interactive NOC-Based Salary Explorer

## URL
https://tinyurl.com/4yu9pep9

## Purpose
Provides Canadian job seekers, newcomers, and international students with reliable, province-specific wage data for occupations classified under the National Occupational Classification system. The application enables informed career and relocation decisions by consolidating wage information from multiple government sources into an accessible, searchable interface.

## Tech Stack
Python, Pandas, Streamlit, Plotly, Selenium, BeautifulSoup, Numpy, String Matching (difflib)

## Technical Highlights
The project implements a multi-stage data pipeline that combines web scraping with sophisticated data reconciliation techniques. Selenium and BeautifulSoup are used to extract NOC codes and occupational classifications from government websites, processing paginated tables across 60+ pages. A custom string matching algorithm using Python's difflib library identifies equivalent job titles across NOC 2016 and 2021 classification systems, resolving discrepancies caused by nomenclature changes with configurable similarity thresholds. The data integration merges wage statistics from multiple government surveys (Census, Labour Force Survey, Employment Insurance) with NOC codes, transforming the consolidated dataset into geographic visualizations using Plotly's Choropleth maps within a Streamlit web application.

## Skills Demonstrated
Demonstrated expertise in full-stack data engineering including web scraping automation, data cleaning and validation, exploratory data analysis, and relational data joining strategies. Proficient in building interactive data applications with responsive user interfaces, geospatial visualization design, and end-to-end data pipeline orchestration from source collection through visualization delivery.

## Result/Impact
Successfully deployed a live web application serving real wage data across all Canadian provinces and occupations, transforming disparate government data sources into a unified, searchable platform. The application provides accessible labor market intelligence to support career planning decisions for international audiences.

# Optimizing Ad Campaign: 62% Reduction in Ad Spend

## URL
https://tinyurl.com/ms8vv7my

## Purpose
This project analyzes the return on ad spend (ROAS) performance across an advertising campaign to identify underperforming keywords and revenue optimization opportunities. The analysis provides data-driven recommendations to eliminate low-ROI keywords and improve overall campaign profitability.

## Tech Stack
Python, Pandas, NumPy, Seaborn, Jupyter Notebook, Power BI

## Technical Highlights
The analysis implements comprehensive data preprocessing, cleaning missing values and standardizing data formats from raw campaign data containing multiple keyword performance attributes. Advanced filtering and statistical analysis segments keywords by ROAS thresholds to isolate high-performing versus underperforming groups, with median-based stratification identifying top-tier performers. Word frequency analysis uses tokenization and value counting on high-performing keywords to identify common word patterns, generating actionable keyword composition recommendations. The solution provides both statistical summaries and interactive Power BI visualizations for business stakeholder communication.

## Skills Demonstrated
Data cleaning and transformation, exploratory data analysis, statistical analysis and filtering, feature engineering through text processing, and business intelligence visualization. Demonstrates ability to translate raw data into actionable business insights with clear ROI impact.

## Result/Impact
Identified $1,623 in annual advertising spend savings opportunity by removing keywords with ROAS below 1 (62% reduction in inefficient spend). Determined that 63% of underperforming keywords are automatically generated, enabling platform optimization recommendations. Provided specific keyword composition patterns (dairy, cheese, cream, free combinations) associated with higher profitability for targeted keyword experimentation.

# Customer Review Sentiment Analysis

## URL
https://tinyurl.com/3baruh6x

## Purpose
This project decodes emotional opinions in customer reviews using advanced natural language processing techniques, enabling businesses to refine marketing strategies, optimize product offerings, and identify emerging market trends. By automatically classifying sentiment across large volumes of customer feedback, it provides actionable intelligence for competitive positioning in consumer goods industries.

## Tech Stack
Python, Streamlit, Transformers, PyTorch, NLTK, Pandas, NumPy, Matplotlib, Seaborn, Scipy

## Technical Highlights
The project implements a comparative analysis of two distinct sentiment analysis approaches: a traditional VADER (Valence Aware Dictionary and Sentiment Reasoner) model using bag-of-words methodology, and a state-of-the-art RoBERTa transformer-based model that captures contextual relationships between words. The RoBERTa implementation leverages the HuggingFace ecosystem to tokenize input text, generate embeddings, and compute probability distributions across sentiment classes. The analysis was conducted on a substantial dataset of approximately 500,000 Amazon food reviews, with comprehensive exploratory data analysis visualizations comparing model performance across different rating scales to validate prediction accuracy. The results demonstrate that the transformer-based approach provides superior performance over the lexicon-based approach.

## Skills Demonstrated
Full-stack NLP pipeline development encompassing data preprocessing, exploratory data analysis, and model comparison frameworks. Proficiency in deploying interactive machine learning solutions using modern Python web frameworks and demonstrating expertise in leveraging pre-trained transformer models for production-grade applications.

## Result/Impact
The comparative analysis demonstrated that the RoBERTa model outperformed the VADER baseline in sentiment classification accuracy across review categories. The deployed web application enables real-time sentiment analysis on user-provided text, transforming raw customer feedback into structured emotional intelligence that directly supports business decision-making in product development and marketing optimization.

# Food Safety Analytics: Environmental Monitoring Program for Food Safety Compliance

## URL
https://tinyurl.com/52yv69e3

## Purpose
Developed a comprehensive environmental monitoring program (EMP) to record, track, and analyze microbiological swabs across a 45,000 sq. ft. food manufacturing facility. The system ensures validation of food safety programs against pathogens while maintaining full traceability and supporting GFSI certifications (SQF, FSSC 22000, HACCP).

## Tech Stack
Microsoft Excel, Microsoft Forms, Microsoft SharePoint, Pivot Tables

## Technical Highlights
Built a multi-layered data architecture with automated data collection via Microsoft Forms that flows directly into Excel, eliminating manual entry errors and ensuring real-time traceability. Implemented a sophisticated concatenation algorithm that generates standardized sample nomenclature based on swab type, location, date, and zone identifiers—critical for tracking samples across 261+ monthly swabs. Engineered a statistical binning approach for CFU/swab analysis, grouping results into 100 CFU/swab ranges to handle high variability in pathogenic indicator organisms (aerobic count, yeast, mold). Leveraged pivot tables and pivot charts to transform raw microbiological data into actionable insights across multiple production zones.

## Skills Demonstrated
Demonstrates expertise in designing end-to-end data systems for quality assurance, including data collection automation, standardization protocols, and advanced statistical analysis techniques for microbiological data. Shows proficiency in data cleaning, normalization, and analysis of complex food safety parameters with high variability, combined with knowledge of regulatory compliance requirements for food manufacturing.

## Result/Impact
Revealed that sanitation practices improved through the monitoring period with decreasing indicator organism counts in subsequent months. Identified that Zone 4 maintained E. coli and total coliform counts below detectable limits, validating effective hand washing practices as part of Good Manufacturing Practices. Provided data-driven evidence showing Listeria-positive results concentrated in drain locations, enabling targeted sanitation procedure optimization.

# Google Diet Search Trends 2020 - Canada Geographic Analytics Dashboard

## URL
https://tinyurl.com/mrxvv8un

## Purpose
This project analyzes the most searched diet trends across Canadian regions in 2020 using Google Trends data, transforming raw search interest patterns into interactive visualizations. The dashboard provides insights into consumer interest in various diet types including Sirtfood, GERD, Candida, Keto, Plant-based, Anti-inflammatory, Fatty-liver, DASH, and Alkaline diets.

## Tech Stack
Tableau

## Technical Highlights
The project aggregates multi-year Google Trends data (2019-2020) across 10 different diet categories, structuring it for both temporal trend analysis and geographic subregion comparisons. The data pipeline processes interest metrics over time and by location, enabling dual-view analytics that reveal seasonal patterns and regional preferences. The interactive Tableau dashboard integrates multiple datasets with filtering capabilities, allowing users to explore diet interest trends dynamically across the Canadian market. The visualization architecture separates time-series analysis from geospatial breakdowns, optimizing query performance and user exploration patterns.

## Skills Demonstrated
Demonstrated expertise in data aggregation and structuring from external sources, creating interactive business intelligence dashboards, and translating raw trend data into actionable consumer insights. Proficiency in designing user-friendly analytics interfaces that balance comprehensiveness with accessibility for stakeholder decision-making.

## Result/Impact
The dashboard enables data-driven identification of emerging diet trends and regional preferences, supporting marketing and business strategy decisions in the health and wellness sector. The interactive visualization framework facilitates rapid exploration of multi-dimensional trend data across ten diet categories and multiple Canadian regions.

# Breakfast Cereals Nutritional Analysis Dashboard

## URL
https://tinyurl.com/52b3kr8t

## Purpose
This interactive dashboard evaluates the nutritional profiles of popular breakfast cereals against Canada's health guidelines, helping consumers make informed dietary choices. The project demonstrates the ability to translate raw nutritional data into actionable business intelligence for health-conscious decision making.

## Tech Stack
Tableau

## Technical Highlights
The dashboard leverages Tableau's advanced visualization capabilities to compare nutritional metrics across multiple cereal brands while referencing evidence-based health standards. The design implements interactive filtering and drill-down functionality, allowing users to explore correlations between nutritional components and health classifications. The workbook architecture demonstrates best practices in data organization, calculated fields, and dashboard layout optimization for clarity and user engagement. The visualization transforms structured nutritional datasets into an intuitive interface that supports rapid insights and comparative analysis.

## Skills Demonstrated
Proficiency in data visualization design, dashboard development, and translating health/nutrition domain knowledge into interactive analytical tools. Strong understanding of user experience design principles to create accessible, professional analytics interfaces that communicate complex nutritional data effectively.

## Result/Impact
This project demonstrates the ability to create production-ready business intelligence solutions that were deployed on Tableau Public, making the analysis accessible to a broad audience. The interactive dashboard enables end-users to independently explore and evaluate nutritional information, extending the impact beyond static reporting.

# Supermarket Sales Dashboard

## URL
https://tinyurl.com/55967ndn

## Purpose
Transforms raw transactional retail data into actionable business intelligence through an interactive visual analytics platform. Enables data-driven decision-making for inventory management, sales analysis, and customer purchasing patterns across product categories.

## Tech Stack
Power BI

## Technical Highlights
The project demonstrates comprehensive data visualization and analytical dashboard design, converting a large-scale transactional dataset (228,267+ invoice line items spanning multiple years) into an intuitive, interactive reporting interface. The dashboard implements multi-dimensional analysis across temporal, product, and customer dimensions with drill-down capabilities. Key analytical components include sales metrics aggregation, quantity and product categorization (dry items vs. chiller items), and temporal trend analysis. The solution showcases effective data modeling and DAX calculations to deliver real-time insights into retail performance metrics and customer behavior patterns.

## Skills Demonstrated
Advanced data visualization design and business intelligence tool expertise with Power BI. Demonstrates proficiency in translating raw transactional data into strategic insights through interactive dashboards, data aggregation, and KPI tracking for retail analytics.

## Result/Impact
Successfully created an interactive analytics platform enabling stakeholders to visualize and analyze comprehensive sales data across multiple dimensions, facilitating faster decision-making and business performance monitoring.

# Government of Canada Food Recalls Data Extraction and Web Scraping Pipeline

## URL
https://tinyurl.com/5n6mfjm3

## Purpose
This project extracts comprehensive food recall data from the Government of Canada's official Recalls and Safety Alerts database (2011-2022), aggregating over 4,600 recall records across multiple years. The initiative addresses the need for structured, analyzable food safety data that enables researchers and analysts to identify patterns, trends, and risks in the Canadian food supply chain.

## Tech Stack
Python, Selenium, Beautiful Soup, Pandas, Jupyter Notebook, Chrome WebDriver

## Technical Highlights
The solution implements a sophisticated two-stage web scraping architecture that handles dynamic HTML content and structural variations across a decade of data. Part I systematically extracts metadata (links, titles, summaries, dates) by paginating through 311 search result pages and parsing HTML elements. Part II demonstrates advanced error handling and schema adaptation by identifying and processing two distinct HTML patterns that emerged from website redesigns between 2021-2022 and 2011-2021, implementing conditional extraction logic with fallback mechanisms to maintain data consistency. The pipeline employs data cleaning techniques including regex-based filtering, datetime conversion, and deduplication, while strategically splitting processing into year-based batches to prevent memory exhaustion when processing thousands of URLs sequentially.

## Skills Demonstrated
Proficiency in web automation and data extraction using Selenium and BeautifulSoup, combined with strong data manipulation and pipeline development using Pandas. Demonstrates expertise in handling real-world data integration challenges such as HTML schema evolution, robust error handling with try-except patterns, and implementing scalable batch processing strategies to manage large datasets.

## Result/Impact
Successfully extracted 4,680 food recall records spanning from 2011 to 2022, creating a comprehensive dataset of 12+ CSV files that consolidate detailed information including company names, recall classifications, affected audiences, and distribution details. This structured dataset provides a foundation for downstream analysis and enables data-driven insights into food safety trends across the Canadian market.

# Food Safety Analytics Dashboard: Visualizing 12 Years of Canadian Regulatory Recalls

## URL
https://tinyurl.com/mf246dkf

## Purpose
This project analyzes over a decade of food safety recalls from the Canadian Food Inspection Agency to identify patterns, trends, and key risk factors in food safety incidents. By processing and visualizing historical recall data, the analysis provides actionable insights into the types of hazards, affected products, and geographic distribution of recalls across Canada. This is the second part of a two-part project following data extraction and web scraping. The first part is **Government of Canada Food Recalls Data Extraction and Web Scraping Pipeline**

## Tech Stack
Python, Power BI, Pandas, NumPy, Regex, Matplotlib, Seaborn, Jupyter Notebook

## Technical Highlights
The project implements advanced data extraction and normalization techniques using custom regex functions to parse unstructured government data and extract structured fields including hazard types, product categories, and distribution patterns. A sophisticated multi-step cleaning pipeline handles data inconsistencies, duplicate values, and misaligned columns through conditional replacement logic and slice-based indexing. The solution includes data denormalization using pandas explode() operations to handle multi-value fields (separated by commas), transforming hierarchical recall classifications into analytical structures. A reusable generalization function applies pattern matching across multiple columns to standardize variations in provincial names, ingredient terminology, and hazard classifications, enabling consistent analysis across 3,893 recalls spanning 2011 to 2022.

## Skills Demonstrated
Demonstrates expertise in data cleaning, transformation, and exploratory data analysis on large unstructured datasets. Proficiency with regex pattern matching, pandas data manipulation, and custom functions for scalable text processing; combined with data visualization and business intelligence design for translating raw data into stakeholder-facing insights.

## Result/Impact
Successfully processed and cleaned 3,893 food safety recalls with standardized classification across hazard types, geographic distribution, and product categories. Generated interactive Power BI dashboards and time-series visualizations revealing trends in food safety incidents, enabling data-driven insights into microbiological contamination, allergen exposures, and extraneous material incidents.

# University World Rankings 2023

## URL
https://tinyurl.com/4uwrz5tk

## Purpose
Analyzes 12+ years of global university ranking data to visualize the dramatic shift in academic excellence, demonstrating China's rapid rise in the international higher education landscape while revealing competitive dynamics among world powers.

## Tech Stack
Python, Pandas, NumPy, Jupyter Notebook, JSON, Postman (web scraping API request prototyping), Flourish

## Technical Highlights
The project demonstrates robust data engineering practices by extracting ranking data from a proprietary API through network analysis and Postman, then normalizing complex nested JSON structures into structured DataFrames using pd.json_normalize(). Advanced text processing was implemented using regex patterns to extract numeric values from mixed string-numeric columns, with sophisticated negative lookahead and lookbehind assertions. The pipeline implements intelligent data filtering—selecting only stable ranking bands (top 200) to avoid ranged values that begin at rank 201—and employs temporal aggregation to group universities by country across multiple years for comparative analysis. The final data transformation pivots the aggregated dataset to an optimal format for visualization generation.

## Skills Demonstrated
Data extraction and API integration through browser developer tools and HTTP clients, proficiency with data cleaning and transformation pipelines, and expertise in handling unstructured JSON and string-based numeric data. Demonstrates strong analytical thinking in data quality assessment by tracking null values before and after cleaning operations.

## Result/Impact
Created an animated visualization using Flourish that reveals China's overtaking of the United States in terms of high-quality academic research volume, providing actionable insights into the shifting global education landscape between 2011 and 2023.

# Market Basket Analysis for Customer Personalization

## URL
https://tinyurl.com/26p7p38y

## Purpose
This project applies association analysis to discover purchasing patterns and product affinities in e-commerce transactions. By identifying which items are frequently bought together, the analysis enables data-driven product recommendations and targeted cross-selling strategies to enhance customer engagement and sales across different geographic markets.

## Tech Stack
Python, Pandas, Seaborn, Matplotlib, MLxtend, Jupyter Notebook

## Technical Highlights
The project implements the Apriori algorithm to generate frequent itemsets from transactional data, establishing support thresholds to identify items purchased together with statistical significance. Data preprocessing includes comprehensive cleaning—removing outliers, handling null values, and standardizing country codes—followed by one-hot encoding of product descriptions to prepare data for association rule mining. The analysis employs multiple filtering metrics including support, confidence, and lift to extract meaningful associations, with country-specific analysis demonstrating how purchasing behaviors vary geographically. Exploratory data analysis uncovers temporal demand patterns, revealing seasonality with peak sales in November and significant demand drops in specific months, which can inform inventory and marketing strategies.

## Skills Demonstrated
Demonstrates expertise in data cleaning and validation, exploratory data analysis, and statistical pattern recognition using market basket analysis algorithms. Shows proficiency in applying machine learning techniques for actionable business insights, including feature engineering and multi-dimensional data aggregation.

## Result/Impact
The analysis identifies distinct purchasing patterns across different countries—such as Dutch customers preferring bundled lunchbox purchases and German customers favoring specific product combinations like plasters—enabling personalized product recommendations tailored by geographic market. These insights support targeted cross-selling strategies and optimized product placement to increase average transaction value.

# Sales Forecasting with ML XGBoost Feature Importance for Retail Optimization

## URL
https://tinyurl.com/38urcn9d

## Purpose
Predict grocery store sales across multiple locations using machine learning, identifying key factors that drive product performance. This project applies gradient boosted trees to understand which product and store characteristics have the strongest influence on sales outcomes.

## Tech Stack
Python, Jupyter Notebook, Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn

## Technical Highlights
Implemented an end-to-end machine learning pipeline encompassing data cleaning, exploratory data analysis, and model development. Addressed missing value challenges by applying mean imputation for continuous variables and mode-based imputation for categorical variables after performing chi-squared statistical testing to ensure independence. Encoded categorical features using label encoding to prepare data for the XGBoost regressor. The project demonstrates understanding of feature importance analysis, model evaluation with multiple metrics (R² and Mean Squared Error), and visualization of prediction performance across different feature dimensions.

## Skills Demonstrated
Data preprocessing and exploratory data analysis with handling of missing data using statistical methods, machine learning model development and evaluation using gradient boosting algorithms, and data visualization techniques to interpret model predictions and feature importance relationships.

## Result/Impact
Achieved R² of 0.86 on training data and identified Item_MRP (list price) and Item_Visibility as the most important predictors of sales, providing actionable insights for retail optimization. Test set R² of 0.52 indicates the model captured meaningful patterns but suggests opportunities for further refinement through hyperparameter optimization and outlier removal.

# Optimizing Credit Card Rewards: Cash-Back vs. BMO Air Miles

## URL
https://tinyurl.com/46vdv4wv

## Purpose
Evaluate whether switching from a cash-back credit card to a BMO Air Miles rewards card yields higher net value given real personal spending patterns, and provide a data-driven recommendation.

## Tech Stack
Python, Pandas, NumPy, Matplotlib, Seaborn, Jupyter Notebook.

## Technical Highlights
Implemented a reproducible pipeline that ingests personal expense data, cleans mixed-format currency and missing values, and standardizes temporal fields for time-based aggregation. Modeled tiered rewards using floor-division logic to accurately reflect how miles are awarded across standard and BMO reward schemes, and computed alternative reward scenarios (standard, BMO, BMO World Elite) including annual-fee adjustments. Aggregated spending by year, merchant (e.g., Safeway), and payment method to isolate miles-earning transactions, and used visualizations and grouped statistics to surface purchase frequency and variance that drive rewards outcomes.

## Skills Demonstrated
Data engineering and cleaning of real-world financial spreadsheets, quantitative reward-modeling and scenario analysis, exploratory data analysis and visualization, and translating analytical results into actionable financial recommendations.

## Result/Impact
The analysis found the BMO Air Miles World Elite card would have yielded approximately $300 in rewards for 2021—about $111 more than the current Scotiabank cash-back card—while 2022 results were mixed (estimated ~$167 vs ~$181), highlighting the importance of category-specific spending and temporal accumulation when choosing rewards products. This evidence-based assessment supports an informed card-switch decision and identifies high-impact spending categories (e.g., grocery purchases at Safeway) to maximize returns.

# Renew Amazon Prime? A Cost-Benefit Analysis

## URL
https://tinyurl.com/wzjb5mfv

## Purpose
Analyze personal Amazon order history to determine whether renewing an Amazon Prime membership is cost-effective after a 20% price increase, using historical spending patterns and modeled shipping costs.

## Tech Stack
Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export).

## Technical Highlights
Cleaned and normalized the raw Amazon order export, including sensitive-data removal, numeric coercion, and datetime parsing. Aggregated orders by order_id and order_date and derived year/month features to enable per-year analysis. Performed EDA with boxplots and histograms to demonstrate distributional skewness and justify median-based interpretation. Implemented a conservative shipping-cost model (assigning $7.99 for orders < $25) and aggregated yearly shipping costs to compare against the annual Prime fee, producing net savings estimates.

## Skills Demonstrated
Data ingestion and cleaning, exploratory data analysis, statistical summary interpretation, scenario modeling and assumptions, and clear visualization for stakeholder decision-making within a reproducible Jupyter workflow.

## Result/Impact
Quantified the financial trade-off: using the modeled conservative shipping rates, the analysis shows net annual savings (example: 2021 would have incurred approximately $108.74 more in shipping without Prime), supporting renewal even after a 20% membership price increase. The notebook provides a repeatable method to reassess the decision annually.

# Creating SQL Database of Save-On-Foods products extracted using Web Scraping API

## URL
https://tinyurl.com/39xdn7p8

## Purpose
This project reverse uses web scraping to programmatically extract product data from the Save-On-Foods website to build a clean, queryable dataset for analysis and downstream tooling. It demonstrates a repeatable ETL workflow to turn paginated JSON API results into analytics-ready CSV and relational data.

## Tech Stack
Python, Jupyter Notebook, Pandas, Requests, Numpy, JSON, SQLAlchemy, SQLite, Postman (web scraping API request prototyping).

## Technical Highlights
The solution identifies the vendor API via browser developer tools, converts the API response into structured data, and implements pagination by incrementing the API skip/take parameters to reliably iterate through results. JSON responses are normalized into tabular form using pandas' json_normalize, followed by targeted cleaning and column transformations to parse price and unit fields. The cleaned dataframe is exported to CSV and persisted into a SQLite relational table with explicit column types via SQLAlchemy for easy querying and integration.

## Skills Demonstrated
Practical skills include API-driven data extraction, ETL design, JSON normalization, data cleaning and transformation, relational schema creation, and reproducible analysis in a notebook environment. It also demonstrates use of tooling for API inspection and request generation (Postman) and basic database engineering for analytics.

## Result/Impact
The pipeline extracted and consolidated 1,519 meat-related product records into a CSV and a SQLite database, enabling efficient SQL queries and downstream analysis. This reproducible workflow reduces manual scraping effort and provides a reliable foundation for price analysis, category insights, and inventory-style analytics.