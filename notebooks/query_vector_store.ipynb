{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099a9b12",
   "metadata": {},
   "source": [
    "# Advanced Vector Store Queries\n",
    "\n",
    "This notebook demonstrates detailed querying of the Qdrant vector store with **triple-collection architecture**:\n",
    "- **`resume_data` collection** (from `resume_ale.md`): work experience, education, skills, continuing studies, personal info\n",
    "- **`personality` collection** (from `personalities_16.md`): personality traits with fixed-size chunking\n",
    "- **`projects` collection** (from `portfolio_projects.md`): portfolio projects with hierarchical chunking\n",
    "\n",
    "**Architecture Benefits:**\n",
    "- Semantic separation of resume facts, personality traits, and portfolio projects\n",
    "- Hierarchical chunking for projects (technical summaries + full content)\n",
    "- Faster queries with focused, smaller collections\n",
    "- No cross-contamination between different data types\n",
    "\n",
    "We'll explore:\n",
    "1. Collection metadata and structure (ALL collections)\n",
    "2. Filtering by section type within collections\n",
    "3. Viewing embeddings and payloads\n",
    "4. Projects collection hierarchical querying (NEW)\n",
    "5. Semantic search examples (per collection)\n",
    "6. Complete RAG workflow with all three collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9a043",
   "metadata": {},
   "source": [
    "## 1. Initialize Vector Store Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105c3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedder initialized for semantic search queries\n",
      "âœ… Connected to Qdrant vector store\n",
      "ğŸ“‚ Storage path: c:\\Users\\Ale\\Documents\\Data-Projects\\GitHub\\Resume_Claude_SDK_Agent\\notebooks\\..\\vector_db\\qdrant_storage\n",
      "\n",
      "ğŸ“¦ Collections:\n",
      "   - resume_data (resume content)\n",
      "   - personality (personality traits)\n",
      "   - projects (portfolio projects)\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI embeddings for semantic search\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.core.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "print(\"âœ… Embedder initialized for semantic search queries\")\n",
    "\n",
    "# Initialize Qdrant client with local storage\n",
    "storage_path = \"../vector_db/qdrant_storage\"\n",
    "client = QdrantClient(path=storage_path)\n",
    "\n",
    "# Collection names (triple-collection architecture)\n",
    "resume_collection = \"resume_data\"\n",
    "personality_collection = \"personality\"\n",
    "projects_collection = \"projects\"\n",
    "\n",
    "print(\"âœ… Connected to Qdrant vector store\")\n",
    "print(f\"ğŸ“‚ Storage path: {Path(storage_path).absolute()}\")\n",
    "print(f\"\\nğŸ“¦ Collections:\")\n",
    "print(f\"   - {resume_collection} (resume content)\")\n",
    "print(f\"   - {personality_collection} (personality traits)\")\n",
    "print(f\"   - {projects_collection} (portfolio projects)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c14602",
   "metadata": {},
   "source": [
    "## 2. Explore Collection Structure (Including Projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6215e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Available Collections:\n",
      "   - resume_data\n",
      "   - personality\n",
      "   - projects\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Collection 'resume_data' Details:\n",
      "   Total documents: 35\n",
      "   Vector dimensions: 1536\n",
      "   Distance metric: Cosine\n",
      "   Status: green\n",
      "\n",
      "   ğŸ“ˆ Documents by Section Type:\n",
      "      continuing_studies  :   7 chunks\n",
      "      education           :   2 chunks\n",
      "      personal_info       :   1 chunks\n",
      "      professional_summary:   1 chunks\n",
      "      skills              :   5 chunks\n",
      "      work_experience     :  19 chunks\n",
      "   ----------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Š Collection 'personality' Details:\n",
      "   Total documents: 8\n",
      "   Vector dimensions: 1536\n",
      "   Distance metric: Cosine\n",
      "   Status: green\n",
      "\n",
      "   ğŸ“ˆ Documents by Section Type:\n",
      "                          :   8 chunks\n",
      "   ----------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Š Collection 'projects' Details:\n",
      "   Total documents: 4\n",
      "   Vector dimensions: 1536\n",
      "   Distance metric: Cosine\n",
      "   Status: green\n",
      "\n",
      "   ğŸ“ˆ Documents by Section Type:\n",
      "      project_full        :   2 chunks\n",
      "      project_technical   :   2 chunks\n",
      "   ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get all collections\n",
    "collections = client.get_collections()\n",
    "print(\"ğŸ“š Available Collections:\")\n",
    "for collection in collections.collections:\n",
    "    print(f\"   - {collection.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Explore ALL collections\n",
    "for collection_name in [resume_collection, personality_collection, projects_collection]:\n",
    "    if client.collection_exists(collection_name):\n",
    "        collection_info = client.get_collection(collection_name)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Collection '{collection_name}' Details:\")\n",
    "        print(f\"   Total documents: {collection_info.points_count}\")\n",
    "        print(f\"   Vector dimensions: {collection_info.config.params.vectors.size}\")\n",
    "        print(f\"   Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "        print(f\"   Status: {collection_info.status}\")\n",
    "        \n",
    "        # Count by section type\n",
    "        from collections import Counter\n",
    "        all_records, _ = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=1000,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        \n",
    "        section_counts = Counter(r.payload.get('section_type', 'unknown') for r in all_records)\n",
    "        \n",
    "        print(f\"\\n   ğŸ“ˆ Documents by Section Type:\")\n",
    "        for section, count in sorted(section_counts.items()):\n",
    "            print(f\"      {section:20s}: {count:3d} chunks\")\n",
    "        \n",
    "        print(\"   \" + \"-\"*76)\n",
    "    else:\n",
    "        print(f\"\\nâŒ Collection '{collection_name}' not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b793ad",
   "metadata": {},
   "source": [
    "## 3. Query Resume Data (from resume_ale.md)\n",
    "\n",
    "### 3.1 View Work Experience with Full Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28af347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¼ Work Experience Chunks from 'resume_data' collection (showing 19):\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1 - ID: 0b2b312a-3c2d-4a6d-a6af-cad1bc2a98bc\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Assurance Technician: Coordinated supply chain and production teams to ensure food safety compliance by leading cross-functional meetings and implementing compliance checks, maintaining operational continuity.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        The Very Good Food Company\n",
      "   Position:       Quality Assurance Technician\n",
      "   Start Date:     February-2021\n",
      "   End Date:       February-2022\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 2 - ID: 120e8829-f4e5-4f0e-aba9-00c0632d6772\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Developed a Power BI dashboard to track changes in imported food volumes, collaborating with import inspectors to define metrics and design visualizations in Power BI for stakeholder use.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 3 - ID: 2c352005-a35e-438c-a669-9ac7587df973\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Developed and deployed a time-series forecasting model that achieved 87% accuracy in predicting monthly import/export volumes by training on historical data using Python and time-series ML techniques.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 4 - ID: 495bbc70-277c-481f-8a25-e74c8863b53d\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Analyst: Built three Power BI dashboards for sales and marketing by collaborating with stakeholders to define requirements and implementing interactive reports in Power BI.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Rubicon Organics\n",
      "   Position:       Data Analyst\n",
      "   Start Date:     March-2023\n",
      "   End Date:       December-2023\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 5 - ID: 4b45030c-9493-4299-a2d5-c36ec5b253d1\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Built an unsupervised anomaly detection model to identify potential food safety risks in imported products by training unsupervised ML models using Python and applying them to imported product datasets.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 6 - ID: 683dc6aa-604a-444d-a6c0-6d41590d8c6a\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Assurance Technician: Performed root cause analyses on non-conforming products, reducing rework and generating cost savings by applying corrective actions informed by statistical analysis in Excel.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        The Very Good Food Company\n",
      "   Position:       Quality Assurance Technician\n",
      "   Start Date:     February-2021\n",
      "   End Date:       February-2022\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 7 - ID: 8cfb1b8b-a386-4e06-8edb-d2e8358f0cad\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Analyzed pathogen occurrence trends across 50 facilities by performing statistical analysis with SQL and Power BI and collaborating with inspectors to implement preventive inspections.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 8 - ID: 9f290562-b1a2-472d-b001-b93a8f7442d0\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Implemented daily automated data refreshes, replacing weekly manual CSV exports, by gaining direct data-warehouse access and building an ETL pipeline using SQL and Microsoft Fabric to store data in a Lakehouse.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 9 - ID: a8c3550d-ef73-4b7e-a92e-10b9d869e225\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Analyst: Designed and implemented a reporting tool to pinpoint SKU opportunities at store level, contributing to a 12% increase in store-level sales by analyzing SKU performance with SQL and presenting results in Power BI.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Rubicon Organics\n",
      "   Position:       Data Analyst\n",
      "   Start Date:     March-2023\n",
      "   End Date:       December-2023\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 10 - ID: ab967565-e50f-4c70-87a4-0a231b9c7795\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Extracted and processed millions of import/export transactions by building web-scraping collectors and a PySpark ETL pipeline to load cleaned data into a Microsoft Fabric lakehouse.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 11 - ID: b1a803d2-7e5b-4daf-b47f-02ce0fbec044\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Engineer: Reduced hard-drive screw defects by 15% (â‰ˆ$110,000 annual savings) by extracting and cleaning 50,000+ records with SQL, performing statistical analysis in Excel, and applying Six Sigma root-cause methods.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        IBM\n",
      "   Position:       Quality Engineer\n",
      "   Start Date:     December-2017\n",
      "   End Date:       October-2018\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 12 - ID: b3051ea7-bc51-4d39-9e9e-1068a8472708\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Automated data categorization, reducing data cleaning time by over 15 hours per week, by integrating LLM-based classification into the ETL pipeline.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 13 - ID: c02cab12-4607-4ec8-8056-8ed2cc3cfb83\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Standardized descriptive and statistical reporting in Power BI, reducing report-generation time and improving inspection efficiency by creating templated reports and automated data queries.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 14 - ID: cb1defa7-f89a-48f8-a349-42ac9a50c25c\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Automated forecasting and reduced manual effort by 40 hours per month by deploying the forecasting pipeline and scheduling automated runs using Python and Microsoft Fabric.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 15 - ID: cd13be35-b9c4-4db2-9851-32c564d98702\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Developed and deployed anomaly-detection and time-series forecasting models achieving 86% accuracy in predicting food safety risk prevalence by training models on historical data using Python and ML techniques.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 16 - ID: d10e77bc-fec7-40c2-9615-a827b9a1314b\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Engineer: Developed an Excel dashboard with automated SQL queries, reducing manual reporting time by 40% and enabling near-real-time quality monitoring by automating data extraction and transformation.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        IBM\n",
      "   Position:       Quality Engineer\n",
      "   Start Date:     December-2017\n",
      "   End Date:       October-2018\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 17 - ID: d32f10d4-abc0-4400-bd8c-309c8634df80\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Implemented a Python algorithm to automatically select sampling plans, reducing inspector manual work by 3 hours per inspector per day and generating approximately CAD 4,000,000 in annual savings by translating business rules into an automated algorithm.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 18 - ID: e214dec8-cc52-44d2-a214-3eab719ecfab\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Analyst: Built an ETL pipeline integrating five data sources totaling over 1M records using SQL and Python, automating ingestion and cleaning and saving 8 hours weekly in data preparation.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Rubicon Organics\n",
      "   Position:       Data Analyst\n",
      "   Start Date:     March-2023\n",
      "   End Date:       December-2023\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 19 - ID: f7329237-27a1-435f-b4b6-f00ba634c2d7\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Assurance Technician: Automated sanitation KPI reporting using Excel and MS Forms, contributing to an 80% SQF audit score by building automated reports and forms to track sanitation metrics.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        The Very Good Food Company\n",
      "   Position:       Quality Assurance Technician\n",
      "   Start Date:     February-2021\n",
      "   End Date:       February-2022\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for work experience entries (from resume_data collection)\n",
    "work_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"section_type\",\n",
    "            match=MatchValue(value=\"work_experience\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "work_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,  # Query resume_data collection\n",
    "    scroll_filter=work_filter,\n",
    "    limit=20,\n",
    "    with_payload=True,\n",
    "    with_vectors=False  # Set True to see embeddings\n",
    ")\n",
    "\n",
    "print(f\"ğŸ’¼ Work Experience Chunks from '{resume_collection}' collection (showing {len(work_records)}):\\n\")\n",
    "\n",
    "for i, record in enumerate(work_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CHUNK {i} - ID: {record.id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“„ Content (Achievement):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ¢ Metadata:\")\n",
    "    print(f\"   Company:        {metadata.get('company', 'N/A')}\")\n",
    "    print(f\"   Position:       {metadata.get('position', 'N/A')}\")\n",
    "    print(f\"   Start Date:     {metadata.get('start_date', 'N/A')}\")\n",
    "    print(f\"   End Date:       {metadata.get('end_date', 'N/A')}\")\n",
    "    print(f\"   Source File:    {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"   Section Type:   {payload.get('section_type', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ee076",
   "metadata": {},
   "source": [
    "### 3.2 View Work Experience WITH Embeddings\n",
    "\n",
    "Each chunk has a 1536-dimensional embedding vector generated by OpenAI's `text-embedding-3-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5d0755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Embedding Vector Details:\n",
      "   Vector dimensions: 1536\n",
      "   Vector type: <class 'list'>\n",
      "   First 10 values: [-0.019191697239875793, 0.011832953430712223, 0.06691659241914749, 0.00033292826265096664, -0.016301382333040237, 0.029689325019717216, 0.020486559718847275, 0.019688831642270088, 0.03362015634775162, 0.01056121475994587]\n",
      "   Last 10 values:  [0.024139918386936188, 0.00474011804908514, -0.013596045784652233, -0.0012016488471999764, -0.01553833857178688, -0.006566797848790884, -0.006665068678557873, 0.014879346825182438, 0.003659140085801482, -0.0037920945324003696]\n",
      "\n",
      "ğŸ“Š Vector Statistics:\n",
      "   Min value:  -0.097485\n",
      "   Max value:  0.109878\n",
      "   Mean value: 0.000860\n",
      "   Std dev:    0.025501\n",
      "\n",
      "ğŸ“„ Associated Content:\n",
      "   Quality Assurance Technician: Coordinated supply chain and production teams to ensure food safety compliance by leading cross-functional meetings and ...\n"
     ]
    }
   ],
   "source": [
    "# Get one work experience record WITH embeddings\n",
    "work_with_vector, _ = client.scroll(\n",
    "    collection_name=resume_collection,\n",
    "    scroll_filter=work_filter,\n",
    "    limit=20,\n",
    "    with_payload=True,\n",
    "    with_vectors=True  # Include embeddings\n",
    ")\n",
    "\n",
    "if work_with_vector:\n",
    "    record = work_with_vector[0]\n",
    "    vector = record.vector\n",
    "    \n",
    "    print(f\"ğŸ”¢ Embedding Vector Details:\")\n",
    "    print(f\"   Vector dimensions: {len(vector)}\")\n",
    "    print(f\"   Vector type: {type(vector)}\")\n",
    "    print(f\"   First 10 values: {vector[:10]}\")\n",
    "    print(f\"   Last 10 values:  {vector[-10:]}\")\n",
    "    print(f\"\\nğŸ“Š Vector Statistics:\")\n",
    "    import numpy as np\n",
    "    vector_array = np.array(vector)\n",
    "    print(f\"   Min value:  {vector_array.min():.6f}\")\n",
    "    print(f\"   Max value:  {vector_array.max():.6f}\")\n",
    "    print(f\"   Mean value: {vector_array.mean():.6f}\")\n",
    "    print(f\"   Std dev:    {vector_array.std():.6f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Associated Content:\")\n",
    "    print(f\"   {record.payload.get('content', 'N/A')[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058d26e",
   "metadata": {},
   "source": [
    "## 6. Query Personality Traits Data (from personalities_16.md)\n",
    "\n",
    "### 6.1 View Personality Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef4b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Education Entries from 'resume_data' collection (2):\n",
      "\n",
      "================================================================================\n",
      "EDUCATION CHUNK 1\n",
      "================================================================================\n",
      "ğŸ“ Degree:        BSc in Biotechnology Engineering\n",
      "ğŸ« Institution:   Tec de Monterrey\n",
      "ğŸ“… Year:          N/A\n",
      "ğŸ“‚ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  education\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   BSc in Biotechnology Engineering from Tec de Monterrey. August-2012 - May-2017 | Mexico\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"degree\": \"BSc in Biotechnology Engineering\",\n",
      "  \"institution\": \"Tec de Monterrey\",\n",
      "  \"dates\": \"August-2012 - May-2017 | Mexico\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "EDUCATION CHUNK 2\n",
      "================================================================================\n",
      "ğŸ“ Degree:        MSc in Food Science\n",
      "ğŸ« Institution:   University of British Columbia\n",
      "ğŸ“… Year:          N/A\n",
      "ğŸ“‚ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  education\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   MSc in Food Science from University of British Columbia. January-2019 - October-2020 | Canada\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"degree\": \"MSc in Food Science\",\n",
      "  \"institution\": \"University of British Columbia\",\n",
      "  \"dates\": \"January-2019 - October-2020 | Canada\"\n",
      "}\n",
      "\n",
      "\n",
      "ğŸ› ï¸  Skills Entries from 'resume_data' collection (5):\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 1\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Development Tools\n",
      "ğŸ“„ Skills:        Development Tools: Git, Docker, Azure DevOps, VS Code, API\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Development Tools\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 2\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Programming Languages\n",
      "ğŸ“„ Skills:        Programming Languages: Python, SQL, PySpark, T-SQL\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Programming Languages\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 3\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Methodologies\n",
      "ğŸ“„ Skills:        Methodologies: Agile, Six Sigma\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Methodologies\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 4\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Business Intelligence\n",
      "ğŸ“„ Skills:        Business Intelligence: Power BI, Microsoft Fabric, OneLake, Power Query, Power Pivot, Excel, Delta Lake\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Business Intelligence\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 5\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Cloud Platforms\n",
      "ğŸ“„ Skills:        Cloud Platforms: Azure, Google Cloud\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Cloud Platforms\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query education entries (from resume_data collection)\n",
    "education_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"education\"))]\n",
    ")\n",
    "\n",
    "education_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    scroll_filter=education_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“ Education Entries from '{resume_collection}' collection ({len(education_records)}):\\n\")\n",
    "for i, record in enumerate(education_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"EDUCATION CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Degree:        {metadata.get('degree', 'N/A')}\")\n",
    "    print(f\"ğŸ« Institution:   {metadata.get('institution', 'N/A')}\")\n",
    "    print(f\"ğŸ“… Year:          {metadata.get('year', 'N/A')}\")\n",
    "    print(f\"ğŸ“‚ Source File:   {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type:  {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()\n",
    "\n",
    "# Query skills (from resume_data collection)\n",
    "skills_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"skills\"))]\n",
    ")\n",
    "\n",
    "skills_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,\n",
    "    scroll_filter=skills_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ› ï¸  Skills Entries from '{resume_collection}' collection ({len(skills_records)}):\\n\")\n",
    "for i, record in enumerate(skills_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"SKILL CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“‚ Category:      {metadata.get('category', 'N/A')}\")\n",
    "    print(f\"ğŸ“„ Skills:        {payload.get('content', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Source File:   {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type:  {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c60b4",
   "metadata": {},
   "source": [
    "## 4. Query Personality Traits Data (from personalities_16.md)\n",
    "\n",
    "### 4.1 View Personality Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d24204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Personality Trait Chunks from 'personality' collection (8):\n",
      "\n",
      "ğŸ’¡ Note: This collection contains ONLY personality data with simplified fixed-size chunking\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 1\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 3\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 900 - 1300\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   also attending to crucial details, which makes me a valuable asset in any organization.\n",
      "\n",
      "## Strengths\n",
      "### Innovative Mindset\n",
      "My ability to see possibilities others overlook often helps me find smarter solutions and effective improvements at work.\n",
      "\n",
      "### Independent Worker\n",
      "My talent for working productively on my own allows me to manage tasks effectively without the need for constant direction or sup\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 3,\n",
      "  \"char_start\": 900,\n",
      "  \"char_end\": 1300,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 2\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 0\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 0 - 400\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   # Personality Traits\n",
      "I'm very analytical, highly curious and constantly seek to improve systems. I approach life with a strategic mindset, always looking several steps ahead and planning for contingencies. I value autonomy but also enjoy collaborating with others. This allows me to tackle complex problems with confidence and innovation. I hold high standards for myself and others, always striving\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 0,\n",
      "  \"char_start\": 0,\n",
      "  \"char_end\": 400,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 3\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 2\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 600 - 1000\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   rences\n",
      "I thrive in environments that challenge me intellectually and provide opportunities for growth. I seek roles where I can implement innovative ideas and apply strategic thinking and problem-solvingâ€”often in fields like science, technology, or business strategy. I can see the big picture while also attending to crucial details, which makes me a valuable asset in any organization.\n",
      "\n",
      "## Strength\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 2,\n",
      "  \"char_start\": 600,\n",
      "  \"char_end\": 1000,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 4\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 6\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 1800 - 2200\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   Reliable Performance\n",
      "When entrusted with critical tasks, I consistently deliver precise, high-quality results, making me a valued and dependable asset.\n",
      "\n",
      "### Big-Picture Focus\n",
      "I prefer focusing on overarching goals and strategies rather than micromanaging small details.\n",
      "\n",
      "### Goal-Oriented\n",
      "I stay motivated by clear goals and visible progress, consistently tracking achievements and identifying next\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 6,\n",
      "  \"char_start\": 1800,\n",
      "  \"char_end\": 2200,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 5\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 7\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 2100 - 2207\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   ivated by clear goals and visible progress, consistently tracking achievements and identifying next steps.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 7,\n",
      "  \"char_start\": 2100,\n",
      "  \"char_end\": 2207,\n",
      "  \"overlap_chars\": 0\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 6\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 5\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 1500 - 1900\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   ent\n",
      "I naturally focus on refining work processes and spotting inefficiencies, consistently improving project outcomes wherever I go.\n",
      "\n",
      "### Objective Judgment\n",
      "My capacity to make impartial decisions based on facts rather than favoritism or personal bias earns respect and trust from my colleagues.\n",
      "\n",
      "### Reliable Performance\n",
      "When entrusted with critical tasks, I consistently deliver precise, high-quali\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 5,\n",
      "  \"char_start\": 1500,\n",
      "  \"char_end\": 1900,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 7\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 1\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 300 - 700\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   oblems with confidence and innovation. I hold high standards for myself and others, always striving for efficiency and effectiveness. I prefer direct communication, honesty, and logical discussion. This tendency can make me appear aloof or detached to others, even when I care deeply.\n",
      "\n",
      "# Career Preferences\n",
      "I thrive in environments that challenge me intellectually and provide opportunities for growt\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 1,\n",
      "  \"char_start\": 300,\n",
      "  \"char_end\": 700,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 8\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 4\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 1200 - 1600\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   ively on my own allows me to manage tasks effectively without the need for constant direction or supervision.\n",
      "\n",
      "### Conceptual Thinking\n",
      "I effortlessly grasp abstract, complex ideas, making me particularly suited to roles that require strategic analysis and long-term planning.\n",
      "\n",
      "### Continuous Improvement\n",
      "I naturally focus on refining work processes and spotting inefficiencies, consistently improving\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 4,\n",
      "  \"char_start\": 1200,\n",
      "  \"char_end\": 1600,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personality_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"\"))]\n",
    ")\n",
    "\n",
    "personality_records, _ = client.scroll(\n",
    "    collection_name=personality_collection,\n",
    "    scroll_filter=personality_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"ğŸ§  Personality Trait Chunks from '{personality_collection}' collection ({len(personality_records)}):\\n\")\n",
    "print(f\"ğŸ’¡ Note: This collection contains ONLY personality data with simplified fixed-size chunking\\n\")\n",
    "\n",
    "for i, record in enumerate(personality_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PERSONALITY CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Chunk Index: {metadata.get('chunk_index', 'N/A')}\")\n",
    "    print(f\"ğŸ“‚ Source File: {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Character Range: {metadata.get('char_start', 'N/A')} - {metadata.get('char_end', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4443867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE PROJECTS RETRIEVAL WORKFLOW\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Job Requirements: data visualization matplotlib seaborn statistical analysis\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: Search Technical Summaries\n",
      "================================================================================\n",
      "\n",
      "âœ… Found 2 matching projects:\n",
      "\n",
      "1. [0.352] Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "   ID: project_0\n",
      "   Tech: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "\n",
      "2. [0.334] SQL Database of Save-On-Foods products extracted using API\\\n",
      "   ID: project_1\n",
      "   Tech: Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: Retrieve Full Project Content\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Retrieving full content for 2 projects...\n",
      "\n",
      "================================================================================\n",
      "PROJECT: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "================================================================================\n",
      "ğŸ†” ID: project_0\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "ğŸ”— URL: https://github.com/aleivaar94/Renew-Amazon-Prime-2022\n",
      "\n",
      "ğŸ“„ Full Content (first 600 chars):\n",
      "# Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "\n",
      "## Purpose\n",
      "Analyze personal Amazon order history to determine whether renewing an Amazon Prime membership is cost-effective after a 20% price increase, using historical spending patterns and modeled shipping costs.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export).\n",
      "\n",
      "## Technical Highlights\n",
      "Cleaned and normalized the raw Amazon order export, including sensitive-data removal, numeric coercion, and datetime parsing. Aggregated orders by order_id and order_date and derived year/month features\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "PROJECT: SQL Database of Save-On-Foods products extracted using API\\\n",
      "================================================================================\n",
      "ğŸ†” ID: project_1\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git\n",
      "ğŸ”— URL: https://github.com/aleivaar94/SQL-Database-of-Save-On-Foods-Products-Extracted-Using-API/blob/master/images/save-on-foods-logo.png\n",
      "\n",
      "ğŸ“„ Full Content (first 600 chars):\n",
      "# SQL Database of Save-On-Foods products extracted using API\\\n",
      "\n",
      "## Purpose\n",
      "This project programmatically extracts product data from an eâ€‘commerce API to build a clean, queryable dataset for analysis and downstream tooling. It demonstrates a repeatable ETL workflow to turn paginated JSON API results into analytics-ready CSV and relational data.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git.\n",
      "\n",
      "## Technical Highlights\n",
      "The solution identifies the vendor API via browser developer tools, converts the API response \n",
      "...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "âœ… WORKFLOW COMPLETE\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ Benefits of Hierarchical Chunking:\n",
      "   âœ“ Fast initial matching using technical summaries\n",
      "   âœ“ Retrieve full context only for relevant projects\n",
      "   âœ“ Efficient token usage (don't embed full content for initial search)\n",
      "   âœ“ Clear separation of filtering vs. detailed context\n"
     ]
    }
   ],
   "source": [
    "# Complete workflow simulation\n",
    "print(f\"{'='*80}\")\n",
    "print(\"COMPLETE PROJECTS RETRIEVAL WORKFLOW\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "job_tech_requirements = \"data visualization matplotlib seaborn statistical analysis\"\n",
    "\n",
    "print(f\"ğŸ“‹ Job Requirements: {job_tech_requirements}\\n\")\n",
    "\n",
    "# PHASE 1: Search technical summaries\n",
    "print(f\"{'='*80}\")\n",
    "print(\"PHASE 1: Search Technical Summaries\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "query_vector = embedder.embed_query(job_tech_requirements)\n",
    "\n",
    "tech_results = client.query_points(\n",
    "    collection_name=projects_collection,\n",
    "    query=query_vector,\n",
    "    query_filter=Filter(\n",
    "        must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    "    ),\n",
    "    limit=3,\n",
    "    score_threshold=0.3  # Minimum similarity\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Found {len(tech_results)} matching projects:\\n\")\n",
    "\n",
    "matched_project_ids = []\n",
    "for i, result in enumerate(tech_results, 1):\n",
    "    metadata = result.payload.get('metadata', {})\n",
    "    project_id = metadata.get('project_id')\n",
    "    matched_project_ids.append(project_id)\n",
    "    \n",
    "    print(f\"{i}. [{result.score:.3f}] {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"   ID: {project_id}\")\n",
    "    print(f\"   Tech: {', '.join(metadata.get('tech_stack', []))}\\n\")\n",
    "\n",
    "# PHASE 2: Retrieve full content for matched projects\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 2: Retrieve Full Project Content\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"ğŸ” Retrieving full content for {len(matched_project_ids)} projects...\\n\")\n",
    "\n",
    "for project_id in matched_project_ids:\n",
    "    # Filter for full content of this project\n",
    "    full_filter = Filter(\n",
    "        must=[\n",
    "            FieldCondition(key=\"metadata.project_id\", match=MatchValue(value=project_id)),\n",
    "            FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_full\"))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    full_results, _ = client.scroll(\n",
    "        collection_name=projects_collection,\n",
    "        scroll_filter=full_filter,\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    \n",
    "    if full_results:\n",
    "        record = full_results[0]\n",
    "        payload = record.payload\n",
    "        metadata = payload.get('metadata', {})\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"PROJECT: {metadata.get('project_title', 'N/A')}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ†” ID: {project_id}\")\n",
    "        print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "        print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "        print(f\"\\nğŸ“„ Full Content (first 600 chars):\")\n",
    "        print(payload.get('content', 'N/A')[:600])\n",
    "        print(\"...\\n\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… WORKFLOW COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nğŸ’¡ Benefits of Hierarchical Chunking:\")\n",
    "print(\"   âœ“ Fast initial matching using technical summaries\")\n",
    "print(\"   âœ“ Retrieve full context only for relevant projects\")\n",
    "print(\"   âœ“ Efficient token usage (don't embed full content for initial search)\")\n",
    "print(\"   âœ“ Clear separation of filtering vs. detailed context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01dd59",
   "metadata": {},
   "source": [
    "### 5.6 Complete Projects Retrieval Workflow\n",
    "\n",
    "Demonstrates the two-phase retrieval:\n",
    "1. **Phase 1**: Search `project_technical` for matching projects\n",
    "2. **Phase 2**: Retrieve `project_full` content using project_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82216e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Semantic Search: Finding projects matching job requirements\n",
      "================================================================================\n",
      "\n",
      "Query: 'Python data visualization pandas matplotlib plotly'\n",
      "Collection: projects\n",
      "Target: project_technical (for fast matching)\n",
      "\n",
      "âœ… Top 2 Matching Projects (by technical summary):\n",
      "\n",
      "================================================================================\n",
      "MATCH 1 - Similarity Score: 0.3120\n",
      "================================================================================\n",
      "ğŸ“¦ Project: SQL Database of Save-On-Foods products extracted using API\\\n",
      "ğŸ†” ID: project_1\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git\n",
      "ğŸ”— URL: https://github.com/aleivaar94/SQL-Database-of-Save-On-Foods-Products-Extracted-Using-API/blob/master/images/save-on-foods-logo.png\n",
      "\n",
      "ğŸ“„ Technical Summary (first 300 chars):\n",
      "   Project: SQL Database of Save-On-Foods products extracted using API\\\n",
      "\n",
      "Technologies: Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git.\n",
      "\n",
      "Technical Work:\n",
      "The solution identifies the vendor API via browser developer tools, converts t...\n",
      "\n",
      "================================================================================\n",
      "MATCH 2 - Similarity Score: 0.3104\n",
      "================================================================================\n",
      "ğŸ“¦ Project: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "ğŸ†” ID: project_0\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "ğŸ”— URL: https://github.com/aleivaar94/Renew-Amazon-Prime-2022\n",
      "\n",
      "ğŸ“„ Technical Summary (first 300 chars):\n",
      "   Project: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "\n",
      "Technologies: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export).\n",
      "\n",
      "Technical Work:\n",
      "Cleaned and normalized the raw Amazon order export, including sensitive-data removal, numeric coercion, and datetime pa...\n",
      "\n",
      "\n",
      "ğŸ’¡ Workflow: Search technical summaries â†’ Get project_id â†’ Retrieve full content using metadata filter\n"
     ]
    }
   ],
   "source": [
    "# Simulate job requirements\n",
    "job_requirements = \"Python data visualization pandas matplotlib plotly\"\n",
    "\n",
    "print(f\"ğŸ” Semantic Search: Finding projects matching job requirements\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Query: '{job_requirements}'\")\n",
    "print(f\"Collection: {projects_collection}\")\n",
    "print(f\"Target: project_technical (for fast matching)\\n\")\n",
    "\n",
    "# Generate query embedding\n",
    "query_vector = embedder.embed_query(job_requirements)\n",
    "\n",
    "# Search technical summaries first (faster, focused)\n",
    "tech_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    ")\n",
    "\n",
    "tech_results = client.query_points(\n",
    "    collection_name=projects_collection,\n",
    "    query=query_vector,\n",
    "    query_filter=tech_filter,\n",
    "    limit=5\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Top {len(tech_results)} Matching Projects (by technical summary):\\n\")\n",
    "\n",
    "for i, result in enumerate(tech_results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"MATCH {i} - Similarity Score: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“¦ Project: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"ğŸ†” ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Technical Summary (first 300 chars):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:300]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ Workflow: Search technical summaries â†’ Get project_id â†’ Retrieve full content using metadata filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2583756",
   "metadata": {},
   "source": [
    "### 5.5 Semantic Search: Find Projects by Technical Requirements\n",
    "\n",
    "Search for projects matching specific technologies or technical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe72b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching for project by metadata: project_id = 'project_0'\n",
      "================================================================================\n",
      "\n",
      "âœ… Found 2 chunks for project_0\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1: project_technical\n",
      "================================================================================\n",
      "ğŸ“¦ Project Title: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "ğŸ†” Project ID: project_0\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "\n",
      "ğŸ”§ Technical Summary (first 400 chars):\n",
      "   Project: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "\n",
      "Technologies: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export).\n",
      "\n",
      "Technical Work:\n",
      "Cleaned and normalized the raw Amazon order export, including sensitive-data removal, numeric coercion, and datetime parsing. Aggregated orders by order_id and order_date and derived year/month features to enable per-ye...\n",
      "\n",
      "================================================================================\n",
      "CHUNK 2: project_full\n",
      "================================================================================\n",
      "ğŸ“¦ Project Title: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "ğŸ†” Project ID: project_0\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "\n",
      "ğŸ“š Full Content (first 500 chars):\n",
      "   # Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "\n",
      "## Purpose\n",
      "Analyze personal Amazon order history to determine whether renewing an Amazon Prime membership is cost-effective after a 20% price increase, using historical spending patterns and modeled shipping costs.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export).\n",
      "\n",
      "## Technical Highlights\n",
      "Cleaned and normalized the raw Amazon order export, including sensitive-data removal, numeric coercion...\n",
      "\n",
      "\n",
      "ğŸ’¡ Use Case: Retrieve full project details after finding relevant technical summary\n"
     ]
    }
   ],
   "source": [
    "# Query for a specific project using metadata filtering\n",
    "target_project_id = \"project_0\"  # Change this to test different projects\n",
    "\n",
    "print(f\"ğŸ” Searching for project by metadata: project_id = '{target_project_id}'\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Build filter for exact metadata match\n",
    "metadata_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"metadata.project_id\",\n",
    "            match=MatchValue(value=target_project_id)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieve matching documents\n",
    "results, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    scroll_filter=metadata_filter,\n",
    "    limit=10,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Found {len(results)} chunks for {target_project_id}\\n\")\n",
    "\n",
    "for i, record in enumerate(results, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    section_type = payload.get('section_type', 'N/A')\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CHUNK {i}: {section_type}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“¦ Project Title: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"ğŸ†” Project ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ“¦ Chunk Type: {metadata.get('chunk_type', 'N/A')}\")\n",
    "    \n",
    "    if section_type == \"project_technical\":\n",
    "        print(f\"\\nğŸ”§ Technical Summary (first 400 chars):\")\n",
    "        print(f\"   {payload.get('content', 'N/A')[:400]}...\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“š Full Content (first 500 chars):\")\n",
    "        print(f\"   {payload.get('content', 'N/A')[:500]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ Use Case: Retrieve full project details after finding relevant technical summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbf24c",
   "metadata": {},
   "source": [
    "### 5.4 Query by Metadata: Retrieve Specific Project by ID\n",
    "\n",
    "This demonstrates the `search_by_metadata()` pattern from the vector_store module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636aa2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Full Project Content Chunks (project_full)\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ These chunks contain ALL sections for complete project context\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE FULL PROJECT: SQL Database of Save-On-Foods products extracted using API\\\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_1\n",
      "ğŸ”— URL: https://github.com/aleivaar94/SQL-Database-of-Save-On-Foods-Products-Extracted-Using-API/blob/master/images/save-on-foods-logo.png\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content:\n",
      "# SQL Database of Save-On-Foods products extracted using API\\\n",
      "\n",
      "## Purpose\n",
      "This project programmatically extracts product data from an eâ€‘commerce API to build a clean, queryable dataset for analysis and downstream tooling. It demonstrates a repeatable ETL workflow to turn paginated JSON API results into analytics-ready CSV and relational data.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git.\n",
      "\n",
      "## Technical Highlights\n",
      "The solution identifies the vendor API via browser developer tools, converts the API response into structured data, and implements pagination by incrementing the API skip/take parameters to reliably iterate through results. JSON responses are normalized into tabular form using pandas' json_normalize, followed by targeted cleaning and column transformations to parse price and unit fields. The cleaned dataframe is exported to CSV and persisted into a SQLite relational table with explicit column types via SQLAlchemy for easy querying and integration.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Practical skills include API-driven data extraction, ETL design, JSON normalization, data cleaning and transformation, relational schema creation, and reproducible analysis in a notebook environment. It also demonstrates use of tooling for API inspection and request generation (Postman) and basic database engineering for analytics.\n",
      "\n",
      "## Result/Impact\n",
      "The pipeline extracted and consolidated 1,519 meat-related product records into a CSV and a SQLite database, enabling efficient SQL queries and downstream analysis. This reproducible workflow reduces manual scraping effort and provides a reliable foundation for price analysis, category insights, and inventory-style analytics.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ğŸ“Š Summary of All Full Project Chunks (2 total):\n",
      "\n",
      "1. SQL Database of Save-On-Foods products extracted using API\\\n",
      "   Tech: Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git\n",
      "\n",
      "2. Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "   Tech: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for full content chunks\n",
    "full_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_full\"))]\n",
    ")\n",
    "\n",
    "full_records, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    scroll_filter=full_filter,\n",
    "    limit=100,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“š Full Project Content Chunks (project_full)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"ğŸ’¡ These chunks contain ALL sections for complete project context\\n\")\n",
    "\n",
    "# Display first project in detail\n",
    "if full_records:\n",
    "    record = full_records[0]\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"EXAMPLE FULL PROJECT: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ†” Project ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ“¦ Chunk Type: {metadata.get('chunk_type', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Full Content:\")\n",
    "    print(payload.get('content', 'N/A'))\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Summary of all full projects\n",
    "print(f\"\\nğŸ“Š Summary of All Full Project Chunks ({len(full_records)} total):\\n\")\n",
    "for i, record in enumerate(full_records, 1):\n",
    "    metadata = record.payload.get('metadata', {})\n",
    "    print(f\"{i}. {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"   Tech: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da1d51",
   "metadata": {},
   "source": [
    "### 5.3 View Full Project Content (project_full chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a4d4200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Technical Summary Chunks (project_technical)\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ These chunks contain Tech Stack + Technical Highlights for fast matching\n",
      "\n",
      "================================================================================\n",
      "PROJECT 1: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_0\n",
      "ğŸ”— URL: https://github.com/aleivaar94/Renew-Amazon-Prime-2022\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "\n",
      "Technologies: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export).\n",
      "\n",
      "Technical Work:\n",
      "Cleaned and normalized the raw Amazon order export, including sensitive-data removal, numeric coercion, and datetime pa...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 2: SQL Database of Save-On-Foods products extracted using API\\\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_1\n",
      "ğŸ”— URL: https://github.com/aleivaar94/SQL-Database-of-Save-On-Foods-Products-Extracted-Using-API/blob/master/images/save-on-foods-logo.png\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: SQL Database of Save-On-Foods products extracted using API\\\n",
      "\n",
      "Technologies: Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git.\n",
      "\n",
      "Technical Work:\n",
      "The solution identifies the vendor API via browser developer tools, converts t...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for technical summary chunks\n",
    "tech_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    ")\n",
    "\n",
    "tech_records, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    scroll_filter=tech_filter,\n",
    "    limit=100,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ”§ Technical Summary Chunks (project_technical)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"ğŸ’¡ These chunks contain Tech Stack + Technical Highlights for fast matching\\n\")\n",
    "\n",
    "for i, record in enumerate(tech_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PROJECT {i}: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ†” Project ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ“¦ Chunk Type: {metadata.get('chunk_type', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content Preview (first 300 chars):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:300]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41969762",
   "metadata": {},
   "source": [
    "### 5.2 View Technical Summaries (project_technical chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6099a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Projects Collection Overview\n",
      "================================================================================\n",
      "\n",
      "Total chunks in collection: 4\n",
      "Total unique projects: 2\n",
      "\n",
      "ğŸ’¡ Each project has 2 chunks: technical_summary + full_content\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¦ Project: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "   ID: project_0\n",
      "   URL: https://github.com/aleivaar94/Renew-Amazon-Prime-2022\n",
      "   Tech Stack: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: SQL Database of Save-On-Foods products extracted using API\\\n",
      "   ID: project_1\n",
      "   URL: https://github.com/aleivaar94/SQL-Database-of-Save-On-Foods-Products-Extracted-Using-API/blob/master/images/save-on-foods-logo.png\n",
      "   Tech Stack: Python, Jupyter Notebook, pandas, requests, numpy, json, SQLAlchemy, SQLite, Postman (for request prototyping), and Git\n",
      "   Chunks: 2 (technical + full)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View all projects in the collection\n",
    "all_projects, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    limit=100,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“‚ Projects Collection Overview\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Total chunks in collection: {len(all_projects)}\")\n",
    "\n",
    "# Group by project_id\n",
    "from collections import defaultdict\n",
    "projects_by_id = defaultdict(list)\n",
    "for record in all_projects:\n",
    "    project_id = record.payload.get('metadata', {}).get('project_id')\n",
    "    projects_by_id[project_id].append(record)\n",
    "\n",
    "print(f\"Total unique projects: {len(projects_by_id)}\")\n",
    "print(f\"\\nğŸ’¡ Each project has 2 chunks: technical_summary + full_content\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Display each project's chunks\n",
    "for project_id, chunks in sorted(projects_by_id.items()):\n",
    "    tech_chunk = next((c for c in chunks if c.payload.get('section_type') == 'project_technical'), None)\n",
    "    full_chunk = next((c for c in chunks if c.payload.get('section_type') == 'project_full'), None)\n",
    "    \n",
    "    if tech_chunk:\n",
    "        metadata = tech_chunk.payload.get('metadata', {})\n",
    "        print(f\"ğŸ“¦ Project: {metadata.get('project_title', 'N/A')}\")\n",
    "        print(f\"   ID: {project_id}\")\n",
    "        print(f\"   URL: {metadata.get('project_url', 'N/A')}\")\n",
    "        print(f\"   Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "        print(f\"   Chunks: {len(chunks)} (technical + full)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1499a58",
   "metadata": {},
   "source": [
    "## 5. Query Portfolio Projects Collection (from portfolio_projects.md)\n",
    "\n",
    "### 5.1 Understanding Hierarchical Project Chunking\n",
    "\n",
    "The `projects` collection uses a **hierarchical chunking strategy** with two chunk types per project:\n",
    "\n",
    "1. **`project_technical`**: Tech Stack + Technical Highlights (for filtering/matching)\n",
    "2. **`project_full`**: Complete project with all sections (Purpose, Tech Stack, Technical Highlights, Skills Demonstrated, Result/Impact)\n",
    "\n",
    "This dual-chunk approach enables:\n",
    "- Fast filtering by technology stack\n",
    "- Quick matching based on technical work\n",
    "- Full context retrieval when needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b01149",
   "metadata": {},
   "source": [
    "### 4.2 View All Personality Chunks\n",
    "\n",
    "With simplified fixed-size chunking, all chunks in the personality collection are treated equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfb16d",
   "metadata": {},
   "source": [
    "## 7. Semantic Search Examples\n",
    "\n",
    "### 7.1 Search for Python-Related Work Experience\n",
    "\n",
    "This demonstrates how semantic search works with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f9e2eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Semantic Search Query: 'Python data analysis ETL pipeline machine learning'\n",
      "   Query vector dimensions: 1536\n",
      "   Searching in: resume_data collection\n",
      "\n",
      "ğŸ“Š Top 4 Results (by semantic similarity):\n",
      "\n",
      "================================================================================\n",
      "RESULT 1 - Similarity Score: 0.6346\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Analyst: Built an ETL pipeline integrating five data sources totaling over 1M records using SQL and Python, automating ingestion and cleaning and saving 8 hours weekly in data preparation.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Rubicon Organics\n",
      "   Position: Data Analyst\n",
      "\n",
      "================================================================================\n",
      "RESULT 2 - Similarity Score: 0.5505\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Scientist II: Extracted and processed millions of import/export transactions by building web-scraping collectors and a PySpark ETL pipeline to load cleaned data into a Microsoft Fabric lakehouse.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Canadian Food Inspection Agency\n",
      "   Position: Data Scientist II\n",
      "\n",
      "================================================================================\n",
      "RESULT 3 - Similarity Score: 0.5154\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Scientist II: Automated data categorization, reducing data cleaning time by over 15 hours per week, by integrating LLM-based classification into the ETL pipeline.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Canadian Food Inspection Agency\n",
      "   Position: Data Scientist II\n",
      "\n",
      "================================================================================\n",
      "RESULT 4 - Similarity Score: 0.5121\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Scientist II: Implemented daily automated data refreshes, replacing weekly manual CSV exports, by gaining direct data-warehouse access and building an ETL pipeline using SQL and Microsoft Fabric to store data in a Lakehouse.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Canadian Food Inspection Agency\n",
      "   Position: Data Scientist II\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a query for Python-related achievements\n",
    "query_text = \"Python data analysis ETL pipeline machine learning\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "print(f\"ğŸ” Semantic Search Query: '{query_text}'\")\n",
    "print(f\"   Query vector dimensions: {len(query_vector)}\")\n",
    "print(f\"   Searching in: {resume_collection} collection\")\n",
    "\n",
    "# Search with vector similarity using query_points (newer API)\n",
    "results = client.query_points(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    query=query_vector,\n",
    "    limit=5,\n",
    "    score_threshold=0.5  # Only return results with similarity > 0.5\n",
    ").points\n",
    "\n",
    "print(f\"\\nğŸ“Š Top {len(results)} Results (by semantic similarity):\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"RESULT {i} - Similarity Score: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“„ Content: {payload.get('content', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    if payload.get('section_type') == 'work_experience':\n",
    "        print(f\"   Company: {metadata.get('company', 'N/A')}\")\n",
    "        print(f\"   Position: {metadata.get('position', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe440754",
   "metadata": {},
   "source": [
    "### 7.2 Search for Personality Traits Matching Job Requirements\n",
    "\n",
    "**NEW: Direct query to personality collection (no filtering needed!)**\n",
    "\n",
    "This mimics how `retrieve_personality_traits()` works in the resume generator with the new architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114100a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Job Requirements Query: 'analytical thinking problem-solving collaboration strategic innovative team player'\n",
      "   Searching in: personality collection (NEW!)\n",
      "\n",
      "âœ… Retrieved 8 results from personality collection\n",
      "\n",
      "ğŸ“Š Top 5 Personality Trait Chunks by Semantic Similarity:\n",
      "\n",
      "ğŸ’¡ Benefits of simplified chunking:\n",
      "   - Pure semantic search without complex filtering\n",
      "   - Fixed-size chunks maintain consistent context windows\n",
      "   - Faster search (smaller collection)\n",
      "\n",
      "================================================================================\n",
      "TRAIT 1 - Similarity: 0.5586\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 0\n",
      "ğŸ“ Characters: 0 - 400\n",
      "ğŸ“„ Content:\n",
      "   # Personality Traits\n",
      "I'm very analytical, highly curious and constantly seek to improve systems. I approach life with a strategic mindset, always looking several steps ahead and planning for contingencies. I value autonomy but also enjoy collaborating with others. This allows me to tackle complex problems with confidence and innovation. I hold high standards for myself and others, always striving\n",
      "\n",
      "================================================================================\n",
      "TRAIT 2 - Similarity: 0.4968\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 3\n",
      "ğŸ“ Characters: 900 - 1300\n",
      "ğŸ“„ Content:\n",
      "   also attending to crucial details, which makes me a valuable asset in any organization.\n",
      "\n",
      "## Strengths\n",
      "### Innovative Mindset\n",
      "My ability to see possibilities others overlook often helps me find smarter solutions and effective improvements at work.\n",
      "\n",
      "### Independent Worker\n",
      "My talent for working productively on my own allows me to manage tasks effectively without the need for constant direction or sup\n",
      "\n",
      "================================================================================\n",
      "TRAIT 3 - Similarity: 0.4910\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 4\n",
      "ğŸ“ Characters: 1200 - 1600\n",
      "ğŸ“„ Content:\n",
      "   ively on my own allows me to manage tasks effectively without the need for constant direction or supervision.\n",
      "\n",
      "### Conceptual Thinking\n",
      "I effortlessly grasp abstract, complex ideas, making me particularly suited to roles that require strategic analysis and long-term planning.\n",
      "\n",
      "### Continuous Improvement\n",
      "I naturally focus on refining work processes and spotting inefficiencies, consistently improving\n",
      "\n",
      "================================================================================\n",
      "TRAIT 4 - Similarity: 0.4732\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 2\n",
      "ğŸ“ Characters: 600 - 1000\n",
      "ğŸ“„ Content:\n",
      "   rences\n",
      "I thrive in environments that challenge me intellectually and provide opportunities for growth. I seek roles where I can implement innovative ideas and apply strategic thinking and problem-solvingâ€”often in fields like science, technology, or business strategy. I can see the big picture while also attending to crucial details, which makes me a valuable asset in any organization.\n",
      "\n",
      "## Strength\n",
      "\n",
      "================================================================================\n",
      "TRAIT 5 - Similarity: 0.4607\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 5\n",
      "ğŸ“ Characters: 1500 - 1900\n",
      "ğŸ“„ Content:\n",
      "   ent\n",
      "I naturally focus on refining work processes and spotting inefficiencies, consistently improving project outcomes wherever I go.\n",
      "\n",
      "### Objective Judgment\n",
      "My capacity to make impartial decisions based on facts rather than favoritism or personal bias earns respect and trust from my colleagues.\n",
      "\n",
      "### Reliable Performance\n",
      "When entrusted with critical tasks, I consistently deliver precise, high-quali\n",
      "\n",
      "\n",
      "ğŸ’¡ These traits would be deduplicated (removing 100-char overlaps) and injected into the cover letter prompt!\n"
     ]
    }
   ],
   "source": [
    "# Simulate a job analysis with soft skills and keywords\n",
    "job_analysis = {\n",
    "    'soft_skills': ['analytical thinking', 'problem-solving', 'collaboration'],\n",
    "    'keywords': ['strategic', 'innovative', 'team player']\n",
    "}\n",
    "\n",
    "# Build query (same logic as retrieve_personality_traits)\n",
    "query_parts = job_analysis.get('soft_skills', []) + job_analysis.get('keywords', [])\n",
    "query_text = ' '.join(query_parts)\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "print(f\"ğŸ” Job Requirements Query: '{query_text}'\")\n",
    "print(f\"   Searching in: {personality_collection} collection (NEW!)\\n\")\n",
    "\n",
    "# Search the PERSONALITY collection directly (no filtering needed!)\n",
    "all_results = client.query_points(\n",
    "    collection_name=personality_collection,  # â† Query personality collection directly!\n",
    "    query=query_vector,\n",
    "    limit=10\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Retrieved {len(all_results)} results from personality collection\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 5 Personality Trait Chunks by Semantic Similarity:\\n\")\n",
    "print(f\"ğŸ’¡ Benefits of simplified chunking:\")\n",
    "print(f\"   - Pure semantic search without complex filtering\")\n",
    "print(f\"   - Fixed-size chunks maintain consistent context windows\")\n",
    "print(f\"   - Faster search (smaller collection)\\n\")\n",
    "\n",
    "for i, result in enumerate(all_results[:5], 1):  # Top 5\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAIT {i} - Similarity: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Chunk Index: {metadata.get('chunk_index', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Characters: {metadata.get('char_start', 'N/A')} - {metadata.get('char_end', 'N/A')}\")\n",
    "    print(f\"ğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ These traits would be deduplicated (removing 100-char overlaps) and injected into the cover letter prompt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b67c13",
   "metadata": {},
   "source": [
    "### 7.3 Semantic Search with Section Filtering\n",
    "\n",
    "Combine semantic search with metadata filters for precise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "096b45de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: 'data science machine learning SQL Python dashboard visualization'\n",
      "ğŸ“¦ Collection: resume_data\n",
      "ğŸ¯ Filter: section_type = 'work_experience'\n",
      "\n",
      "ğŸ“Š Top 5 Work Achievements:\n",
      "\n",
      "1. [Score: 0.5813] Canadian Food Inspection Agency - Data Scientist\n",
      "   Data Scientist: Standardized descriptive and statistical reporting in Power BI, reducing report-gene...\n",
      "\n",
      "2. [Score: 0.5107] Rubicon Organics - Data Analyst\n",
      "   Data Analyst: Built an ETL pipeline integrating five data sources totaling over 1M records using SQL...\n",
      "\n",
      "3. [Score: 0.5098] Rubicon Organics - Data Analyst\n",
      "   Data Analyst: Built three Power BI dashboards for sales and marketing by collaborating with stakehol...\n",
      "\n",
      "4. [Score: 0.4945] Canadian Food Inspection Agency - Data Scientist II\n",
      "   Data Scientist II: Automated forecasting and reduced manual effort by 40 hours per month by deployin...\n",
      "\n",
      "5. [Score: 0.4919] Canadian Food Inspection Agency - Data Scientist II\n",
      "   Data Scientist II: Implemented daily automated data refreshes, replacing weekly manual CSV exports, ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for data science achievements ONLY in work experience (resume_data collection)\n",
    "query_text = \"data science machine learning SQL Python dashboard visualization\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "# Apply filter to only search work_experience\n",
    "work_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"work_experience\"))]\n",
    ")\n",
    "\n",
    "results = client.query_points(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    query=query_vector,\n",
    "    query_filter=work_filter,  # â† Apply filter during search\n",
    "    limit=5\n",
    ").points\n",
    "\n",
    "print(f\"ğŸ” Query: '{query_text}'\")\n",
    "print(f\"ğŸ“¦ Collection: {resume_collection}\")\n",
    "print(f\"ğŸ¯ Filter: section_type = 'work_experience'\")\n",
    "print(f\"\\nğŸ“Š Top {len(results)} Work Achievements:\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{i}. [Score: {result.score:.4f}] {metadata.get('company', 'N/A')} - {metadata.get('position', 'N/A')}\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518991b",
   "metadata": {},
   "source": [
    "## 8. Complete RAG Workflow Example (Triple-Collection Architecture)\n",
    "\n",
    "This demonstrates the updated retrieval flow using **separate collections** for resume, personality, and projects data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6753e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE RAG WORKFLOW: Data Scientist Position\n",
      "(Using Triple-Collection Architecture)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Job: Senior Data Scientist at Tech Corp\n",
      "ğŸ“ Requirements: Python, ML, SQL, data viz, analytical thinking, communication\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: RETRIEVAL (Vector Similarity Search)\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Searching resume_data collection for work achievements...\n",
      "âœ… Retrieved 10 relevant work achievements:\n",
      "   1. [0.452] Canadian Food Inspection Agency - Data Scientist II: Extracted and processed millions of impor...\n",
      "   2. [0.448] Canadian Food Inspection Agency - Data Scientist: Standardized descriptive and statistical rep...\n",
      "   3. [0.448] Canadian Food Inspection Agency - Data Scientist II: Implemented daily automated data refreshe...\n",
      "   4. [0.441] Canadian Food Inspection Agency - Data Scientist: Analyzed pathogen occurrence trends across 5...\n",
      "   5. [0.435] Rubicon Organics - Data Analyst: Built an ETL pipeline integrating five data so...\n",
      "\n",
      "ğŸ“‚ Searching projects collection for matching portfolio work...\n",
      "âœ… Retrieved 2 relevant projects:\n",
      "   1. [0.363] SQL Database of Save-On-Foods products extracted using API\\ - Tech: Python, Jupyter Notebook, pandas\n",
      "   2. [0.308] Renew Amazon Prime? A Cost-Benefit Analysis - Tech: Python, Jupyter Notebook, pandas\n",
      "\n",
      "ğŸ§  Searching personality collection for matching traits...\n",
      "âœ… Retrieved 0 personality traits:\n",
      "\n",
      "ğŸ’¡ Architecture Benefits:\n",
      "   âœ“ Resume, projects, and personality queries run independently\n",
      "   âœ“ No cross-contamination between collections\n",
      "   âœ“ Faster searches (smaller, focused collections)\n",
      "   âœ“ Hierarchical project retrieval (technical summaries first)\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: AUGMENTATION (Combine Context)\n",
      "================================================================================\n",
      "\n",
      "âœ… Would combine:\n",
      "   - Job requirements: Senior Data Scientist, Python, ML, SQL...\n",
      "   - 5 work achievements (from resume_data collection)\n",
      "   - 2 portfolio projects (from projects collection)\n",
      "   - 0 personality traits (from personality collection)\n",
      "   - Into a structured prompt for Claude\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: GENERATION (Claude LLM)\n",
      "================================================================================\n",
      "\n",
      "âœ… Would call Claude API with augmented prompt to generate:\n",
      "   - Tailored resume sections\n",
      "   - Personalized cover letter\n",
      "   - Using ONLY the retrieved context from all three collections\n",
      "\n",
      "================================================================================\n",
      "âœ… RAG WORKFLOW COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Simulate a complete RAG workflow for a Data Scientist job\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE RAG WORKFLOW: Data Scientist Position\")\n",
    "print(\"(Using Triple-Collection Architecture)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Job context\n",
    "job_title = \"Senior Data Scientist\"\n",
    "company = \"Tech Corp\"\n",
    "job_description = \"\"\"\n",
    "Looking for a data scientist with strong Python skills, experience with machine learning,\n",
    "SQL databases, and data visualization. Must have excellent analytical and problem-solving\n",
    "abilities with strong communication skills.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nğŸ“‹ Job: {job_title} at {company}\")\n",
    "print(f\"ğŸ“ Requirements: Python, ML, SQL, data viz, analytical thinking, communication\\n\")\n",
    "\n",
    "# 2. PHASE 1: RETRIEVAL\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: RETRIEVAL (Vector Similarity Search)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create query embedding\n",
    "query_text = f\"{job_title} {company} {job_description}\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "# Retrieve work experience from RESUME_DATA collection\n",
    "print(f\"\\nğŸ” Searching resume_data collection for work achievements...\")\n",
    "work_results = client.query_points(\n",
    "    collection_name=resume_collection,\n",
    "    query=query_vector,\n",
    "    query_filter=Filter(\n",
    "        must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"work_experience\"))]\n",
    "    ),\n",
    "    limit=10\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Retrieved {len(work_results)} relevant work achievements:\")\n",
    "for i, result in enumerate(work_results[:5], 1):\n",
    "    metadata = result.payload.get('metadata', {})\n",
    "    print(f\"   {i}. [{result.score:.3f}] {metadata.get('company')} - {result.payload.get('content', '')[:60]}...\")\n",
    "\n",
    "# Retrieve relevant projects from PROJECTS collection\n",
    "print(f\"\\nğŸ“‚ Searching projects collection for matching portfolio work...\")\n",
    "projects_results = client.query_points(\n",
    "    collection_name=projects_collection,\n",
    "    query=query_vector,\n",
    "    query_filter=Filter(\n",
    "        must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    "    ),\n",
    "    limit=3\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Retrieved {len(projects_results)} relevant projects:\")\n",
    "for i, result in enumerate(projects_results, 1):\n",
    "    metadata = result.payload.get('metadata', {})\n",
    "    print(f\"   {i}. [{result.score:.3f}] {metadata.get('project_title')} - Tech: {', '.join(metadata.get('tech_stack', [])[:3])}\")\n",
    "\n",
    "# Retrieve personality traits from PERSONALITY collection\n",
    "job_analysis = {\n",
    "    'soft_skills': ['analytical', 'problem-solving', 'communication'],\n",
    "    'keywords': ['data-driven', 'collaborative']\n",
    "}\n",
    "\n",
    "personality_query = ' '.join(job_analysis['soft_skills'] + job_analysis['keywords'])\n",
    "personality_vector = embedder.embed_query(personality_query)\n",
    "\n",
    "print(f\"\\nğŸ§  Searching personality collection for matching traits...\")\n",
    "personality_results = client.query_points(\n",
    "    collection_name=personality_collection,\n",
    "    query=personality_vector,\n",
    "    limit=10\n",
    ").points\n",
    "\n",
    "# Filter for personality/strength (exclude weaknesses)\n",
    "personality_filtered = [\n",
    "    r for r in personality_results \n",
    "    if r.payload.get('section_type') in ['personality', 'strength']\n",
    "][:5]\n",
    "\n",
    "print(f\"âœ… Retrieved {len(personality_filtered)} personality traits:\")\n",
    "for i, result in enumerate(personality_filtered, 1):\n",
    "    print(f\"   {i}. [{result.score:.3f}] {result.payload.get('content', '')[:60]}...\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Architecture Benefits:\")\n",
    "print(f\"   âœ“ Resume, projects, and personality queries run independently\")\n",
    "print(f\"   âœ“ No cross-contamination between collections\")\n",
    "print(f\"   âœ“ Faster searches (smaller, focused collections)\")\n",
    "print(f\"   âœ“ Hierarchical project retrieval (technical summaries first)\")\n",
    "\n",
    "# 3. PHASE 2: AUGMENTATION\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 2: AUGMENTATION (Combine Context)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ… Would combine:\")\n",
    "print(f\"   - Job requirements: {job_title}, Python, ML, SQL...\")\n",
    "print(f\"   - {len(work_results[:5])} work achievements (from resume_data collection)\")\n",
    "print(f\"   - {len(projects_results)} portfolio projects (from projects collection)\")\n",
    "print(f\"   - {len(personality_filtered)} personality traits (from personality collection)\")\n",
    "print(\"   - Into a structured prompt for Claude\")\n",
    "\n",
    "# 4. PHASE 3: GENERATION\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 3: GENERATION (Claude LLM)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ… Would call Claude API with augmented prompt to generate:\")\n",
    "print(\"   - Tailored resume sections\")\n",
    "print(\"   - Personalized cover letter\")\n",
    "print(\"   - Using ONLY the retrieved context from all three collections\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… RAG WORKFLOW COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721c8f5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the **triple-collection architecture** with comprehensive querying patterns:\n",
    "\n",
    "### Collections Overview\n",
    "\n",
    "1. **`resume_data`**: Work experience, education, skills, continuing studies, personal info\n",
    "2. **`personality`**: Personality traits with fixed-size chunking (400 chars, 100 overlap)\n",
    "3. **`projects`**: Portfolio projects with hierarchical chunking (technical summaries + full content)\n",
    "\n",
    "### Key Features Demonstrated\n",
    "\n",
    "#### Resume Collection\n",
    "- Section-based chunking with rich metadata (company, position, dates)\n",
    "- Filtered queries by section_type (work_experience, education, skills)\n",
    "- Semantic search for relevant achievements\n",
    "\n",
    "#### Personality Collection\n",
    "- Simplified fixed-size chunking without complex section parsing\n",
    "- Direct semantic search without filtering\n",
    "- Efficient retrieval of matching traits\n",
    "\n",
    "#### Projects Collection (NEW)\n",
    "- **Hierarchical chunking**: Each project has 2 chunks\n",
    "  - `project_technical`: Tech Stack + Technical Highlights (fast matching)\n",
    "  - `project_full`: Complete project with all sections (detailed context)\n",
    "- **Metadata-based retrieval**: Query by `project_id` to get specific projects\n",
    "- **Two-phase workflow**:\n",
    "  1. Search technical summaries for matches\n",
    "  2. Retrieve full content using project_ids\n",
    "- **Benefits**:\n",
    "  - Fast filtering by technology stack\n",
    "  - Efficient token usage (only embed full content when needed)\n",
    "  - Clear separation of filtering vs. detailed context\n",
    "\n",
    "### Query Patterns Explored\n",
    "\n",
    "1. **Collection structure inspection**: View document counts and section types\n",
    "2. **Filtered scrolling**: Retrieve documents by section_type or metadata\n",
    "3. **Semantic search**: Find similar content using embeddings\n",
    "4. **Combined filters**: Semantic search + metadata filtering\n",
    "5. **Metadata queries**: Exact match on specific fields (e.g., project_id)\n",
    "6. **Multi-phase retrieval**: Search summaries â†’ retrieve full content\n",
    "\n",
    "### Architecture Benefits\n",
    "\n",
    "- âœ… **Semantic separation**: Resume, projects, and personality data stored independently\n",
    "- âœ… **Hierarchical retrieval**: Fast matching â†’ detailed context (projects)\n",
    "- âœ… **No cross-contamination**: Queries return only relevant collection data\n",
    "- âœ… **Faster searches**: Smaller, focused collections improve performance\n",
    "- âœ… **Flexible workflows**: Different retrieval patterns per collection\n",
    "- âœ… **Efficient tokens**: Don't embed full project content for initial search\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Resume generation**: Retrieve relevant work achievements + matching projects + personality traits\n",
    "- **Cover letter**: Search all collections for job-specific content\n",
    "- **Portfolio showcase**: Find projects by technology stack or technical requirements\n",
    "- **Skills matching**: Semantic search across resume, projects, and personality\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Run cells to explore your actual triple-collection database\n",
    "- Experiment with different query combinations\n",
    "- Test hierarchical project retrieval workflow\n",
    "- Try metadata-based queries for specific projects\n",
    "- Combine results from multiple collections for complete RAG workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Resume_Claude_SDK_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
