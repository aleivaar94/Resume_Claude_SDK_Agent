{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099a9b12",
   "metadata": {},
   "source": [
    "# Advanced Vector Store Queries\n",
    "\n",
    "This notebook demonstrates detailed querying of the Qdrant vector store with **dual-collection architecture**:\n",
    "- **`resume_data` collection** (from `resume_ale.md`): work experience, education, skills, continuing studies, personal info\n",
    "- **`personality` collection** (from `personalities_16.md`): personality traits, strengths, weaknesses\n",
    "\n",
    "**New Architecture Benefits:**\n",
    "- Semantic separation of resume facts vs personality traits\n",
    "- Faster queries (smaller, focused collections)\n",
    "- No cross-contamination between resume and personality data\n",
    "\n",
    "We'll explore:\n",
    "1. Collection metadata and structure (BOTH collections)\n",
    "2. Filtering by section type within collections\n",
    "3. Viewing embeddings and payloads\n",
    "4. Semantic search examples (separate collection queries)\n",
    "5. Specific queries for resume vs personality data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9a043",
   "metadata": {},
   "source": [
    "## 1. Initialize Vector Store Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105c3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Qdrant vector store\n",
      "ğŸ“‚ Storage path: c:\\Users\\Ale\\Documents\\Data-Science-Projects\\GitHub\\Resume_Claude_SDK_Agent\\notebooks\\..\\vector_db\\qdrant_storage\n",
      "\n",
      "ğŸ“¦ Collections:\n",
      "   - resume_data (resume content)\n",
      "   - personality (personality traits)\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Initialize Qdrant client with local storage\n",
    "storage_path = \"../vector_db/qdrant_storage\"\n",
    "client = QdrantClient(path=storage_path)\n",
    "\n",
    "# Collection names (dual-collection architecture)\n",
    "resume_collection = \"resume_data\"\n",
    "personality_collection = \"personality\"\n",
    "\n",
    "print(\"âœ… Connected to Qdrant vector store\")\n",
    "print(f\"ğŸ“‚ Storage path: {Path(storage_path).absolute()}\")\n",
    "print(f\"\\nğŸ“¦ Collections:\")\n",
    "print(f\"   - {resume_collection} (resume content)\")\n",
    "print(f\"   - {personality_collection} (personality traits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c14602",
   "metadata": {},
   "source": [
    "## 2. Explore Collection Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6215e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Available Collections:\n",
      "   - resume_data\n",
      "   - personality\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Collection 'resume_data' Details:\n",
      "   Total documents: 35\n",
      "   Vector dimensions: 1536\n",
      "   Distance metric: Cosine\n",
      "   Status: green\n",
      "\n",
      "   ğŸ“ˆ Documents by Section Type:\n",
      "      continuing_studies  :   7 chunks\n",
      "      education           :   2 chunks\n",
      "      personal_info       :   1 chunks\n",
      "      professional_summary:   1 chunks\n",
      "      skills              :   5 chunks\n",
      "      work_experience     :  19 chunks\n",
      "   ----------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Š Collection 'personality' Details:\n",
      "   Total documents: 14\n",
      "   Vector dimensions: 1536\n",
      "   Distance metric: Cosine\n",
      "   Status: green\n",
      "\n",
      "   ğŸ“ˆ Documents by Section Type:\n",
      "      personality         :  14 chunks\n",
      "   ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get all collections\n",
    "collections = client.get_collections()\n",
    "print(\"ğŸ“š Available Collections:\")\n",
    "for collection in collections.collections:\n",
    "    print(f\"   - {collection.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Explore BOTH collections\n",
    "for collection_name in [resume_collection, personality_collection]:\n",
    "    if client.collection_exists(collection_name):\n",
    "        collection_info = client.get_collection(collection_name)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Collection '{collection_name}' Details:\")\n",
    "        print(f\"   Total documents: {collection_info.points_count}\")\n",
    "        print(f\"   Vector dimensions: {collection_info.config.params.vectors.size}\")\n",
    "        print(f\"   Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "        print(f\"   Status: {collection_info.status}\")\n",
    "        \n",
    "        # Count by section type\n",
    "        from collections import Counter\n",
    "        all_records, _ = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=1000,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        \n",
    "        section_counts = Counter(r.payload.get('section_type', 'unknown') for r in all_records)\n",
    "        \n",
    "        print(f\"\\n   ğŸ“ˆ Documents by Section Type:\")\n",
    "        for section, count in sorted(section_counts.items()):\n",
    "            print(f\"      {section:20s}: {count:3d} chunks\")\n",
    "        \n",
    "        print(\"   \" + \"-\"*76)\n",
    "    else:\n",
    "        print(f\"\\nâŒ Collection '{collection_name}' not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b793ad",
   "metadata": {},
   "source": [
    "## 3. Query Resume Data (from resume_ale.md)\n",
    "\n",
    "### 3.1 View Work Experience with Full Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28af347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¼ Work Experience Chunks from 'resume_data' collection (showing 19):\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1 - ID: 11e55900-961b-449b-88e5-080429d6688a\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Implemented a Python algorithm to automatically select sampling plans, reducing inspector manual work by 3 hours per inspector per day and generating approximately CAD 4,000,000 in annual savings by translating business rules into an automated algorithm.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 2 - ID: 1a038c02-a181-4bf5-88a4-fb38dcea017a\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Standardized descriptive and statistical reporting in Power BI, reducing report-generation time and improving inspection efficiency by creating templated reports and automated data queries.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 3 - ID: 2daf0f00-444c-44e2-80e0-7bc02d9373ae\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Analyzed pathogen occurrence trends across 50 facilities by performing statistical analysis with SQL and Power BI and collaborating with inspectors to implement preventive inspections.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 4 - ID: 2e9b2568-2f36-4899-b0e9-80cf637db5c6\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Analyst: Designed and implemented a reporting tool to pinpoint SKU opportunities at store level, contributing to a 12% increase in store-level sales by analyzing SKU performance with SQL and presenting results in Power BI.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Rubicon Organics\n",
      "   Position:       Data Analyst\n",
      "   Start Date:     March-2023\n",
      "   End Date:       December-2023\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 5 - ID: 400f8648-82db-4bc2-a624-745775b0f39b\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Automated forecasting and reduced manual effort by 40 hours per month by deploying the forecasting pipeline and scheduling automated runs using Python and Microsoft Fabric.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 6 - ID: 40278e0e-1129-48ae-bd71-39ff2b201d68\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Engineer: Reduced hard-drive screw defects by 15% (â‰ˆ$110,000 annual savings) by extracting and cleaning 50,000+ records with SQL, performing statistical analysis in Excel, and applying Six Sigma root-cause methods.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        IBM\n",
      "   Position:       Quality Engineer\n",
      "   Start Date:     December-2017\n",
      "   End Date:       October-2018\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 7 - ID: 4038b2f9-b05c-4d73-898c-80532b64dc3a\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Developed and deployed anomaly-detection and time-series forecasting models achieving 86% accuracy in predicting food safety risk prevalence by training models on historical data using Python and ML techniques.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 8 - ID: 43662d81-e5bd-435d-8222-3912588ce7a4\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Implemented daily automated data refreshes, replacing weekly manual CSV exports, by gaining direct data-warehouse access and building an ETL pipeline using SQL and Microsoft Fabric to store data in a Lakehouse.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 9 - ID: 4a77459b-2913-47df-a9e5-576e5deebae8\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Analyst: Built three Power BI dashboards for sales and marketing by collaborating with stakeholders to define requirements and implementing interactive reports in Power BI.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Rubicon Organics\n",
      "   Position:       Data Analyst\n",
      "   Start Date:     March-2023\n",
      "   End Date:       December-2023\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 10 - ID: 4e082751-294c-4db1-80e6-23c3ad50f1b8\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Assurance Technician: Coordinated supply chain and production teams to ensure food safety compliance by leading cross-functional meetings and implementing compliance checks, maintaining operational continuity.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        The Very Good Food Company\n",
      "   Position:       Quality Assurance Technician\n",
      "   Start Date:     February-2021\n",
      "   End Date:       February-2022\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 11 - ID: 6a28d00d-395c-4dc0-b368-e4d29b131aab\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Engineer: Developed an Excel dashboard with automated SQL queries, reducing manual reporting time by 40% and enabling near-real-time quality monitoring by automating data extraction and transformation.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        IBM\n",
      "   Position:       Quality Engineer\n",
      "   Start Date:     December-2017\n",
      "   End Date:       October-2018\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 12 - ID: 7e1343c4-6ef0-4731-8ab7-9457d632e83f\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Automated data categorization, reducing data cleaning time by over 15 hours per week, by integrating LLM-based classification into the ETL pipeline.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 13 - ID: a7daf7f1-8afd-4135-8555-b16155a1cad6\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Developed a Power BI dashboard to track changes in imported food volumes, collaborating with import inspectors to define metrics and design visualizations in Power BI for stakeholder use.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 14 - ID: ab372e0b-134c-4e73-ab52-e8874ff06b93\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Developed and deployed a time-series forecasting model that achieved 87% accuracy in predicting monthly import/export volumes by training on historical data using Python and time-series ML techniques.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 15 - ID: b809b06d-f547-4d02-b595-604ed1df0bbb\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Analyst: Built an ETL pipeline integrating five data sources totaling over 1M records using SQL and Python, automating ingestion and cleaning and saving 8 hours weekly in data preparation.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Rubicon Organics\n",
      "   Position:       Data Analyst\n",
      "   Start Date:     March-2023\n",
      "   End Date:       December-2023\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 16 - ID: d35b251f-13cb-4289-945c-25ad9c3fcb7f\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Assurance Technician: Performed root cause analyses on non-conforming products, reducing rework and generating cost savings by applying corrective actions informed by statistical analysis in Excel.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        The Very Good Food Company\n",
      "   Position:       Quality Assurance Technician\n",
      "   Start Date:     February-2021\n",
      "   End Date:       February-2022\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 17 - ID: d9c07655-2688-412d-b630-2abf457adb97\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Assurance Technician: Automated sanitation KPI reporting using Excel and MS Forms, contributing to an 80% SQF audit score by building automated reports and forms to track sanitation metrics.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        The Very Good Food Company\n",
      "   Position:       Quality Assurance Technician\n",
      "   Start Date:     February-2021\n",
      "   End Date:       February-2022\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 18 - ID: dbd5b40f-952b-40f1-97f8-84456a794607\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Extracted and processed millions of import/export transactions by building web-scraping collectors and a PySpark ETL pipeline to load cleaned data into a Microsoft Fabric lakehouse.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 19 - ID: ef4b8b5d-0362-4b57-b43f-6c4bbb949a22\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Built an unsupervised anomaly detection model to identify potential food safety risks in imported products by training unsupervised ML models using Python and applying them to imported product datasets.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for work experience entries (from resume_data collection)\n",
    "work_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"section_type\",\n",
    "            match=MatchValue(value=\"work_experience\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "work_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,  # Query resume_data collection\n",
    "    scroll_filter=work_filter,\n",
    "    limit=20,\n",
    "    with_payload=True,\n",
    "    with_vectors=False  # Set True to see embeddings\n",
    ")\n",
    "\n",
    "print(f\"ğŸ’¼ Work Experience Chunks from '{resume_collection}' collection (showing {len(work_records)}):\\n\")\n",
    "\n",
    "for i, record in enumerate(work_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CHUNK {i} - ID: {record.id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“„ Content (Achievement):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ¢ Metadata:\")\n",
    "    print(f\"   Company:        {metadata.get('company', 'N/A')}\")\n",
    "    print(f\"   Position:       {metadata.get('position', 'N/A')}\")\n",
    "    print(f\"   Start Date:     {metadata.get('start_date', 'N/A')}\")\n",
    "    print(f\"   End Date:       {metadata.get('end_date', 'N/A')}\")\n",
    "    print(f\"   Source File:    {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"   Section Type:   {payload.get('section_type', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ee076",
   "metadata": {},
   "source": [
    "### 3.2 View Work Experience WITH Embeddings\n",
    "\n",
    "Each chunk has a 1536-dimensional embedding vector generated by OpenAI's `text-embedding-3-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d0755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Embedding Vector Details:\n",
      "   Vector dimensions: 1536\n",
      "   Vector type: <class 'list'>\n",
      "   First 10 values: [0.02034231647849083, -0.03654035925865173, 0.06752133369445801, -0.015060896053910255, -0.008598103187978268, -0.007789465133100748, -0.008636008948087692, -0.007606257684528828, -0.010771320201456547, 0.0278222244232893]\n",
      "   Last 10 values:  [0.0033703807275742292, 0.006867111194878817, 0.008636008948087692, 0.016122233122587204, 0.0037968114484101534, -0.0016251743072643876, 0.022692423313856125, -0.04225137084722519, 0.027266286313533783, 0.02266715280711651]\n",
      "\n",
      "ğŸ“Š Vector Statistics:\n",
      "   Min value:  -0.123924\n",
      "   Max value:  0.070857\n",
      "   Mean value: 0.000012\n",
      "   Std dev:    0.025516\n",
      "\n",
      "ğŸ“„ Associated Content:\n",
      "   Data Scientist II: Implemented a Python algorithm to automatically select sampling plans, reducing inspector manual work by 3 hours per inspector per ...\n"
     ]
    }
   ],
   "source": [
    "# Get one work experience record WITH embeddings\n",
    "work_with_vector, _ = client.scroll(\n",
    "    collection_name=resume_collection,\n",
    "    scroll_filter=work_filter,\n",
    "    limit=20,\n",
    "    with_payload=True,\n",
    "    with_vectors=True  # Include embeddings\n",
    ")\n",
    "\n",
    "if work_with_vector:\n",
    "    record = work_with_vector[0]\n",
    "    vector = record.vector\n",
    "    \n",
    "    print(f\"ğŸ”¢ Embedding Vector Details:\")\n",
    "    print(f\"   Vector dimensions: {len(vector)}\")\n",
    "    print(f\"   Vector type: {type(vector)}\")\n",
    "    print(f\"   First 10 values: {vector[:10]}\")\n",
    "    print(f\"   Last 10 values:  {vector[-10:]}\")\n",
    "    print(f\"\\nğŸ“Š Vector Statistics:\")\n",
    "    import numpy as np\n",
    "    vector_array = np.array(vector)\n",
    "    print(f\"   Min value:  {vector_array.min():.6f}\")\n",
    "    print(f\"   Max value:  {vector_array.max():.6f}\")\n",
    "    print(f\"   Mean value: {vector_array.mean():.6f}\")\n",
    "    print(f\"   Std dev:    {vector_array.std():.6f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Associated Content:\")\n",
    "    print(f\"   {record.payload.get('content', 'N/A')[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058d26e",
   "metadata": {},
   "source": [
    "### 3.3 Query Education & Skills Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Education Entries from 'resume_data' collection (2):\n",
      "\n",
      "================================================================================\n",
      "EDUCATION CHUNK 1\n",
      "================================================================================\n",
      "ğŸ“ Degree:        MSc in Food Science\n",
      "ğŸ« Institution:   University of British Columbia\n",
      "ğŸ“… Year:          N/A\n",
      "ğŸ“‚ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  education\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   MSc in Food Science from University of British Columbia. January-2019 - October-2020 | Canada\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"degree\": \"MSc in Food Science\",\n",
      "  \"institution\": \"University of British Columbia\",\n",
      "  \"dates\": \"January-2019 - October-2020 | Canada\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "EDUCATION CHUNK 2\n",
      "================================================================================\n",
      "ğŸ“ Degree:        BSc in Biotechnology Engineering\n",
      "ğŸ« Institution:   Tec de Monterrey\n",
      "ğŸ“… Year:          N/A\n",
      "ğŸ“‚ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  education\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   BSc in Biotechnology Engineering from Tec de Monterrey. August-2012 - May-2017 | Mexico\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"degree\": \"BSc in Biotechnology Engineering\",\n",
      "  \"institution\": \"Tec de Monterrey\",\n",
      "  \"dates\": \"August-2012 - May-2017 | Mexico\"\n",
      "}\n",
      "\n",
      "\n",
      "ğŸ› ï¸  Skills Entries from 'resume_data' collection (5):\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 1\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Methodologies\n",
      "ğŸ“„ Skills:        Methodologies: Agile, Six Sigma\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Methodologies\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 2\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Cloud Platforms\n",
      "ğŸ“„ Skills:        Cloud Platforms: Azure, Google Cloud\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Cloud Platforms\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 3\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Development Tools\n",
      "ğŸ“„ Skills:        Development Tools: Git, Docker, Azure DevOps, VS Code, API\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Development Tools\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 4\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Programming Languages\n",
      "ğŸ“„ Skills:        Programming Languages: Python, SQL, PySpark, T-SQL\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Programming Languages\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 5\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Business Intelligence\n",
      "ğŸ“„ Skills:        Business Intelligence: Power BI, Microsoft Fabric, OneLake, Power Query, Power Pivot, Excel, Delta Lake\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Business Intelligence\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query education entries (from resume_data collection)\n",
    "education_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"education\"))]\n",
    ")\n",
    "\n",
    "education_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    scroll_filter=education_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“ Education Entries from '{resume_collection}' collection ({len(education_records)}):\\n\")\n",
    "for i, record in enumerate(education_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"EDUCATION CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Degree:        {metadata.get('degree', 'N/A')}\")\n",
    "    print(f\"ğŸ« Institution:   {metadata.get('institution', 'N/A')}\")\n",
    "    print(f\"ğŸ“… Year:          {metadata.get('year', 'N/A')}\")\n",
    "    print(f\"ğŸ“‚ Source File:   {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type:  {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()\n",
    "\n",
    "# Query skills (from resume_data collection)\n",
    "skills_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"skills\"))]\n",
    ")\n",
    "\n",
    "skills_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,\n",
    "    scroll_filter=skills_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ› ï¸  Skills Entries from '{resume_collection}' collection ({len(skills_records)}):\\n\")\n",
    "for i, record in enumerate(skills_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"SKILL CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“‚ Category:      {metadata.get('category', 'N/A')}\")\n",
    "    print(f\"ğŸ“„ Skills:        {payload.get('content', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Source File:   {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type:  {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c60b4",
   "metadata": {},
   "source": [
    "## 4. Query Personality Traits Data (from personalities_16.md)\n",
    "\n",
    "### 4.1 View Personality Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d24204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Personality Trait Chunks from 'personality' collection (14):\n",
      "\n",
      "ğŸ’¡ Note: This collection contains ONLY personality data (no resume content)\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 1\n",
      "================================================================================\n",
      "ğŸ“ Section:       Big-Picture Focus\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   I prefer focusing on overarching goals and strategies rather than micromanaging small details.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Big-Picture Focus\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 2\n",
      "================================================================================\n",
      "ğŸ“ Section:       Conceptual Thinking\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   I effortlessly grasp abstract, complex ideas, making me particularly suited to roles that require strategic analysis and long-term planning.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Conceptual Thinking\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 3\n",
      "================================================================================\n",
      "ğŸ“ Section:       Reluctance to Delegate Tasks\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   Believing strongly in my own abilities, I often hesitate to entrust responsibilities to others, leading to stress or unnecessary workload.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Reluctance to Delegate Tasks\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 4\n",
      "================================================================================\n",
      "ğŸ“ Section:       Continuous Improvement\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   I naturally focus on refining work processes and spotting inefficiencies, consistently improving project outcomes wherever I go.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Continuous Improvement\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 5\n",
      "================================================================================\n",
      "ğŸ“ Section:       Objective Judgment\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   My capacity to make impartial decisions based on facts rather than favoritism or personal bias earns respect and trust from my colleagues.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Objective Judgment\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 6\n",
      "================================================================================\n",
      "ğŸ“ Section:       Reliable Performance\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   When entrusted with critical tasks, I consistently deliver precise, high-quality results, making me a valued and dependable asset.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Reliable Performance\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 7\n",
      "================================================================================\n",
      "ğŸ“ Section:       Ignoring Social Dynamics\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   I tend to neglect office politics and informal social interactions, possibly missing cues or causing unintended misunderstandings.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Ignoring Social Dynamics\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 8\n",
      "================================================================================\n",
      "ğŸ“ Section:       Impatience with Routine\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   I often feel restless when assigned tasks that seem repetitive or mundane, leading to occasional lapses in my attention or motivation.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Impatience with Routine\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 9\n",
      "================================================================================\n",
      "ğŸ“ Section:       Overly Blunt Feedback\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   In my pursuit of truth and efficiency, I may deliver criticisms in ways that unintentionally demotivate or upset sensitive colleagues.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Overly Blunt Feedback\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 10\n",
      "================================================================================\n",
      "ğŸ“ Section:       Discomfort with Networking\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   My aversion to promoting myself or making connections can limit career advancement opportunities, hiding my true worth from others.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Discomfort with Networking\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 11\n",
      "================================================================================\n",
      "ğŸ“ Section:       Goal-Oriented\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   I stay motivated by clear goals and visible progress, consistently tracking achievements and identifying next steps. # Weaknesses My preference for working independently and my dislike for office politics can sometimes hinder my career progression. I need to learn to navigate social dynamics and communicate my ideas effectively to others for my professional growth. My unique insights are most valuable when they can be implemented, which often requires collaboration and buy-in from others.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Goal-Oriented\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 12\n",
      "================================================================================\n",
      "ğŸ“ Section:       Frustration with Constraints\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   I chafe at rules or procedures I deem pointless, potentially straining relationships with supervisors or organizational hierarchy.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Frustration with Constraints\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 13\n",
      "================================================================================\n",
      "ğŸ“ Section:       Innovative Mindset\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   My ability to see possibilities others overlook often helps me find smarter solutions and effective improvements at work.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Innovative Mindset\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 14\n",
      "================================================================================\n",
      "ğŸ“ Section:       Independent Worker\n",
      "ğŸ“‚ Source File:   personalities_16.md\n",
      "ğŸ·ï¸  Section Type:  personality\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   My talent for working productively on my own allows me to manage tasks effectively without the need for constant direction or supervision.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"section\": \"Independent Worker\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personality_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"\"))]\n",
    ")\n",
    "\n",
    "personality_records, _ = client.scroll(\n",
    "    collection_name=personality_collection,\n",
    "    scroll_filter=personality_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"ğŸ§  Personality Trait Chunks from '{personality_collection}' collection ({len(personality_records)}):\\n\")\n",
    "print(f\"ğŸ’¡ Note: This collection contains ONLY personality data with simplified fixed-size chunking\\n\")\n",
    "\n",
    "for i, record in enumerate(personality_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PERSONALITY CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Chunk Index: {metadata.get('chunk_index', 'N/A')}\")\n",
    "    print(f\"ğŸ“‚ Source File: {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Character Range: {metadata.get('char_start', 'N/A')} - {metadata.get('char_end', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b01149",
   "metadata": {},
   "source": [
    "### 4.2 View All Personality Chunks\n",
    "\n",
    "With simplified fixed-size chunking, all chunks in the personality collection are treated equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfb16d",
   "metadata": {},
   "source": [
    "## 5. Semantic Search Examples\n",
    "\n",
    "### 5.1 Search for Python-Related Work Experience\n",
    "\n",
    "This demonstrates how semantic search works with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f9e2eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Semantic Search Query: 'Python data analysis ETL pipeline machine learning'\n",
      "   Query vector dimensions: 1536\n",
      "   Searching in: resume_data collection\n",
      "\n",
      "ğŸ“Š Top 4 Results (by semantic similarity):\n",
      "\n",
      "================================================================================\n",
      "RESULT 1 - Similarity Score: 0.6346\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Analyst: Built an ETL pipeline integrating five data sources totaling over 1M records using SQL and Python, automating ingestion and cleaning and saving 8 hours weekly in data preparation.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Rubicon Organics\n",
      "   Position: Data Analyst\n",
      "\n",
      "================================================================================\n",
      "RESULT 2 - Similarity Score: 0.5538\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Scientist II: Extracted and processed millions of import/export transactions by building web-scraping collectors and a PySpark ETL pipeline to load cleaned data into a Microsoft Fabric lakehouse.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Canadian Food Inspection Agency\n",
      "   Position: Data Scientist II\n",
      "\n",
      "================================================================================\n",
      "RESULT 3 - Similarity Score: 0.5154\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Scientist II: Automated data categorization, reducing data cleaning time by over 15 hours per week, by integrating LLM-based classification into the ETL pipeline.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Canadian Food Inspection Agency\n",
      "   Position: Data Scientist II\n",
      "\n",
      "================================================================================\n",
      "RESULT 4 - Similarity Score: 0.5105\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Scientist II: Implemented daily automated data refreshes, replacing weekly manual CSV exports, by gaining direct data-warehouse access and building an ETL pipeline using SQL and Microsoft Fabric to store data in a Lakehouse.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Canadian Food Inspection Agency\n",
      "   Position: Data Scientist II\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import OpenAI embeddings to create query vectors\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.core.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize embedder\n",
    "embedder = OpenAIEmbeddings()\n",
    "\n",
    "# Create a query for Python-related achievements\n",
    "query_text = \"Python data analysis ETL pipeline machine learning\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "print(f\"ğŸ” Semantic Search Query: '{query_text}'\")\n",
    "print(f\"   Query vector dimensions: {len(query_vector)}\")\n",
    "print(f\"   Searching in: {resume_collection} collection\")\n",
    "\n",
    "# Search with vector similarity using query_points (newer API)\n",
    "results = client.query_points(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    query=query_vector,\n",
    "    limit=5,\n",
    "    score_threshold=0.5  # Only return results with similarity > 0.5\n",
    ").points\n",
    "\n",
    "print(f\"\\nğŸ“Š Top {len(results)} Results (by semantic similarity):\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"RESULT {i} - Similarity Score: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“„ Content: {payload.get('content', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    if payload.get('section_type') == 'work_experience':\n",
    "        print(f\"   Company: {metadata.get('company', 'N/A')}\")\n",
    "        print(f\"   Position: {metadata.get('position', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe440754",
   "metadata": {},
   "source": [
    "### 5.2 Search for Personality Traits Matching Job Requirements\n",
    "\n",
    "**NEW: Direct query to personality collection (no filtering needed!)**\n",
    "\n",
    "This mimics how `retrieve_personality_traits()` works in the resume generator with the new architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114100a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Job Requirements Query: 'analytical thinking problem-solving collaboration strategic innovative team player'\n",
      "   Searching in: personality collection (NEW!)\n",
      "\n",
      "âœ… Retrieved 10 results from personality collection\n",
      "ğŸ“Š After filtering weaknesses: 10 Personality/Strength Traits\n",
      "\n",
      "ğŸ’¡ Benefits of separate collection:\n",
      "   - No work experience contamination (used to get mixed in old architecture)\n",
      "   - Faster search (14 docs vs 49)\n",
      "   - Cleaner semantic space\n",
      "\n",
      "================================================================================\n",
      "TRAIT 1 - Similarity: 0.4947\n",
      "================================================================================\n",
      "ğŸ·ï¸  Type: personality\n",
      "ğŸ“ Section: Conceptual Thinking\n",
      "ğŸ“„ Content:\n",
      "   I effortlessly grasp abstract, complex ideas, making me particularly suited to roles that require strategic analysis and long-term planning.\n",
      "\n",
      "================================================================================\n",
      "TRAIT 2 - Similarity: 0.4716\n",
      "================================================================================\n",
      "ğŸ·ï¸  Type: personality\n",
      "ğŸ“ Section: Innovative Mindset\n",
      "ğŸ“„ Content:\n",
      "   My ability to see possibilities others overlook often helps me find smarter solutions and effective improvements at work.\n",
      "\n",
      "================================================================================\n",
      "TRAIT 3 - Similarity: 0.4081\n",
      "================================================================================\n",
      "ğŸ·ï¸  Type: personality\n",
      "ğŸ“ Section: Goal-Oriented\n",
      "ğŸ“„ Content:\n",
      "   I stay motivated by clear goals and visible progress, consistently tracking achievements and identifying next steps. # Weaknesses My preference for working independently and my dislike for office politics can sometimes hinder my career progression. I need to learn to navigate social dynamics and communicate my ideas effectively to others for my professional growth. My unique insights are most valuable when they can be implemented, which often requires collaboration and buy-in from others.\n",
      "\n",
      "================================================================================\n",
      "TRAIT 4 - Similarity: 0.3796\n",
      "================================================================================\n",
      "ğŸ·ï¸  Type: personality\n",
      "ğŸ“ Section: Continuous Improvement\n",
      "ğŸ“„ Content:\n",
      "   I naturally focus on refining work processes and spotting inefficiencies, consistently improving project outcomes wherever I go.\n",
      "\n",
      "================================================================================\n",
      "TRAIT 5 - Similarity: 0.3725\n",
      "================================================================================\n",
      "ğŸ·ï¸  Type: personality\n",
      "ğŸ“ Section: Reliable Performance\n",
      "ğŸ“„ Content:\n",
      "   When entrusted with critical tasks, I consistently deliver precise, high-quality results, making me a valued and dependable asset.\n",
      "\n",
      "\n",
      "ğŸ’¡ These traits would be injected into the cover letter prompt!\n"
     ]
    }
   ],
   "source": [
    "# Simulate a job analysis with soft skills and keywords\n",
    "job_analysis = {\n",
    "    'soft_skills': ['analytical thinking', 'problem-solving', 'collaboration'],\n",
    "    'keywords': ['strategic', 'innovative', 'team player']\n",
    "}\n",
    "\n",
    "# Build query (same logic as retrieve_personality_traits)\n",
    "query_parts = job_analysis.get('soft_skills', []) + job_analysis.get('keywords', [])\n",
    "query_text = ' '.join(query_parts)\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "print(f\"ğŸ” Job Requirements Query: '{query_text}'\")\n",
    "print(f\"   Searching in: {personality_collection} collection (NEW!)\\n\")\n",
    "\n",
    "# Search the PERSONALITY collection directly (no filtering needed!)\n",
    "all_results = client.query_points(\n",
    "    collection_name=personality_collection,  # â† Query personality collection directly!\n",
    "    query=query_vector,\n",
    "    limit=10\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Retrieved {len(all_results)} results from personality collection\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 5 Personality Trait Chunks by Semantic Similarity:\\n\")\n",
    "print(f\"ğŸ’¡ Benefits of simplified chunking:\")\n",
    "print(f\"   - Pure semantic search without complex filtering\")\n",
    "print(f\"   - Fixed-size chunks maintain consistent context windows\")\n",
    "print(f\"   - Faster search (smaller collection)\\n\")\n",
    "\n",
    "for i, result in enumerate(all_results[:5], 1):  # Top 5\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAIT {i} - Similarity: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Chunk Index: {metadata.get('chunk_index', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Characters: {metadata.get('char_start', 'N/A')} - {metadata.get('char_end', 'N/A')}\")\n",
    "    print(f\"ğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ These traits would be deduplicated (removing 100-char overlaps) and injected into the cover letter prompt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b67c13",
   "metadata": {},
   "source": [
    "### 5.3 Semantic Search with Section Filtering\n",
    "\n",
    "Combine semantic search with metadata filters for precise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "096b45de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: 'data science machine learning SQL Python dashboard visualization'\n",
      "ğŸ“¦ Collection: resume_data\n",
      "ğŸ¯ Filter: section_type = 'work_experience'\n",
      "\n",
      "ğŸ“Š Top 5 Work Achievements:\n",
      "\n",
      "1. [Score: 0.5813] Canadian Food Inspection Agency - Data Scientist\n",
      "   Data Scientist: Standardized descriptive and statistical reporting in Power BI, reducing report-gene...\n",
      "\n",
      "2. [Score: 0.5107] Rubicon Organics - Data Analyst\n",
      "   Data Analyst: Built an ETL pipeline integrating five data sources totaling over 1M records using SQL...\n",
      "\n",
      "3. [Score: 0.5098] Rubicon Organics - Data Analyst\n",
      "   Data Analyst: Built three Power BI dashboards for sales and marketing by collaborating with stakehol...\n",
      "\n",
      "4. [Score: 0.4948] Canadian Food Inspection Agency - Data Scientist II\n",
      "   Data Scientist II: Automated forecasting and reduced manual effort by 40 hours per month by deployin...\n",
      "\n",
      "5. [Score: 0.4914] Rubicon Organics - Data Analyst\n",
      "   Data Analyst: Designed and implemented a reporting tool to pinpoint SKU opportunities at store level...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for data science achievements ONLY in work experience (resume_data collection)\n",
    "query_text = \"data science machine learning SQL Python dashboard visualization\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "# Apply filter to only search work_experience\n",
    "work_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"work_experience\"))]\n",
    ")\n",
    "\n",
    "results = client.query_points(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    query=query_vector,\n",
    "    query_filter=work_filter,  # â† Apply filter during search\n",
    "    limit=5\n",
    ").points\n",
    "\n",
    "print(f\"ğŸ” Query: '{query_text}'\")\n",
    "print(f\"ğŸ“¦ Collection: {resume_collection}\")\n",
    "print(f\"ğŸ¯ Filter: section_type = 'work_experience'\")\n",
    "print(f\"\\nğŸ“Š Top {len(results)} Work Achievements:\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{i}. [Score: {result.score:.4f}] {metadata.get('company', 'N/A')} - {metadata.get('position', 'N/A')}\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518991b",
   "metadata": {},
   "source": [
    "## 6. Complete RAG Workflow Example (NEW Dual-Collection Architecture)\n",
    "\n",
    "This demonstrates the updated retrieval flow using **separate collections** for resume and personality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6753e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE RAG WORKFLOW: Data Scientist Position\n",
      "(Using NEW Dual-Collection Architecture)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Job: Senior Data Scientist at Tech Corp\n",
      "ğŸ“ Requirements: Python, ML, SQL, data viz, analytical thinking, communication\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: RETRIEVAL (Vector Similarity Search)\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Searching resume_data collection for work achievements...\n",
      "âœ… Retrieved 10 relevant work achievements:\n",
      "   1. [0.454] Canadian Food Inspection Agency - Data Scientist II: Extracted and processed millions of impor...\n",
      "   2. [0.448] Canadian Food Inspection Agency - Data Scientist: Standardized descriptive and statistical rep...\n",
      "   3. [0.447] Canadian Food Inspection Agency - Data Scientist II: Implemented daily automated data refreshe...\n",
      "   4. [0.441] Canadian Food Inspection Agency - Data Scientist: Analyzed pathogen occurrence trends across 5...\n",
      "   5. [0.435] Rubicon Organics - Data Analyst: Built an ETL pipeline integrating five data so...\n",
      "\n",
      "ğŸ§  Searching personality collection for matching traits...\n",
      "âœ… Retrieved 5 personality traits:\n",
      "   1. [0.350] My ability to see possibilities others overlook often helps ...\n",
      "   2. [0.338] I stay motivated by clear goals and visible progress, consis...\n",
      "   3. [0.329] I effortlessly grasp abstract, complex ideas, making me part...\n",
      "   4. [0.319] In my pursuit of truth and efficiency, I may deliver critici...\n",
      "   5. [0.283] I naturally focus on refining work processes and spotting in...\n",
      "\n",
      "ğŸ’¡ Architecture Benefits:\n",
      "   âœ“ Resume and personality queries run independently\n",
      "   âœ“ No cross-contamination (personality search can't return work achievements)\n",
      "   âœ“ Faster searches (smaller collections)\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: AUGMENTATION (Combine Context)\n",
      "================================================================================\n",
      "\n",
      "âœ… Would combine:\n",
      "   - Job requirements: Senior Data Scientist, Python, ML, SQL...\n",
      "   - 5 work achievements (from resume_data collection)\n",
      "   - 5 personality traits (from personality collection)\n",
      "   - Into a structured prompt for Claude\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: GENERATION (Claude LLM)\n",
      "================================================================================\n",
      "\n",
      "âœ… Would call Claude API with augmented prompt to generate:\n",
      "   - Tailored resume sections\n",
      "   - Personalized cover letter\n",
      "   - Using ONLY the retrieved context\n",
      "\n",
      "================================================================================\n",
      "âœ… RAG WORKFLOW COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Simulate a complete RAG workflow for a Data Scientist job\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE RAG WORKFLOW: Data Scientist Position\")\n",
    "print(\"(Using NEW Dual-Collection Architecture)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Job context\n",
    "job_title = \"Senior Data Scientist\"\n",
    "company = \"Tech Corp\"\n",
    "job_description = \"\"\"\n",
    "Looking for a data scientist with strong Python skills, experience with machine learning,\n",
    "SQL databases, and data visualization. Must have excellent analytical and problem-solving\n",
    "abilities with strong communication skills.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nğŸ“‹ Job: {job_title} at {company}\")\n",
    "print(f\"ğŸ“ Requirements: Python, ML, SQL, data viz, analytical thinking, communication\\n\")\n",
    "\n",
    "# 2. PHASE 1: RETRIEVAL\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: RETRIEVAL (Vector Similarity Search)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create query embedding\n",
    "query_text = f\"{job_title} {company} {job_description}\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "# Retrieve work experience from RESUME_DATA collection\n",
    "print(f\"\\nğŸ” Searching resume_data collection for work achievements...\")\n",
    "work_results = client.query_points(\n",
    "    collection_name=resume_collection,  # â† Query resume collection\n",
    "    query=query_vector,\n",
    "    query_filter=Filter(\n",
    "        must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"work_experience\"))]\n",
    "    ),\n",
    "    limit=10\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Retrieved {len(work_results)} relevant work achievements:\")\n",
    "for i, result in enumerate(work_results[:5], 1):\n",
    "    metadata = result.payload.get('metadata', {})\n",
    "    print(f\"   {i}. [{result.score:.3f}] {metadata.get('company')} - {result.payload.get('content', '')[:60]}...\")\n",
    "\n",
    "# Retrieve personality traits from PERSONALITY collection\n",
    "job_analysis = {\n",
    "    'soft_skills': ['analytical', 'problem-solving', 'communication'],\n",
    "    'keywords': ['data-driven', 'collaborative']\n",
    "}\n",
    "\n",
    "personality_query = ' '.join(job_analysis['soft_skills'] + job_analysis['keywords'])\n",
    "personality_vector = embedder.embed_query(personality_query)\n",
    "\n",
    "print(f\"\\nğŸ§  Searching personality collection for matching traits...\")\n",
    "personality_results = client.query_points(\n",
    "    collection_name=personality_collection,  # â† Query personality collection (NEW!)\n",
    "    query=personality_vector,\n",
    "    limit=10\n",
    ").points\n",
    "\n",
    "# Filter for personality/strength (exclude weaknesses)\n",
    "personality_filtered = [\n",
    "    r for r in personality_results \n",
    "    if r.payload.get('section_type') in ['personality', 'strength']\n",
    "][:5]\n",
    "\n",
    "print(f\"âœ… Retrieved {len(personality_filtered)} personality traits:\")\n",
    "for i, result in enumerate(personality_filtered, 1):\n",
    "    print(f\"   {i}. [{result.score:.3f}] {result.payload.get('content', '')[:60]}...\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Architecture Benefits:\")\n",
    "print(f\"   âœ“ Resume and personality queries run independently\")\n",
    "print(f\"   âœ“ No cross-contamination (personality search can't return work achievements)\")\n",
    "print(f\"   âœ“ Faster searches (smaller collections)\")\n",
    "\n",
    "# 3. PHASE 2: AUGMENTATION\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 2: AUGMENTATION (Combine Context)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ… Would combine:\")\n",
    "print(f\"   - Job requirements: {job_title}, Python, ML, SQL...\")\n",
    "print(f\"   - {len(work_results[:5])} work achievements (from resume_data collection)\")\n",
    "print(f\"   - {len(personality_filtered)} personality traits (from personality collection)\")\n",
    "print(\"   - Into a structured prompt for Claude\")\n",
    "\n",
    "# 4. PHASE 3: GENERATION\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 3: GENERATION (Claude LLM)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ… Would call Claude API with augmented prompt to generate:\")\n",
    "print(\"   - Tailored resume sections\")\n",
    "print(\"   - Personalized cover letter\")\n",
    "print(\"   - Using ONLY the retrieved context\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… RAG WORKFLOW COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721c8f5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the **dual-collection architecture** with **simplified personality chunking**:\n",
    "\n",
    "### What Changed\n",
    "\n",
    "**Before (Section-Aware Chunking for Personality):**\n",
    "- Personality collection used regex to identify sections (Personality Traits, Career Preferences, Strengths)\n",
    "- Complex metadata with `section_type`, `section_name`, `traits_included`\n",
    "- Required header parsing and section-based grouping\n",
    "\n",
    "**After (Simple Fixed-Size Chunking):**\n",
    "- Personality collection uses simple 400-character chunks with 100-character overlap\n",
    "- No header identification or section parsing\n",
    "- Minimal metadata: `chunk_index`, `char_start`, `char_end`, `overlap_chars`\n",
    "- No `section_type` field (empty string for personality chunks)\n",
    "- No `traits_included` metadata\n",
    "\n",
    "### Key Features Demonstrated\n",
    "\n",
    "1. **Collection Structure**: Viewing BOTH collections with separate document counts\n",
    "2. **Resume Data Queries**: Querying `resume_data` for work experience, education, skills with full metadata\n",
    "3. **Personality Data Queries**: Querying `personality` collection with simplified fixed-size chunking\n",
    "4. **Embeddings**: Inspecting 1536-dimensional vectors and their statistics\n",
    "5. **Semantic Search**: Using OpenAI embeddings for similarity-based retrieval from specific collections\n",
    "6. **Section Filtering**: Combining semantic search with metadata filters within resume collection\n",
    "7. **Complete RAG Flow**: End-to-end retrieval â†’ augmentation â†’ generation workflow using both collections\n",
    "\n",
    "### Architecture Benefits\n",
    "\n",
    "- âœ… **Semantic separation**: Resume facts vs personality traits stored independently\n",
    "- âœ… **Simplified storage**: Personality collection doesn't require complex section metadata\n",
    "- âœ… **No cross-contamination**: Personality searches retrieve only personality data\n",
    "- âœ… **Faster queries**: Smaller, focused collections = faster semantic search\n",
    "- âœ… **Cleaner code**: No complex section_type filtering or header parsing logic\n",
    "- âœ… **Better relevance**: Semantic matching within focused collections yields better results\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Chunking preserves context**: Each 400-char chunk maintains semantic meaning through overlap\n",
    "- **Embeddings enable semantic matching**: Query \"analytical thinking\" matches related personality traits\n",
    "- **Collection isolation prevents noise**: Searching personality collection won't return work achievements\n",
    "- **Metadata enables filtering**: Can retrieve specific section types within resume collection\n",
    "- **Similarity scores guide selection**: Higher scores = more relevant to query\n",
    "- **Simplified approach still works**: Fixed-size chunking is sufficient for personality content\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Run cells to explore your actual dual-collection vector database\n",
    "- Compare query results between collections\n",
    "- Modify queries to test different job requirements\n",
    "- Experiment with `score_threshold` values\n",
    "- Try combining multiple filters within each collection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Resume_Claude_SDK_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
