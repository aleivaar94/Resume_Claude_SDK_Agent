{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099a9b12",
   "metadata": {},
   "source": [
    "- **`resume_data` collection** (from `resume_ale.md`): work experience, education, skills, continuing studies with header based chunking\n",
    "- **`personality` collection** (from `personalities_16.md`): personality traits with fixed-size chunking\n",
    "- **`projects` collection** (from `portfolio_projects.md`): portfolio projects with hierarchical chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9a043",
   "metadata": {},
   "source": [
    "## Initialize Vector Store Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI embeddings for semantic search\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.core.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "print(\"âœ… Embedder initialized for semantic search queries\")\n",
    "\n",
    "# Initialize Qdrant client with local storage\n",
    "storage_path = \"../vector_db/qdrant_storage\"\n",
    "client = QdrantClient(path=storage_path)\n",
    "\n",
    "# Collection names (triple-collection architecture)\n",
    "resume_collection = \"resume_data\"\n",
    "personality_collection = \"personality\"\n",
    "projects_collection = \"projects\"\n",
    "\n",
    "print(\"âœ… Connected to Qdrant vector store\")\n",
    "print(f\"ğŸ“‚ Storage path: {Path(storage_path).absolute()}\")\n",
    "print(f\"\\nğŸ“¦ Collections:\")\n",
    "print(f\"   - {resume_collection} (resume content)\")\n",
    "print(f\"   - {personality_collection} (personality traits)\")\n",
    "print(f\"   - {projects_collection} (portfolio projects)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c14602",
   "metadata": {},
   "source": [
    "## Explore Collections Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all collections\n",
    "collections = client.get_collections()\n",
    "print(\"ğŸ“š Available Collections:\")\n",
    "for collection in collections.collections:\n",
    "    print(f\"   - {collection.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Explore ALL collections\n",
    "for collection_name in [resume_collection, personality_collection, projects_collection]:\n",
    "    if client.collection_exists(collection_name):\n",
    "        collection_info = client.get_collection(collection_name)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Collection '{collection_name}' Details:\")\n",
    "        print(f\"   Total documents: {collection_info.points_count}\")\n",
    "        print(f\"   Vector dimensions: {collection_info.config.params.vectors.size}\")\n",
    "        print(f\"   Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "        print(f\"   Status: {collection_info.status}\")\n",
    "        \n",
    "        # Count by section type\n",
    "        from collections import Counter\n",
    "        all_records, _ = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=1000,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        \n",
    "        section_counts = Counter(r.payload.get('section_type', 'unknown') for r in all_records)\n",
    "        \n",
    "        print(f\"\\n   ğŸ“ˆ Documents by Section Type:\")\n",
    "        for section, count in sorted(section_counts.items()):\n",
    "            print(f\"      {section:20s}: {count:3d} chunks\")\n",
    "        \n",
    "        print(\"   \" + \"-\"*76)\n",
    "    else:\n",
    "        print(f\"\\nâŒ Collection '{collection_name}' not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b793ad",
   "metadata": {},
   "source": [
    "## Resume Data Queries\n",
    "\n",
    "### Work Experience with Full Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28af347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for work experience entries (from resume_data collection)\n",
    "work_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"section_type\",\n",
    "            match=MatchValue(value=\"work_experience\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "work_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,  # Query resume_data collection\n",
    "    scroll_filter=work_filter,\n",
    "    limit=20,\n",
    "    with_payload=True,\n",
    "    with_vectors=False  # Set True to see embeddings\n",
    ")\n",
    "\n",
    "print(f\"ğŸ’¼ Work Experience Chunks from '{resume_collection}' collection (showing {len(work_records)}):\\n\")\n",
    "\n",
    "for i, record in enumerate(work_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CHUNK {i} - ID: {record.id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“„ Content (Achievement):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ¢ Metadata:\")\n",
    "    print(f\"   Company:        {metadata.get('company', 'N/A')}\")\n",
    "    print(f\"   Position:       {metadata.get('position', 'N/A')}\")\n",
    "    print(f\"   Start Date:     {metadata.get('start_date', 'N/A')}\")\n",
    "    print(f\"   End Date:       {metadata.get('end_date', 'N/A')}\")\n",
    "    print(f\"   Source File:    {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"   Section Type:   {payload.get('section_type', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ee076",
   "metadata": {},
   "source": [
    "### Work Experience with Embeddings\n",
    "\n",
    "Each chunk has a 1536-dimensional embedding vector generated by OpenAI's `text-embedding-3-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one work experience record WITH embeddings\n",
    "work_with_vector, _ = client.scroll(\n",
    "    collection_name=resume_collection,\n",
    "    scroll_filter=work_filter,\n",
    "    limit=20,\n",
    "    with_payload=True,\n",
    "    with_vectors=True  # Include embeddings\n",
    ")\n",
    "\n",
    "if work_with_vector:\n",
    "    record = work_with_vector[0]\n",
    "    vector = record.vector\n",
    "    \n",
    "    print(f\"ğŸ”¢ Embedding Vector Details:\")\n",
    "    print(f\"   Vector dimensions: {len(vector)}\")\n",
    "    print(f\"   Vector type: {type(vector)}\")\n",
    "    print(f\"   First 10 values: {vector[:10]}\")\n",
    "    print(f\"   Last 10 values:  {vector[-10:]}\")\n",
    "    print(f\"\\nğŸ“Š Vector Statistics:\")\n",
    "    import numpy as np\n",
    "    vector_array = np.array(vector)\n",
    "    print(f\"   Min value:  {vector_array.min():.6f}\")\n",
    "    print(f\"   Max value:  {vector_array.max():.6f}\")\n",
    "    print(f\"   Mean value: {vector_array.mean():.6f}\")\n",
    "    print(f\"   Std dev:    {vector_array.std():.6f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Associated Content:\")\n",
    "    print(f\"   {record.payload.get('content', 'N/A')[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058d26e",
   "metadata": {},
   "source": [
    "### Education and Skills\n",
    "\n",
    "Query education entries and skills from the resume data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query education entries (from resume_data collection)\n",
    "education_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"education\"))]\n",
    ")\n",
    "\n",
    "education_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    scroll_filter=education_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“ Education Entries from '{resume_collection}' collection ({len(education_records)}):\\n\")\n",
    "for i, record in enumerate(education_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"EDUCATION CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Degree:        {metadata.get('degree', 'N/A')}\")\n",
    "    print(f\"ğŸ« Institution:   {metadata.get('institution', 'N/A')}\")\n",
    "    print(f\"ğŸ“… Year:          {metadata.get('year', 'N/A')}\")\n",
    "    print(f\"ğŸ“‚ Source File:   {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type:  {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()\n",
    "\n",
    "# Query skills (from resume_data collection)\n",
    "skills_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"skills\"))]\n",
    ")\n",
    "\n",
    "skills_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,\n",
    "    scroll_filter=skills_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ› ï¸  Skills Entries from '{resume_collection}' collection ({len(skills_records)}):\\n\")\n",
    "for i, record in enumerate(skills_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"SKILL CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“‚ Category:      {metadata.get('category', 'N/A')}\")\n",
    "    print(f\"ğŸ“„ Skills:        {payload.get('content', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Source File:   {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type:  {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c60b4",
   "metadata": {},
   "source": [
    "## Personality Traits Queries\n",
    "\n",
    "### Personality Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d24204",
   "metadata": {},
   "outputs": [],
   "source": [
    "personality_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"\"))]\n",
    ")\n",
    "\n",
    "personality_records, _ = client.scroll(\n",
    "    collection_name=personality_collection,\n",
    "    scroll_filter=personality_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"ğŸ§  Personality Trait Chunks from '{personality_collection}' collection ({len(personality_records)}):\\n\")\n",
    "print(f\"ğŸ’¡ Note: This collection contains ONLY personality data with simplified fixed-size chunking\\n\")\n",
    "\n",
    "for i, record in enumerate(personality_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PERSONALITY CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Chunk Index: {metadata.get('chunk_index', 'N/A')}\")\n",
    "    print(f\"ğŸ“‚ Source File: {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Character Range: {metadata.get('char_start', 'N/A')} - {metadata.get('char_end', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4443867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow simulation\n",
    "print(f\"{'='*80}\")\n",
    "print(\"COMPLETE PROJECTS RETRIEVAL WORKFLOW\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "job_tech_requirements = \"data visualization matplotlib seaborn statistical analysis\"\n",
    "\n",
    "print(f\"ğŸ“‹ Job Requirements: {job_tech_requirements}\\n\")\n",
    "\n",
    "# PHASE 1: Search technical summaries\n",
    "print(f\"{'='*80}\")\n",
    "print(\"PHASE 1: Search Technical Summaries\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "query_vector = embedder.embed_query(job_tech_requirements)\n",
    "\n",
    "tech_results = client.query_points(\n",
    "    collection_name=projects_collection,\n",
    "    query=query_vector,\n",
    "    query_filter=Filter(\n",
    "        must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    "    ),\n",
    "    limit=3,\n",
    "    score_threshold=0.3  # Minimum similarity\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Found {len(tech_results)} matching projects:\\n\")\n",
    "\n",
    "matched_project_ids = []\n",
    "for i, result in enumerate(tech_results, 1):\n",
    "    metadata = result.payload.get('metadata', {})\n",
    "    project_id = metadata.get('project_id')\n",
    "    matched_project_ids.append(project_id)\n",
    "    \n",
    "    print(f\"{i}. [{result.score:.3f}] {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"   ID: {project_id}\")\n",
    "    print(f\"   Tech: {', '.join(metadata.get('tech_stack', []))}\\n\")\n",
    "\n",
    "# PHASE 2: Retrieve full content for matched projects\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 2: Retrieve Full Project Content\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"ğŸ” Retrieving full content for {len(matched_project_ids)} projects...\\n\")\n",
    "\n",
    "for project_id in matched_project_ids:\n",
    "    # Filter for full content of this project\n",
    "    full_filter = Filter(\n",
    "        must=[\n",
    "            FieldCondition(key=\"metadata.project_id\", match=MatchValue(value=project_id)),\n",
    "            FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_full\"))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    full_results, _ = client.scroll(\n",
    "        collection_name=projects_collection,\n",
    "        scroll_filter=full_filter,\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    \n",
    "    if full_results:\n",
    "        record = full_results[0]\n",
    "        payload = record.payload\n",
    "        metadata = payload.get('metadata', {})\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"PROJECT: {metadata.get('project_title', 'N/A')}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ†” ID: {project_id}\")\n",
    "        print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "        print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "        print(f\"\\nğŸ“„ Full Content (first 600 chars):\")\n",
    "        print(payload.get('content', 'N/A')[:600])\n",
    "        print(\"...\\n\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… WORKFLOW COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nğŸ’¡ Benefits of Hierarchical Chunking:\")\n",
    "print(\"   âœ“ Fast initial matching using technical summaries\")\n",
    "print(\"   âœ“ Retrieve full context only for relevant projects\")\n",
    "print(\"   âœ“ Efficient token usage (don't embed full content for initial search)\")\n",
    "print(\"   âœ“ Clear separation of filtering vs. detailed context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01dd59",
   "metadata": {},
   "source": [
    "## Projects Collection Queries\n",
    "\n",
    "### Complete Projects Retrieval Workflow\n",
    "\n",
    "Demonstrates the two-phase retrieval:\n",
    "1. **Phase 1**: Search `project_technical` for matching projects\n",
    "2. **Phase 2**: Retrieve `project_full` content using project_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82216e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate job requirements\n",
    "job_requirements = \"Python data visualization pandas matplotlib plotly\"\n",
    "\n",
    "print(f\"ğŸ” Semantic Search: Finding projects matching job requirements\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Query: '{job_requirements}'\")\n",
    "print(f\"Collection: {projects_collection}\")\n",
    "print(f\"Target: project_technical (for fast matching)\\n\")\n",
    "\n",
    "# Generate query embedding\n",
    "query_vector = embedder.embed_query(job_requirements)\n",
    "\n",
    "# Search technical summaries first (faster, focused)\n",
    "tech_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    ")\n",
    "\n",
    "tech_results = client.query_points(\n",
    "    collection_name=projects_collection,\n",
    "    query=query_vector,\n",
    "    query_filter=tech_filter,\n",
    "    limit=5\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Top {len(tech_results)} Matching Projects (by technical summary):\\n\")\n",
    "\n",
    "for i, result in enumerate(tech_results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"MATCH {i} - Similarity Score: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“¦ Project: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"ğŸ†” ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Technical Summary (first 300 chars):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:300]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ Workflow: Search technical summaries â†’ Get project_id â†’ Retrieve full content using metadata filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2583756",
   "metadata": {},
   "source": [
    "### Semantic Search: Find Projects by Technical Requirements\n",
    "\n",
    "Search for projects matching specific technologies or technical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe72b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for a specific project using metadata filtering\n",
    "target_project_id = \"project_0\"  # Change this to test different projects\n",
    "\n",
    "print(f\"ğŸ” Searching for project by metadata: project_id = '{target_project_id}'\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Build filter for exact metadata match\n",
    "metadata_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"metadata.project_id\",\n",
    "            match=MatchValue(value=target_project_id)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieve matching documents\n",
    "results, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    scroll_filter=metadata_filter,\n",
    "    limit=10,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Found {len(results)} chunks for {target_project_id}\\n\")\n",
    "\n",
    "for i, record in enumerate(results, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    section_type = payload.get('section_type', 'N/A')\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CHUNK {i}: {section_type}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“¦ Project Title: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"ğŸ†” Project ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ“¦ Chunk Type: {metadata.get('chunk_type', 'N/A')}\")\n",
    "    \n",
    "    if section_type == \"project_technical\":\n",
    "        print(f\"\\nğŸ”§ Technical Summary (first 400 chars):\")\n",
    "        print(f\"   {payload.get('content', 'N/A')[:400]}...\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“š Full Content (first 500 chars):\")\n",
    "        print(f\"   {payload.get('content', 'N/A')[:500]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ Use Case: Retrieve full project details after finding relevant technical summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbf24c",
   "metadata": {},
   "source": [
    "### Query by Metadata: Retrieve Specific Project by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636aa2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for full content chunks\n",
    "full_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_full\"))]\n",
    ")\n",
    "\n",
    "full_records, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    scroll_filter=full_filter,\n",
    "    limit=1000,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“š Full Project Content Chunks (project_full)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"ğŸ’¡ These chunks contain ALL sections for complete project context\\n\")\n",
    "\n",
    "for i, record in enumerate(full_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PROJECT {i}: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ†” Project ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ“¦ Chunk Type: {metadata.get('chunk_type', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Full Content\")\n",
    "    print(payload.get('content', 'N/A'))\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Summary of all full projects\n",
    "print(f\"\\nğŸ“Š Summary of All Full Project Chunks ({len(full_records)} total):\\n\")\n",
    "for i, record in enumerate(full_records, 1):\n",
    "    metadata = record.payload.get('metadata', {})\n",
    "    print(f\"{i}. {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"   Tech: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da1d51",
   "metadata": {},
   "source": [
    "### View Full Project Content (project_full chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for technical summary chunks\n",
    "tech_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    ")\n",
    "\n",
    "tech_records, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    scroll_filter=tech_filter,\n",
    "    limit=100,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ”§ Technical Summary Chunks (project_technical)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"ğŸ’¡ These chunks contain Tech Stack + Technical Highlights for fast matching\\n\")\n",
    "\n",
    "for i, record in enumerate(tech_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PROJECT {i}: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ†” Project ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ“¦ Chunk Type: {metadata.get('chunk_type', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content Preview (first 300 chars):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:300]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41969762",
   "metadata": {},
   "source": [
    "### View Technical Summaries (project_technical chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6099a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all projects in the collection\n",
    "all_projects, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    limit=100,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“‚ Projects Collection Overview\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Total chunks in collection: {len(all_projects)}\")\n",
    "\n",
    "# Group by project_id\n",
    "from collections import defaultdict\n",
    "projects_by_id = defaultdict(list)\n",
    "for record in all_projects:\n",
    "    project_id = record.payload.get('metadata', {}).get('project_id')\n",
    "    projects_by_id[project_id].append(record)\n",
    "\n",
    "print(f\"Total unique projects: {len(projects_by_id)}\")\n",
    "print(f\"\\nğŸ’¡ Each project has 2 chunks: technical_summary + full_content\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Display each project's chunks\n",
    "for project_id, chunks in sorted(projects_by_id.items()):\n",
    "    tech_chunk = next((c for c in chunks if c.payload.get('section_type') == 'project_technical'), None)\n",
    "    full_chunk = next((c for c in chunks if c.payload.get('section_type') == 'project_full'), None)\n",
    "    \n",
    "    if tech_chunk:\n",
    "        metadata = tech_chunk.payload.get('metadata', {})\n",
    "        print(f\"ğŸ“¦ Project: {metadata.get('project_title', 'N/A')}\")\n",
    "        print(f\"   ID: {project_id}\")\n",
    "        print(f\"   URL: {metadata.get('project_url', 'N/A')}\")\n",
    "        print(f\"   Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "        print(f\"   Chunks: {len(chunks)} (technical + full)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1499a58",
   "metadata": {},
   "source": [
    "### Hierarchical Chunking Strategy\n",
    "\n",
    "The `projects` collection uses a **hierarchical chunking strategy** with two chunk types per project:\n",
    "\n",
    "1. **`project_technical`**: Tech Stack + Technical Highlights (for filtering/matching)\n",
    "2. **`project_full`**: Complete project with all sections (Purpose, Tech Stack, Technical Highlights, Skills Demonstrated, Result/Impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e2eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query for Python-related achievements\n",
    "query_text = \"Python data analysis ETL pipeline machine learning\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "print(f\"ğŸ” Semantic Search Query: '{query_text}'\")\n",
    "print(f\"   Query vector dimensions: {len(query_vector)}\")\n",
    "print(f\"   Searching in: {resume_collection} collection\")\n",
    "\n",
    "# Search with vector similarity using query_points (newer API)\n",
    "results = client.query_points(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    query=query_vector,\n",
    "    limit=5,\n",
    "    score_threshold=0.5  # Only return results with similarity > 0.5\n",
    ").points\n",
    "\n",
    "print(f\"\\nğŸ“Š Top {len(results)} Results (by semantic similarity):\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"RESULT {i} - Similarity Score: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“„ Content: {payload.get('content', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    if payload.get('section_type') == 'work_experience':\n",
    "        print(f\"   Company: {metadata.get('company', 'N/A')}\")\n",
    "        print(f\"   Position: {metadata.get('position', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe440754",
   "metadata": {},
   "source": [
    "### Search for Personality Traits Matching Job Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114100a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a job analysis with soft skills and keywords\n",
    "job_analysis = {\n",
    "    'soft_skills': ['analytical thinking', 'problem-solving', 'collaboration'],\n",
    "    'keywords': ['strategic', 'innovative', 'team player']\n",
    "}\n",
    "\n",
    "# Build query (same logic as retrieve_personality_traits)\n",
    "query_parts = job_analysis.get('soft_skills', []) + job_analysis.get('keywords', [])\n",
    "query_text = ' '.join(query_parts)\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "print(f\"ğŸ” Job Requirements Query: '{query_text}'\")\n",
    "print(f\"   Searching in: {personality_collection} collection (NEW!)\\n\")\n",
    "\n",
    "# Search the PERSONALITY collection directly (no filtering needed!)\n",
    "all_results = client.query_points(\n",
    "    collection_name=personality_collection,  # â† Query personality collection directly!\n",
    "    query=query_vector,\n",
    "    limit=10\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Retrieved {len(all_results)} results from personality collection\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 5 Personality Trait Chunks by Semantic Similarity:\\n\")\n",
    "print(f\"ğŸ’¡ Benefits of simplified chunking:\")\n",
    "print(f\"   - Pure semantic search without complex filtering\")\n",
    "print(f\"   - Fixed-size chunks maintain consistent context windows\")\n",
    "print(f\"   - Faster search (smaller collection)\\n\")\n",
    "\n",
    "for i, result in enumerate(all_results[:5], 1):  # Top 5\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAIT {i} - Similarity: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Chunk Index: {metadata.get('chunk_index', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Characters: {metadata.get('char_start', 'N/A')} - {metadata.get('char_end', 'N/A')}\")\n",
    "    print(f\"ğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ These traits would be deduplicated (removing 100-char overlaps) and injected into the cover letter prompt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b67c13",
   "metadata": {},
   "source": [
    "### Semantic Search with Section Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for data science achievements ONLY in work experience (resume_data collection)\n",
    "query_text = \"data science machine learning SQL Python dashboard visualization\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "# Apply filter to only search work_experience\n",
    "work_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"work_experience\"))]\n",
    ")\n",
    "\n",
    "results = client.query_points(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    query=query_vector,\n",
    "    query_filter=work_filter,  # â† Apply filter during search\n",
    "    limit=5\n",
    ").points\n",
    "\n",
    "print(f\"ğŸ” Query: '{query_text}'\")\n",
    "print(f\"ğŸ“¦ Collection: {resume_collection}\")\n",
    "print(f\"ğŸ¯ Filter: section_type = 'work_experience'\")\n",
    "print(f\"\\nğŸ“Š Top {len(results)} Work Achievements:\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{i}. [Score: {result.score:.4f}] {metadata.get('company', 'N/A')} - {metadata.get('position', 'N/A')}\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:100]}...\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Resume_Claude_SDK_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
