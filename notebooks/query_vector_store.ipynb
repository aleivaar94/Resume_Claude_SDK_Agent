{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099a9b12",
   "metadata": {},
   "source": [
    "- **`resume_data` collection** (from `resume_ale.md`): work experience, education, skills, continuing studies with header based chunking\n",
    "- **`personality` collection** (from `personalities_16.md`): personality traits with fixed-size chunking\n",
    "- **`projects` collection** (from `portfolio_projects.md`): portfolio projects with hierarchical chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9a043",
   "metadata": {},
   "source": [
    "## Initialize Vector Store Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105c3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedder initialized for semantic search queries\n",
      "âœ… Connected to Qdrant vector store\n",
      "ğŸ“‚ Storage path: c:\\Users\\Ale\\Documents\\Data-Projects\\GitHub\\Resume_Claude_SDK_Agent\\notebooks\\..\\vector_db\\qdrant_storage\n",
      "\n",
      "ğŸ“¦ Collections:\n",
      "   - resume_data (resume content)\n",
      "   - personality (personality traits)\n",
      "   - projects (portfolio projects)\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI embeddings for semantic search\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.core.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "print(\"âœ… Embedder initialized for semantic search queries\")\n",
    "\n",
    "# Initialize Qdrant client with local storage\n",
    "storage_path = \"../vector_db/qdrant_storage\"\n",
    "client = QdrantClient(path=storage_path)\n",
    "\n",
    "# Collection names (triple-collection architecture)\n",
    "resume_collection = \"resume_data\"\n",
    "personality_collection = \"personality\"\n",
    "projects_collection = \"projects\"\n",
    "\n",
    "print(\"âœ… Connected to Qdrant vector store\")\n",
    "print(f\"ğŸ“‚ Storage path: {Path(storage_path).absolute()}\")\n",
    "print(f\"\\nğŸ“¦ Collections:\")\n",
    "print(f\"   - {resume_collection} (resume content)\")\n",
    "print(f\"   - {personality_collection} (personality traits)\")\n",
    "print(f\"   - {projects_collection} (portfolio projects)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c14602",
   "metadata": {},
   "source": [
    "## Explore Collections Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6215e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Available Collections:\n",
      "   - resume_data\n",
      "   - personality\n",
      "   - projects\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Collection 'resume_data' Details:\n",
      "   Total documents: 35\n",
      "   Vector dimensions: 1536\n",
      "   Distance metric: Cosine\n",
      "   Status: green\n",
      "\n",
      "   ğŸ“ˆ Documents by Section Type:\n",
      "      continuing_studies  :   7 chunks\n",
      "      education           :   2 chunks\n",
      "      personal_info       :   1 chunks\n",
      "      professional_summary:   1 chunks\n",
      "      skills              :   5 chunks\n",
      "      work_experience     :  19 chunks\n",
      "   ----------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Š Collection 'personality' Details:\n",
      "   Total documents: 8\n",
      "   Vector dimensions: 1536\n",
      "   Distance metric: Cosine\n",
      "   Status: green\n",
      "\n",
      "   ğŸ“ˆ Documents by Section Type:\n",
      "                          :   8 chunks\n",
      "   ----------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“Š Collection 'projects' Details:\n",
      "   Total documents: 42\n",
      "   Vector dimensions: 1536\n",
      "   Distance metric: Cosine\n",
      "   Status: green\n",
      "\n",
      "   ğŸ“ˆ Documents by Section Type:\n",
      "      project_full        :  21 chunks\n",
      "      project_technical   :  21 chunks\n",
      "   ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Get all collections\n",
    "collections = client.get_collections()\n",
    "print(\"ğŸ“š Available Collections:\")\n",
    "for collection in collections.collections:\n",
    "    print(f\"   - {collection.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Explore ALL collections\n",
    "for collection_name in [resume_collection, personality_collection, projects_collection]:\n",
    "    if client.collection_exists(collection_name):\n",
    "        collection_info = client.get_collection(collection_name)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Collection '{collection_name}' Details:\")\n",
    "        print(f\"   Total documents: {collection_info.points_count}\")\n",
    "        print(f\"   Vector dimensions: {collection_info.config.params.vectors.size}\")\n",
    "        print(f\"   Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "        print(f\"   Status: {collection_info.status}\")\n",
    "        \n",
    "        # Count by section type\n",
    "        from collections import Counter\n",
    "        all_records, _ = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=1000,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        \n",
    "        section_counts = Counter(r.payload.get('section_type', 'unknown') for r in all_records)\n",
    "        \n",
    "        print(f\"\\n   ğŸ“ˆ Documents by Section Type:\")\n",
    "        for section, count in sorted(section_counts.items()):\n",
    "            print(f\"      {section:20s}: {count:3d} chunks\")\n",
    "        \n",
    "        print(\"   \" + \"-\"*76)\n",
    "    else:\n",
    "        print(f\"\\nâŒ Collection '{collection_name}' not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b793ad",
   "metadata": {},
   "source": [
    "## Resume Data Queries\n",
    "\n",
    "### Work Experience with Full Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28af347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¼ Work Experience Chunks from 'resume_data' collection (showing 19):\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1 - ID: 0b2b312a-3c2d-4a6d-a6af-cad1bc2a98bc\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Assurance Technician: Coordinated supply chain and production teams to ensure food safety compliance by leading cross-functional meetings and implementing compliance checks, maintaining operational continuity.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        The Very Good Food Company\n",
      "   Position:       Quality Assurance Technician\n",
      "   Start Date:     February-2021\n",
      "   End Date:       February-2022\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 2 - ID: 120e8829-f4e5-4f0e-aba9-00c0632d6772\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Developed a Power BI dashboard to track changes in imported food volumes, collaborating with import inspectors to define metrics and design visualizations in Power BI for stakeholder use.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 3 - ID: 2c352005-a35e-438c-a669-9ac7587df973\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Developed and deployed a time-series forecasting model that achieved 87% accuracy in predicting monthly import/export volumes by training on historical data using Python and time-series ML techniques.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 4 - ID: 495bbc70-277c-481f-8a25-e74c8863b53d\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Analyst: Built three Power BI dashboards for sales and marketing by collaborating with stakeholders to define requirements and implementing interactive reports in Power BI.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Rubicon Organics\n",
      "   Position:       Data Analyst\n",
      "   Start Date:     March-2023\n",
      "   End Date:       December-2023\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 5 - ID: 4b45030c-9493-4299-a2d5-c36ec5b253d1\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Built an unsupervised anomaly detection model to identify potential food safety risks in imported products by training unsupervised ML models using Python and applying them to imported product datasets.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 6 - ID: 683dc6aa-604a-444d-a6c0-6d41590d8c6a\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Assurance Technician: Performed root cause analyses on non-conforming products, reducing rework and generating cost savings by applying corrective actions informed by statistical analysis in Excel.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        The Very Good Food Company\n",
      "   Position:       Quality Assurance Technician\n",
      "   Start Date:     February-2021\n",
      "   End Date:       February-2022\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 7 - ID: 8cfb1b8b-a386-4e06-8edb-d2e8358f0cad\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Analyzed pathogen occurrence trends across 50 facilities by performing statistical analysis with SQL and Power BI and collaborating with inspectors to implement preventive inspections.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 8 - ID: 9f290562-b1a2-472d-b001-b93a8f7442d0\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Implemented daily automated data refreshes, replacing weekly manual CSV exports, by gaining direct data-warehouse access and building an ETL pipeline using SQL and Microsoft Fabric to store data in a Lakehouse.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 9 - ID: a8c3550d-ef73-4b7e-a92e-10b9d869e225\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Analyst: Designed and implemented a reporting tool to pinpoint SKU opportunities at store level, contributing to a 12% increase in store-level sales by analyzing SKU performance with SQL and presenting results in Power BI.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Rubicon Organics\n",
      "   Position:       Data Analyst\n",
      "   Start Date:     March-2023\n",
      "   End Date:       December-2023\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 10 - ID: ab967565-e50f-4c70-87a4-0a231b9c7795\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Extracted and processed millions of import/export transactions by building web-scraping collectors and a PySpark ETL pipeline to load cleaned data into a Microsoft Fabric lakehouse.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 11 - ID: b1a803d2-7e5b-4daf-b47f-02ce0fbec044\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Engineer: Reduced hard-drive screw defects by 15% (â‰ˆ$110,000 annual savings) by extracting and cleaning 50,000+ records with SQL, performing statistical analysis in Excel, and applying Six Sigma root-cause methods.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        IBM\n",
      "   Position:       Quality Engineer\n",
      "   Start Date:     December-2017\n",
      "   End Date:       October-2018\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 12 - ID: b3051ea7-bc51-4d39-9e9e-1068a8472708\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Automated data categorization, reducing data cleaning time by over 15 hours per week, by integrating LLM-based classification into the ETL pipeline.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 13 - ID: c02cab12-4607-4ec8-8056-8ed2cc3cfb83\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Standardized descriptive and statistical reporting in Power BI, reducing report-generation time and improving inspection efficiency by creating templated reports and automated data queries.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 14 - ID: cb1defa7-f89a-48f8-a349-42ac9a50c25c\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Automated forecasting and reduced manual effort by 40 hours per month by deploying the forecasting pipeline and scheduling automated runs using Python and Microsoft Fabric.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 15 - ID: cd13be35-b9c4-4db2-9851-32c564d98702\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist: Developed and deployed anomaly-detection and time-series forecasting models achieving 86% accuracy in predicting food safety risk prevalence by training models on historical data using Python and ML techniques.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist\n",
      "   Start Date:     December-2023\n",
      "   End Date:       March-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 16 - ID: d10e77bc-fec7-40c2-9615-a827b9a1314b\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Engineer: Developed an Excel dashboard with automated SQL queries, reducing manual reporting time by 40% and enabling near-real-time quality monitoring by automating data extraction and transformation.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        IBM\n",
      "   Position:       Quality Engineer\n",
      "   Start Date:     December-2017\n",
      "   End Date:       October-2018\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 17 - ID: d32f10d4-abc0-4400-bd8c-309c8634df80\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Scientist II: Implemented a Python algorithm to automatically select sampling plans, reducing inspector manual work by 3 hours per inspector per day and generating approximately CAD 4,000,000 in annual savings by translating business rules into an automated algorithm.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Canadian Food Inspection Agency\n",
      "   Position:       Data Scientist II\n",
      "   Start Date:     March-2025\n",
      "   End Date:       November-2025\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 18 - ID: e214dec8-cc52-44d2-a214-3eab719ecfab\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Data Analyst: Built an ETL pipeline integrating five data sources totaling over 1M records using SQL and Python, automating ingestion and cleaning and saving 8 hours weekly in data preparation.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        Rubicon Organics\n",
      "   Position:       Data Analyst\n",
      "   Start Date:     March-2023\n",
      "   End Date:       December-2023\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n",
      "================================================================================\n",
      "CHUNK 19 - ID: f7329237-27a1-435f-b4b6-f00ba634c2d7\n",
      "================================================================================\n",
      "ğŸ“„ Content (Achievement):\n",
      "   Quality Assurance Technician: Automated sanitation KPI reporting using Excel and MS Forms, contributing to an 80% SQF audit score by building automated reports and forms to track sanitation metrics.\n",
      "\n",
      "ğŸ¢ Metadata:\n",
      "   Company:        The Very Good Food Company\n",
      "   Position:       Quality Assurance Technician\n",
      "   Start Date:     February-2021\n",
      "   End Date:       February-2022\n",
      "   Source File:    resume_ale.md\n",
      "   Section Type:   work_experience\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for work experience entries (from resume_data collection)\n",
    "work_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"section_type\",\n",
    "            match=MatchValue(value=\"work_experience\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "work_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,  # Query resume_data collection\n",
    "    scroll_filter=work_filter,\n",
    "    limit=20,\n",
    "    with_payload=True,\n",
    "    with_vectors=False  # Set True to see embeddings\n",
    ")\n",
    "\n",
    "print(f\"ğŸ’¼ Work Experience Chunks from '{resume_collection}' collection (showing {len(work_records)}):\\n\")\n",
    "\n",
    "for i, record in enumerate(work_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CHUNK {i} - ID: {record.id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“„ Content (Achievement):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ¢ Metadata:\")\n",
    "    print(f\"   Company:        {metadata.get('company', 'N/A')}\")\n",
    "    print(f\"   Position:       {metadata.get('position', 'N/A')}\")\n",
    "    print(f\"   Start Date:     {metadata.get('start_date', 'N/A')}\")\n",
    "    print(f\"   End Date:       {metadata.get('end_date', 'N/A')}\")\n",
    "    print(f\"   Source File:    {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"   Section Type:   {payload.get('section_type', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ee076",
   "metadata": {},
   "source": [
    "### Work Experience with Embeddings\n",
    "\n",
    "Each chunk has a 1536-dimensional embedding vector generated by OpenAI's `text-embedding-3-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5d0755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Embedding Vector Details:\n",
      "   Vector dimensions: 1536\n",
      "   Vector type: <class 'list'>\n",
      "   First 10 values: [-0.019191697239875793, 0.011832953430712223, 0.06691659241914749, 0.00033292826265096664, -0.016301382333040237, 0.029689325019717216, 0.020486559718847275, 0.019688831642270088, 0.03362015634775162, 0.01056121475994587]\n",
      "   Last 10 values:  [0.024139918386936188, 0.00474011804908514, -0.013596045784652233, -0.0012016488471999764, -0.01553833857178688, -0.006566797848790884, -0.006665068678557873, 0.014879346825182438, 0.003659140085801482, -0.0037920945324003696]\n",
      "\n",
      "ğŸ“Š Vector Statistics:\n",
      "   Min value:  -0.097485\n",
      "   Max value:  0.109878\n",
      "   Mean value: 0.000860\n",
      "   Std dev:    0.025501\n",
      "\n",
      "ğŸ“„ Associated Content:\n",
      "   Quality Assurance Technician: Coordinated supply chain and production teams to ensure food safety compliance by leading cross-functional meetings and ...\n"
     ]
    }
   ],
   "source": [
    "# Get one work experience record WITH embeddings\n",
    "work_with_vector, _ = client.scroll(\n",
    "    collection_name=resume_collection,\n",
    "    scroll_filter=work_filter,\n",
    "    limit=20,\n",
    "    with_payload=True,\n",
    "    with_vectors=True  # Include embeddings\n",
    ")\n",
    "\n",
    "if work_with_vector:\n",
    "    record = work_with_vector[0]\n",
    "    vector = record.vector\n",
    "    \n",
    "    print(f\"ğŸ”¢ Embedding Vector Details:\")\n",
    "    print(f\"   Vector dimensions: {len(vector)}\")\n",
    "    print(f\"   Vector type: {type(vector)}\")\n",
    "    print(f\"   First 10 values: {vector[:10]}\")\n",
    "    print(f\"   Last 10 values:  {vector[-10:]}\")\n",
    "    print(f\"\\nğŸ“Š Vector Statistics:\")\n",
    "    import numpy as np\n",
    "    vector_array = np.array(vector)\n",
    "    print(f\"   Min value:  {vector_array.min():.6f}\")\n",
    "    print(f\"   Max value:  {vector_array.max():.6f}\")\n",
    "    print(f\"   Mean value: {vector_array.mean():.6f}\")\n",
    "    print(f\"   Std dev:    {vector_array.std():.6f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Associated Content:\")\n",
    "    print(f\"   {record.payload.get('content', 'N/A')[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058d26e",
   "metadata": {},
   "source": [
    "### Education and Skills\n",
    "\n",
    "Query education entries and skills from the resume data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef4b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Education Entries from 'resume_data' collection (2):\n",
      "\n",
      "================================================================================\n",
      "EDUCATION CHUNK 1\n",
      "================================================================================\n",
      "ğŸ“ Degree:        BSc in Biotechnology Engineering\n",
      "ğŸ« Institution:   Tec de Monterrey\n",
      "ğŸ“… Year:          N/A\n",
      "ğŸ“‚ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  education\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   BSc in Biotechnology Engineering from Tec de Monterrey. August-2012 - May-2017 | Mexico\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"degree\": \"BSc in Biotechnology Engineering\",\n",
      "  \"institution\": \"Tec de Monterrey\",\n",
      "  \"dates\": \"August-2012 - May-2017 | Mexico\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "EDUCATION CHUNK 2\n",
      "================================================================================\n",
      "ğŸ“ Degree:        MSc in Food Science\n",
      "ğŸ« Institution:   University of British Columbia\n",
      "ğŸ“… Year:          N/A\n",
      "ğŸ“‚ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  education\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   MSc in Food Science from University of British Columbia. January-2019 - October-2020 | Canada\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"degree\": \"MSc in Food Science\",\n",
      "  \"institution\": \"University of British Columbia\",\n",
      "  \"dates\": \"January-2019 - October-2020 | Canada\"\n",
      "}\n",
      "\n",
      "\n",
      "ğŸ› ï¸  Skills Entries from 'resume_data' collection (5):\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 1\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Development Tools\n",
      "ğŸ“„ Skills:        Development Tools: Git, Docker, Azure DevOps, VS Code, API\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Development Tools\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 2\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Programming Languages\n",
      "ğŸ“„ Skills:        Programming Languages: Python, SQL, PySpark, T-SQL\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Programming Languages\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 3\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Methodologies\n",
      "ğŸ“„ Skills:        Methodologies: Agile, Six Sigma\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Methodologies\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 4\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Business Intelligence\n",
      "ğŸ“„ Skills:        Business Intelligence: Power BI, Microsoft Fabric, OneLake, Power Query, Power Pivot, Excel, Delta Lake\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Business Intelligence\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "SKILL CHUNK 5\n",
      "================================================================================\n",
      "ğŸ“‚ Category:      Cloud Platforms\n",
      "ğŸ“„ Skills:        Cloud Platforms: Azure, Google Cloud\n",
      "ğŸ“ Source File:   resume_ale.md\n",
      "ğŸ·ï¸  Section Type:  skills\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"category\": \"Cloud Platforms\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query education entries (from resume_data collection)\n",
    "education_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"education\"))]\n",
    ")\n",
    "\n",
    "education_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    scroll_filter=education_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“ Education Entries from '{resume_collection}' collection ({len(education_records)}):\\n\")\n",
    "for i, record in enumerate(education_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"EDUCATION CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Degree:        {metadata.get('degree', 'N/A')}\")\n",
    "    print(f\"ğŸ« Institution:   {metadata.get('institution', 'N/A')}\")\n",
    "    print(f\"ğŸ“… Year:          {metadata.get('year', 'N/A')}\")\n",
    "    print(f\"ğŸ“‚ Source File:   {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type:  {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()\n",
    "\n",
    "# Query skills (from resume_data collection)\n",
    "skills_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"skills\"))]\n",
    ")\n",
    "\n",
    "skills_records, _ = client.scroll(\n",
    "    collection_name=resume_collection,\n",
    "    scroll_filter=skills_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ› ï¸  Skills Entries from '{resume_collection}' collection ({len(skills_records)}):\\n\")\n",
    "for i, record in enumerate(skills_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"SKILL CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“‚ Category:      {metadata.get('category', 'N/A')}\")\n",
    "    print(f\"ğŸ“„ Skills:        {payload.get('content', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Source File:   {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type:  {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c60b4",
   "metadata": {},
   "source": [
    "## Personality Traits Queries\n",
    "\n",
    "### Personality Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d24204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Personality Trait Chunks from 'personality' collection (8):\n",
      "\n",
      "ğŸ’¡ Note: This collection contains ONLY personality data with simplified fixed-size chunking\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 1\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 3\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 900 - 1300\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   also attending to crucial details, which makes me a valuable asset in any organization.\n",
      "\n",
      "## Strengths\n",
      "### Innovative Mindset\n",
      "My ability to see possibilities others overlook often helps me find smarter solutions and effective improvements at work.\n",
      "\n",
      "### Independent Worker\n",
      "My talent for working productively on my own allows me to manage tasks effectively without the need for constant direction or sup\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 3,\n",
      "  \"char_start\": 900,\n",
      "  \"char_end\": 1300,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 2\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 0\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 0 - 400\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   # Personality Traits\n",
      "I'm very analytical, highly curious and constantly seek to improve systems. I approach life with a strategic mindset, always looking several steps ahead and planning for contingencies. I value autonomy but also enjoy collaborating with others. This allows me to tackle complex problems with confidence and innovation. I hold high standards for myself and others, always striving\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 0,\n",
      "  \"char_start\": 0,\n",
      "  \"char_end\": 400,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 3\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 2\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 600 - 1000\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   rences\n",
      "I thrive in environments that challenge me intellectually and provide opportunities for growth. I seek roles where I can implement innovative ideas and apply strategic thinking and problem-solvingâ€”often in fields like science, technology, or business strategy. I can see the big picture while also attending to crucial details, which makes me a valuable asset in any organization.\n",
      "\n",
      "## Strength\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 2,\n",
      "  \"char_start\": 600,\n",
      "  \"char_end\": 1000,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 4\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 6\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 1800 - 2200\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   Reliable Performance\n",
      "When entrusted with critical tasks, I consistently deliver precise, high-quality results, making me a valued and dependable asset.\n",
      "\n",
      "### Big-Picture Focus\n",
      "I prefer focusing on overarching goals and strategies rather than micromanaging small details.\n",
      "\n",
      "### Goal-Oriented\n",
      "I stay motivated by clear goals and visible progress, consistently tracking achievements and identifying next\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 6,\n",
      "  \"char_start\": 1800,\n",
      "  \"char_end\": 2200,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 5\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 7\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 2100 - 2207\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   ivated by clear goals and visible progress, consistently tracking achievements and identifying next steps.\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 7,\n",
      "  \"char_start\": 2100,\n",
      "  \"char_end\": 2207,\n",
      "  \"overlap_chars\": 0\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 6\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 5\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 1500 - 1900\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   ent\n",
      "I naturally focus on refining work processes and spotting inefficiencies, consistently improving project outcomes wherever I go.\n",
      "\n",
      "### Objective Judgment\n",
      "My capacity to make impartial decisions based on facts rather than favoritism or personal bias earns respect and trust from my colleagues.\n",
      "\n",
      "### Reliable Performance\n",
      "When entrusted with critical tasks, I consistently deliver precise, high-quali\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 5,\n",
      "  \"char_start\": 1500,\n",
      "  \"char_end\": 1900,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 7\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 1\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 300 - 700\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   oblems with confidence and innovation. I hold high standards for myself and others, always striving for efficiency and effectiveness. I prefer direct communication, honesty, and logical discussion. This tendency can make me appear aloof or detached to others, even when I care deeply.\n",
      "\n",
      "# Career Preferences\n",
      "I thrive in environments that challenge me intellectually and provide opportunities for growt\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 1,\n",
      "  \"char_start\": 300,\n",
      "  \"char_end\": 700,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "PERSONALITY CHUNK 8\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 4\n",
      "ğŸ“‚ Source File: personalities_16.md\n",
      "ğŸ“ Character Range: 1200 - 1600\n",
      "\n",
      "ğŸ“„ Content:\n",
      "   ively on my own allows me to manage tasks effectively without the need for constant direction or supervision.\n",
      "\n",
      "### Conceptual Thinking\n",
      "I effortlessly grasp abstract, complex ideas, making me particularly suited to roles that require strategic analysis and long-term planning.\n",
      "\n",
      "### Continuous Improvement\n",
      "I naturally focus on refining work processes and spotting inefficiencies, consistently improving\n",
      "\n",
      "ğŸ” Full Metadata: {\n",
      "  \"chunk_index\": 4,\n",
      "  \"char_start\": 1200,\n",
      "  \"char_end\": 1600,\n",
      "  \"overlap_chars\": 100\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personality_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"\"))]\n",
    ")\n",
    "\n",
    "personality_records, _ = client.scroll(\n",
    "    collection_name=personality_collection,\n",
    "    scroll_filter=personality_filter,\n",
    "    limit=20,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "print(f\"ğŸ§  Personality Trait Chunks from '{personality_collection}' collection ({len(personality_records)}):\\n\")\n",
    "print(f\"ğŸ’¡ Note: This collection contains ONLY personality data with simplified fixed-size chunking\\n\")\n",
    "\n",
    "for i, record in enumerate(personality_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PERSONALITY CHUNK {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Chunk Index: {metadata.get('chunk_index', 'N/A')}\")\n",
    "    print(f\"ğŸ“‚ Source File: {payload.get('source_file', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Character Range: {metadata.get('char_start', 'N/A')} - {metadata.get('char_end', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print(f\"\\nğŸ” Full Metadata: {json.dumps(metadata, indent=2)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4443867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE PROJECTS RETRIEVAL WORKFLOW\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Job Requirements: data visualization matplotlib seaborn statistical analysis\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: Search Technical Summaries\n",
      "================================================================================\n",
      "\n",
      "âœ… Found 3 matching projects:\n",
      "\n",
      "1. [0.526] CERN Career Preferences Analysis: A Data Visualization\n",
      "   ID: project_3\n",
      "   Tech: Python, Pandas, Seaborn, Matplotlib\n",
      "\n",
      "2. [0.509] AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "   ID: project_2\n",
      "   Tech: Python, Pandas, Plotly, Matplotlib\n",
      "\n",
      "3. [0.492] Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "   ID: project_1\n",
      "   Tech: Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: Retrieve Full Project Content\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Retrieving full content for 3 projects...\n",
      "\n",
      "================================================================================\n",
      "PROJECT: CERN Career Preferences Analysis: A Data Visualization\n",
      "================================================================================\n",
      "ğŸ†” ID: project_3\n",
      "ğŸ’» Tech Stack: Python, Pandas, Seaborn, Matplotlib\n",
      "ğŸ”— URL: https://tinyurl.com/hnexp2r8\n",
      "\n",
      "ğŸ“„ Full Content (first 600 chars):\n",
      "# CERN Career Preferences Analysis: A Data Visualization\n",
      "\n",
      "## Purpose\n",
      "This project transforms observational data from a CERN interactive activity into a visual analysis of visitor career preferences at Europe's largest research organization. The visualization reveals which scientific and engineering roles attracted the most interest among facility visitors.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, Seaborn, Matplotlib\n",
      "\n",
      "## Technical Highlights\n",
      "The project demonstrates proficiency in data transformation and visualization with pandas for dataset organization and sorting. Custom styling techniques are applied\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "PROJECT: AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "================================================================================\n",
      "ğŸ†” ID: project_2\n",
      "ğŸ’» Tech Stack: Python, Pandas, Plotly, Matplotlib\n",
      "ğŸ”— URL: https://tinyurl.com/4xktcfsm\n",
      "\n",
      "ğŸ“„ Full Content (first 600 chars):\n",
      "# AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "\n",
      "## Purpose\n",
      "This project evaluates different charting approaches for communicating multi-dimensional AI model performance data. It compares grouped bar charts, and bubble charts to demonstrate how appropriate chart selection significantly improves the clarity and accessibility of model-to-model and benchmark-to-benchmark performance comparisons.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, Plotly, Matplotlib\n",
      "\n",
      "## Technical Highlights\n",
      "The project implements a comparative analysis of three visualization approaches to handle three-dimensional b\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "PROJECT: Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "================================================================================\n",
      "ğŸ†” ID: project_1\n",
      "ğŸ’» Tech Stack: Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "ğŸ”— URL: https://tinyurl.com/2wxtb6af\n",
      "\n",
      "ğŸ“„ Full Content (first 600 chars):\n",
      "# Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "\n",
      "## Purpose\n",
      "This project analyzes grain production trends in Canada using government statistics data, demonstrating expertise in data visualization design principles. It compares different visualization approachesâ€”grouped bar charts versus bubble chartsâ€”to determine the most effective way to communicate complex, multi-dimensional agricultural data.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "\n",
      "## Technical Highlights\n",
      "The project implements a sophisticated data transformation pipeline that processes raw\n",
      "...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "âœ… WORKFLOW COMPLETE\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ Benefits of Hierarchical Chunking:\n",
      "   âœ“ Fast initial matching using technical summaries\n",
      "   âœ“ Retrieve full context only for relevant projects\n",
      "   âœ“ Efficient token usage (don't embed full content for initial search)\n",
      "   âœ“ Clear separation of filtering vs. detailed context\n"
     ]
    }
   ],
   "source": [
    "# Complete workflow simulation\n",
    "print(f\"{'='*80}\")\n",
    "print(\"COMPLETE PROJECTS RETRIEVAL WORKFLOW\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "job_tech_requirements = \"data visualization matplotlib seaborn statistical analysis\"\n",
    "\n",
    "print(f\"ğŸ“‹ Job Requirements: {job_tech_requirements}\\n\")\n",
    "\n",
    "# PHASE 1: Search technical summaries\n",
    "print(f\"{'='*80}\")\n",
    "print(\"PHASE 1: Search Technical Summaries\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "query_vector = embedder.embed_query(job_tech_requirements)\n",
    "\n",
    "tech_results = client.query_points(\n",
    "    collection_name=projects_collection,\n",
    "    query=query_vector,\n",
    "    query_filter=Filter(\n",
    "        must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    "    ),\n",
    "    limit=3,\n",
    "    score_threshold=0.3  # Minimum similarity\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Found {len(tech_results)} matching projects:\\n\")\n",
    "\n",
    "matched_project_ids = []\n",
    "for i, result in enumerate(tech_results, 1):\n",
    "    metadata = result.payload.get('metadata', {})\n",
    "    project_id = metadata.get('project_id')\n",
    "    matched_project_ids.append(project_id)\n",
    "    \n",
    "    print(f\"{i}. [{result.score:.3f}] {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"   ID: {project_id}\")\n",
    "    print(f\"   Tech: {', '.join(metadata.get('tech_stack', []))}\\n\")\n",
    "\n",
    "# PHASE 2: Retrieve full content for matched projects\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 2: Retrieve Full Project Content\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"ğŸ” Retrieving full content for {len(matched_project_ids)} projects...\\n\")\n",
    "\n",
    "for project_id in matched_project_ids:\n",
    "    # Filter for full content of this project\n",
    "    full_filter = Filter(\n",
    "        must=[\n",
    "            FieldCondition(key=\"metadata.project_id\", match=MatchValue(value=project_id)),\n",
    "            FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_full\"))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    full_results, _ = client.scroll(\n",
    "        collection_name=projects_collection,\n",
    "        scroll_filter=full_filter,\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    \n",
    "    if full_results:\n",
    "        record = full_results[0]\n",
    "        payload = record.payload\n",
    "        metadata = payload.get('metadata', {})\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"PROJECT: {metadata.get('project_title', 'N/A')}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ†” ID: {project_id}\")\n",
    "        print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "        print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "        print(f\"\\nğŸ“„ Full Content (first 600 chars):\")\n",
    "        print(payload.get('content', 'N/A')[:600])\n",
    "        print(\"...\\n\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… WORKFLOW COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nğŸ’¡ Benefits of Hierarchical Chunking:\")\n",
    "print(\"   âœ“ Fast initial matching using technical summaries\")\n",
    "print(\"   âœ“ Retrieve full context only for relevant projects\")\n",
    "print(\"   âœ“ Efficient token usage (don't embed full content for initial search)\")\n",
    "print(\"   âœ“ Clear separation of filtering vs. detailed context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01dd59",
   "metadata": {},
   "source": [
    "## Projects Collection Queries\n",
    "\n",
    "### Complete Projects Retrieval Workflow\n",
    "\n",
    "Demonstrates the two-phase retrieval:\n",
    "1. **Phase 1**: Search `project_technical` for matching projects\n",
    "2. **Phase 2**: Retrieve `project_full` content using project_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82216e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Semantic Search: Finding projects matching job requirements\n",
      "================================================================================\n",
      "\n",
      "Query: 'Python data visualization pandas matplotlib plotly'\n",
      "Collection: projects\n",
      "Target: project_technical (for fast matching)\n",
      "\n",
      "âœ… Top 5 Matching Projects (by technical summary):\n",
      "\n",
      "================================================================================\n",
      "MATCH 1 - Similarity Score: 0.5283\n",
      "================================================================================\n",
      "ğŸ“¦ Project: CERN Career Preferences Analysis: A Data Visualization\n",
      "ğŸ†” ID: project_3\n",
      "ğŸ’» Tech Stack: Python, Pandas, Seaborn, Matplotlib\n",
      "ğŸ”— URL: https://tinyurl.com/hnexp2r8\n",
      "\n",
      "ğŸ“„ Technical Summary (first 300 chars):\n",
      "   Project: CERN Career Preferences Analysis: A Data Visualization\n",
      "\n",
      "Technologies: Python, Pandas, Seaborn, Matplotlib\n",
      "\n",
      "Technical Work:\n",
      "The project demonstrates proficiency in data transformation and visualization with pandas for dataset organization and sorting. Custom styling techniques are applied th...\n",
      "\n",
      "================================================================================\n",
      "MATCH 2 - Similarity Score: 0.5170\n",
      "================================================================================\n",
      "ğŸ“¦ Project: AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "ğŸ†” ID: project_2\n",
      "ğŸ’» Tech Stack: Python, Pandas, Plotly, Matplotlib\n",
      "ğŸ”— URL: https://tinyurl.com/4xktcfsm\n",
      "\n",
      "ğŸ“„ Technical Summary (first 300 chars):\n",
      "   Project: AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "\n",
      "Technologies: Python, Pandas, Plotly, Matplotlib\n",
      "\n",
      "Technical Work:\n",
      "The project implements a comparative analysis of three visualization approaches to handle three-dimensional benchmark data (models, benchmarks, and performanc...\n",
      "\n",
      "================================================================================\n",
      "MATCH 3 - Similarity Score: 0.5110\n",
      "================================================================================\n",
      "ğŸ“¦ Project: Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "ğŸ†” ID: project_1\n",
      "ğŸ’» Tech Stack: Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "ğŸ”— URL: https://tinyurl.com/2wxtb6af\n",
      "\n",
      "ğŸ“„ Technical Summary (first 300 chars):\n",
      "   Project: Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "\n",
      "Technologies: Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "\n",
      "Technical Work:\n",
      "The project implements a sophisticated data transformation pipeline that processes raw CSV data with custom cleaning logic, including comm...\n",
      "\n",
      "================================================================================\n",
      "MATCH 4 - Similarity Score: 0.4320\n",
      "================================================================================\n",
      "ğŸ“¦ Project: Food Safety Analytics Dashboard: Visualizing 12 Years of Canadian Regulatory Recalls\n",
      "ğŸ†” ID: project_14\n",
      "ğŸ’» Tech Stack: Python, Power BI, Pandas, NumPy, Regex, Matplotlib, Seaborn, Jupyter Notebook\n",
      "ğŸ”— URL: https://tinyurl.com/mf246dkf\n",
      "\n",
      "ğŸ“„ Technical Summary (first 300 chars):\n",
      "   Project: Food Safety Analytics Dashboard: Visualizing 12 Years of Canadian Regulatory Recalls\n",
      "\n",
      "Technologies: Python, Power BI, Pandas, NumPy, Regex, Matplotlib, Seaborn, Jupyter Notebook\n",
      "\n",
      "Technical Work:\n",
      "The project implements advanced data extraction and normalization techniques using custom regex ...\n",
      "\n",
      "================================================================================\n",
      "MATCH 5 - Similarity Score: 0.4220\n",
      "================================================================================\n",
      "ğŸ“¦ Project: Wage Gap BC Minimum vs. Living Wage 2025\n",
      "ğŸ†” ID: project_0\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "ğŸ”— URL: https://tinyurl.com/yjy6vdfr\n",
      "\n",
      "ğŸ“„ Technical Summary (first 300 chars):\n",
      "   Project: Wage Gap BC Minimum vs. Living Wage 2025\n",
      "\n",
      "Technologies: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "\n",
      "Technical Work:\n",
      "Implements a slope chart visualization techniqueâ€”a sophisticated data storytelling approach that effectively communicates the magnitude of wage disparity bet...\n",
      "\n",
      "\n",
      "ğŸ’¡ Workflow: Search technical summaries â†’ Get project_id â†’ Retrieve full content using metadata filter\n"
     ]
    }
   ],
   "source": [
    "# Simulate job requirements\n",
    "job_requirements = \"Python data visualization pandas matplotlib plotly\"\n",
    "\n",
    "print(f\"ğŸ” Semantic Search: Finding projects matching job requirements\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Query: '{job_requirements}'\")\n",
    "print(f\"Collection: {projects_collection}\")\n",
    "print(f\"Target: project_technical (for fast matching)\\n\")\n",
    "\n",
    "# Generate query embedding\n",
    "query_vector = embedder.embed_query(job_requirements)\n",
    "\n",
    "# Search technical summaries first (faster, focused)\n",
    "tech_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    ")\n",
    "\n",
    "tech_results = client.query_points(\n",
    "    collection_name=projects_collection,\n",
    "    query=query_vector,\n",
    "    query_filter=tech_filter,\n",
    "    limit=5\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Top {len(tech_results)} Matching Projects (by technical summary):\\n\")\n",
    "\n",
    "for i, result in enumerate(tech_results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"MATCH {i} - Similarity Score: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“¦ Project: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"ğŸ†” ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Technical Summary (first 300 chars):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:300]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ Workflow: Search technical summaries â†’ Get project_id â†’ Retrieve full content using metadata filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2583756",
   "metadata": {},
   "source": [
    "### Semantic Search: Find Projects by Technical Requirements\n",
    "\n",
    "Search for projects matching specific technologies or technical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe72b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching for project by metadata: project_id = 'project_0'\n",
      "================================================================================\n",
      "\n",
      "âœ… Found 2 chunks for project_0\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1: project_technical\n",
      "================================================================================\n",
      "ğŸ“¦ Project Title: Wage Gap BC Minimum vs. Living Wage 2025\n",
      "ğŸ†” Project ID: project_0\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "\n",
      "ğŸ”§ Technical Summary (first 400 chars):\n",
      "   Project: Wage Gap BC Minimum vs. Living Wage 2025\n",
      "\n",
      "Technologies: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "\n",
      "Technical Work:\n",
      "Implements a slope chart visualization techniqueâ€”a sophisticated data storytelling approach that effectively communicates the magnitude of wage disparity between two data points. The project demonstrates advanced matplotlib customization including custom ax...\n",
      "\n",
      "================================================================================\n",
      "CHUNK 2: project_full\n",
      "================================================================================\n",
      "ğŸ“¦ Project Title: Wage Gap BC Minimum vs. Living Wage 2025\n",
      "ğŸ†” Project ID: project_0\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "\n",
      "ğŸ“š Full Content (first 500 chars):\n",
      "   # Wage Gap BC Minimum vs. Living Wage 2025\n",
      "\n",
      "## Purpose\n",
      "Analyzes the economic gap between British Columbia's minimum wage and living wage to highlight the challenge faced by minimum wage workers. This project uses strategic data visualization to communicate a critical social and economic issue affecting workers in BC.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "\n",
      "## Technical Highlights\n",
      "Implements a slope chart visualization techniqueâ€”a sophisticated data storytelli...\n",
      "\n",
      "\n",
      "ğŸ’¡ Use Case: Retrieve full project details after finding relevant technical summary\n"
     ]
    }
   ],
   "source": [
    "# Query for a specific project using metadata filtering\n",
    "target_project_id = \"project_0\"  # Change this to test different projects\n",
    "\n",
    "print(f\"ğŸ” Searching for project by metadata: project_id = '{target_project_id}'\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Build filter for exact metadata match\n",
    "metadata_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"metadata.project_id\",\n",
    "            match=MatchValue(value=target_project_id)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieve matching documents\n",
    "results, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    scroll_filter=metadata_filter,\n",
    "    limit=10,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Found {len(results)} chunks for {target_project_id}\\n\")\n",
    "\n",
    "for i, record in enumerate(results, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    section_type = payload.get('section_type', 'N/A')\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"CHUNK {i}: {section_type}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“¦ Project Title: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"ğŸ†” Project ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ“¦ Chunk Type: {metadata.get('chunk_type', 'N/A')}\")\n",
    "    \n",
    "    if section_type == \"project_technical\":\n",
    "        print(f\"\\nğŸ”§ Technical Summary (first 400 chars):\")\n",
    "        print(f\"   {payload.get('content', 'N/A')[:400]}...\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“š Full Content (first 500 chars):\")\n",
    "        print(f\"   {payload.get('content', 'N/A')[:500]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ Use Case: Retrieve full project details after finding relevant technical summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbf24c",
   "metadata": {},
   "source": [
    "### Query by Metadata: Retrieve Specific Project by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636aa2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Full Project Content Chunks (project_full)\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ These chunks contain ALL sections for complete project context\n",
      "\n",
      "================================================================================\n",
      "PROJECT 1: Customer Review Sentiment Analysis\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_8\n",
      "ğŸ”— URL: https://tinyurl.com/3baruh6x\n",
      "ğŸ’» Tech Stack: Python, Streamlit, Transformers, PyTorch, NLTK, Pandas, NumPy, Matplotlib, Seaborn, Scipy\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Customer Review Sentiment Analysis\n",
      "\n",
      "## Purpose\n",
      "This project decodes emotional opinions in customer reviews using advanced natural language processing techniques, enabling businesses to refine marketing strategies, optimize product offerings, and identify emerging market trends. By automatically classifying sentiment across large volumes of customer feedback, it provides actionable intelligence for competitive positioning in consumer goods industries.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Streamlit, Transformers, PyTorch, NLTK, Pandas, NumPy, Matplotlib, Seaborn, Scipy\n",
      "\n",
      "## Technical Highlights\n",
      "The project implements a comparative analysis of two distinct sentiment analysis approaches: a traditional VADER (Valence Aware Dictionary and Sentiment Reasoner) model using bag-of-words methodology, and a state-of-the-art RoBERTa transformer-based model that captures contextual relationships between words. The RoBERTa implementation leverages the HuggingFace ecosystem to tokenize input text, generate embeddings, and compute probability distributions across sentiment classes. The analysis was conducted on a substantial dataset of approximately 500,000 Amazon food reviews, with comprehensive exploratory data analysis visualizations comparing model performance across different rating scales to validate prediction accuracy. The results demonstrate that the transformer-based approach provides superior performance over the lexicon-based approach.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Full-stack NLP pipeline development encompassing data preprocessing, exploratory data analysis, and model comparison frameworks. Proficiency in deploying interactive machine learning solutions using modern Python web frameworks and demonstrating expertise in leveraging pre-trained transformer models for production-grade applications.\n",
      "\n",
      "## Result/Impact\n",
      "The comparative analysis demonstrated that the RoBERTa model outperformed the VADER baseline in sentiment classification accuracy across review categories. The deployed web application enables real-time sentiment analysis on user-provided text, transforming raw customer feedback into structured emotional intelligence that directly supports business decision-making in product development and marketing optimization.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 2: Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_1\n",
      "ğŸ”— URL: https://tinyurl.com/2wxtb6af\n",
      "ğŸ’» Tech Stack: Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "\n",
      "## Purpose\n",
      "This project analyzes grain production trends in Canada using government statistics data, demonstrating expertise in data visualization design principles. It compares different visualization approachesâ€”grouped bar charts versus bubble chartsâ€”to determine the most effective way to communicate complex, multi-dimensional agricultural data.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "\n",
      "## Technical Highlights\n",
      "The project implements a sophisticated data transformation pipeline that processes raw CSV data with custom cleaning logic, including comma removal, numeric type conversion, and categorical renaming to prepare data for analysis. It systematically evaluates multiple visualization strategies: grouped bar charts are tested for their ability to compare categories within groups, but are shown to become cluttered with many categories; bubble charts are then introduced as an alternative where size adds a visual dimension to enhance magnitude differentiation. The implementation further demonstrates advanced color theory application by comparing diverging color scales (RdBu, balance) against sequential scales (Reds) on identical bubble chart datasets, revealing how red-to-blue transitions create cognitive friction due to counterintuitive color-value associations, while red-based sequential scales combined with bubble size provide faster visual comprehension. Each visualization is professionally styled with consistent grid lines and optimized layouts to isolate design variable impacts.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Exceptional expertise in data visualization design, color theory, and human-centered information design that directly impacts audience comprehension. Demonstrates analytical rigor by conducting comparative studies on visualization effectiveness and the ability to translate design principles into technical implementation for maximum audience impact.\n",
      "\n",
      "## Result/Impact\n",
      "The project provides empirical, visual evidence for selecting appropriate chart types and color palettes based on data complexity and cognitive load considerations. This comparative analysis framework enables more informed design decisions that measurably improve data comprehension and decision-making across technical and non-technical audiences.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 3: Sales Forecasting with ML XGBoost Feature Importance for Retail Optimization\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_17\n",
      "ğŸ”— URL: https://tinyurl.com/38urcn9d\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Sales Forecasting with ML XGBoost Feature Importance for Retail Optimization\n",
      "\n",
      "## Purpose\n",
      "Predict grocery store sales across multiple locations using machine learning, identifying key factors that drive product performance. This project applies gradient boosted trees to understand which product and store characteristics have the strongest influence on sales outcomes.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Jupyter Notebook, Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn\n",
      "\n",
      "## Technical Highlights\n",
      "Implemented an end-to-end machine learning pipeline encompassing data cleaning, exploratory data analysis, and model development. Addressed missing value challenges by applying mean imputation for continuous variables and mode-based imputation for categorical variables after performing chi-squared statistical testing to ensure independence. Encoded categorical features using label encoding to prepare data for the XGBoost regressor. The project demonstrates understanding of feature importance analysis, model evaluation with multiple metrics (RÂ² and Mean Squared Error), and visualization of prediction performance across different feature dimensions.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Data preprocessing and exploratory data analysis with handling of missing data using statistical methods, machine learning model development and evaluation using gradient boosting algorithms, and data visualization techniques to interpret model predictions and feature importance relationships.\n",
      "\n",
      "## Result/Impact\n",
      "Achieved RÂ² of 0.86 on training data and identified Item_MRP (list price) and Item_Visibility as the most important predictors of sales, providing actionable insights for retail optimization. Test set RÂ² of 0.52 indicates the model captured meaningful patterns but suggests opportunities for further refinement through hyperparameter optimization and outlier removal.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 4: CERN Career Preferences Analysis: A Data Visualization\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_3\n",
      "ğŸ”— URL: https://tinyurl.com/hnexp2r8\n",
      "ğŸ’» Tech Stack: Python, Pandas, Seaborn, Matplotlib\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# CERN Career Preferences Analysis: A Data Visualization\n",
      "\n",
      "## Purpose\n",
      "This project transforms observational data from a CERN interactive activity into a visual analysis of visitor career preferences at Europe's largest research organization. The visualization reveals which scientific and engineering roles attracted the most interest among facility visitors.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, Seaborn, Matplotlib\n",
      "\n",
      "## Technical Highlights\n",
      "The project demonstrates proficiency in data transformation and visualization with pandas for dataset organization and sorting. Custom styling techniques are applied through matplotlib and seaborn to create a publication-quality visualization, including custom color palettes, grid configuration, and typography management. The code implements responsive figure sizing with DPI adjustment and systematic removal of chart clutter through spine and border removal for improved data-ink ratio. Data is presented as a horizontally-oriented bar chart that prioritizes readability for categorical comparisons.\n",
      "\n",
      "## Skills Demonstrated\n",
      "The project showcases data cleaning and preparation capabilities, along with expertise in creating professional data visualizations for communication. It demonstrates attention to detail in visual design, including color theory application and accessibility-focused chart design conventions.\n",
      "\n",
      "## Result/Impact\n",
      "The visualization successfully aggregated and presented survey responses from 12 career preference categories, with particle physics research emerging as the most popular choice among respondents. This provides clear insights into visitor interests at a major scientific institution.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 5: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_19\n",
      "ğŸ”— URL: https://tinyurl.com/wzjb5mfv\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "\n",
      "## Purpose\n",
      "Analyze personal Amazon order history to determine whether renewing an Amazon Prime membership is cost-effective after a 20% price increase, using historical spending patterns and modeled shipping costs.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export).\n",
      "\n",
      "## Technical Highlights\n",
      "Cleaned and normalized the raw Amazon order export, including sensitive-data removal, numeric coercion, and datetime parsing. Aggregated orders by order_id and order_date and derived year/month features to enable per-year analysis. Performed EDA with boxplots and histograms to demonstrate distributional skewness and justify median-based interpretation. Implemented a conservative shipping-cost model (assigning $7.99 for orders < $25) and aggregated yearly shipping costs to compare against the annual Prime fee, producing net savings estimates.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Data ingestion and cleaning, exploratory data analysis, statistical summary interpretation, scenario modeling and assumptions, and clear visualization for stakeholder decision-making within a reproducible Jupyter workflow.\n",
      "\n",
      "## Result/Impact\n",
      "Quantified the financial trade-off: using the modeled conservative shipping rates, the analysis shows net annual savings (example: 2021 would have incurred approximately $108.74 more in shipping without Prime), supporting renewal even after a 20% membership price increase. The notebook provides a repeatable method to reassess the decision annually.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 6: Creating SQL Database of Save-On-Foods products extracted using Web Scraping API\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_20\n",
      "ğŸ”— URL: https://tinyurl.com/39xdn7p8\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, Pandas, Requests, Numpy, JSON, SQLAlchemy, SQLite, Postman (web scraping API request prototyping)\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Creating SQL Database of Save-On-Foods products extracted using Web Scraping API\n",
      "\n",
      "## Purpose\n",
      "This project reverse uses web scraping to programmatically extract product data from the Save-On-Foods website to build a clean, queryable dataset for analysis and downstream tooling. It demonstrates a repeatable ETL workflow to turn paginated JSON API results into analytics-ready CSV and relational data.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Jupyter Notebook, Pandas, Requests, Numpy, JSON, SQLAlchemy, SQLite, Postman (web scraping API request prototyping).\n",
      "\n",
      "## Technical Highlights\n",
      "The solution identifies the vendor API via browser developer tools, converts the API response into structured data, and implements pagination by incrementing the API skip/take parameters to reliably iterate through results. JSON responses are normalized into tabular form using pandas' json_normalize, followed by targeted cleaning and column transformations to parse price and unit fields. The cleaned dataframe is exported to CSV and persisted into a SQLite relational table with explicit column types via SQLAlchemy for easy querying and integration.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Practical skills include API-driven data extraction, ETL design, JSON normalization, data cleaning and transformation, relational schema creation, and reproducible analysis in a notebook environment. It also demonstrates use of tooling for API inspection and request generation (Postman) and basic database engineering for analytics.\n",
      "\n",
      "## Result/Impact\n",
      "The pipeline extracted and consolidated 1,519 meat-related product records into a CSV and a SQLite database, enabling efficient SQL queries and downstream analysis. This reproducible workflow reduces manual scraping effort and provides a reliable foundation for price analysis, category insights, and inventory-style analytics.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 7: Alternative Protein Industry Market Analysis and Segmentation\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_4\n",
      "ğŸ”— URL: https://tinyurl.com/44ehkutz\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Scikit-Learn, Seaborn, Matplotlib, Plotly, GeoPandas, KModes, Data Storytelling\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Alternative Protein Industry Market Analysis and Segmentation\n",
      "\n",
      "## Purpose\n",
      "Analyzes the alternative protein company database from the Good Food Institute to understand the current state of the alternative protein market, focusing on industry trends, market segments, and regional distribution. This analysis reveals key insights into why the industry experienced a 21% sales decrease in 2023 despite earlier growth signals.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, NumPy, Scikit-Learn, Seaborn, Matplotlib, Plotly, GeoPandas, KModes, Data Storytelling\n",
      "\n",
      "## Technical Highlights\n",
      "Implemented comprehensive exploratory data analysis including automated profiling and multi-step data transformation pipelines with column denormalization and explosion of nested categorical values. Applied unsupervised machine learning using K-Modes clustering to identify three distinct market segments based on protein technology, industry focus, and geographic region. Developed time-series linear regression analysis across three distinct growth periods (1970-2014, 2015-2020, 2021-2023) to quantify changes in company formation rates and market momentum. Created interactive geospatial visualizations using Plotly and GeoPandas to map regional investment patterns and state-level distribution across the United States.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Proficiency in exploratory data analysis, statistical modeling, and unsupervised machine learning for market segmentation and business intelligence. Strong capabilities in data cleaning, normalization, and transformation for nested and categorical data structures, combined with advanced data visualization techniques for communicating complex insights to stakeholders.\n",
      "\n",
      "## Result/Impact\n",
      "Linear regression analysis revealed that alternative protein company formation peaked at 23 new companies per year during 2015-2020, but declined to a net loss of 76 companies annually between 2021-2023. Geographic analysis identified the USA and California as the epicenter of the industry, while segmentation analysis uncovered three distinct market segments: plant-based meat substitutes in Asia Pacific, biomass fermentation dairy alternatives in North America, and plant-based dairy alternatives in Europe.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 8: University World Rankings 2023\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_15\n",
      "ğŸ”— URL: https://tinyurl.com/4uwrz5tk\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Jupyter Notebook, JSON, Postman (web scraping API request prototyping), Flourish\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# University World Rankings 2023\n",
      "\n",
      "## Purpose\n",
      "Analyzes 12+ years of global university ranking data to visualize the dramatic shift in academic excellence, demonstrating China's rapid rise in the international higher education landscape while revealing competitive dynamics among world powers.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, NumPy, Jupyter Notebook, JSON, Postman (web scraping API request prototyping), Flourish\n",
      "\n",
      "## Technical Highlights\n",
      "The project demonstrates robust data engineering practices by extracting ranking data from a proprietary API through network analysis and Postman, then normalizing complex nested JSON structures into structured DataFrames using pd.json_normalize(). Advanced text processing was implemented using regex patterns to extract numeric values from mixed string-numeric columns, with sophisticated negative lookahead and lookbehind assertions. The pipeline implements intelligent data filteringâ€”selecting only stable ranking bands (top 200) to avoid ranged values that begin at rank 201â€”and employs temporal aggregation to group universities by country across multiple years for comparative analysis. The final data transformation pivots the aggregated dataset to an optimal format for visualization generation.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Data extraction and API integration through browser developer tools and HTTP clients, proficiency with data cleaning and transformation pipelines, and expertise in handling unstructured JSON and string-based numeric data. Demonstrates strong analytical thinking in data quality assessment by tracking null values before and after cleaning operations.\n",
      "\n",
      "## Result/Impact\n",
      "Created an animated visualization using Flourish that reveals China's overtaking of the United States in terms of high-quality academic research volume, providing actionable insights into the shifting global education landscape between 2011 and 2023.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 9: Government of Canada Food Recalls Data Extraction and Web Scraping Pipeline\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_13\n",
      "ğŸ”— URL: https://tinyurl.com/5n6mfjm3\n",
      "ğŸ’» Tech Stack: Python, Selenium, Beautiful Soup, Pandas, Jupyter Notebook, Chrome WebDriver\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Government of Canada Food Recalls Data Extraction and Web Scraping Pipeline\n",
      "\n",
      "## Purpose\n",
      "This project extracts comprehensive food recall data from the Government of Canada's official Recalls and Safety Alerts database (2011-2022), aggregating over 4,600 recall records across multiple years. The initiative addresses the need for structured, analyzable food safety data that enables researchers and analysts to identify patterns, trends, and risks in the Canadian food supply chain.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Selenium, Beautiful Soup, Pandas, Jupyter Notebook, Chrome WebDriver\n",
      "\n",
      "## Technical Highlights\n",
      "The solution implements a sophisticated two-stage web scraping architecture that handles dynamic HTML content and structural variations across a decade of data. Part I systematically extracts metadata (links, titles, summaries, dates) by paginating through 311 search result pages and parsing HTML elements. Part II demonstrates advanced error handling and schema adaptation by identifying and processing two distinct HTML patterns that emerged from website redesigns between 2021-2022 and 2011-2021, implementing conditional extraction logic with fallback mechanisms to maintain data consistency. The pipeline employs data cleaning techniques including regex-based filtering, datetime conversion, and deduplication, while strategically splitting processing into year-based batches to prevent memory exhaustion when processing thousands of URLs sequentially.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Proficiency in web automation and data extraction using Selenium and BeautifulSoup, combined with strong data manipulation and pipeline development using Pandas. Demonstrates expertise in handling real-world data integration challenges such as HTML schema evolution, robust error handling with try-except patterns, and implementing scalable batch processing strategies to manage large datasets.\n",
      "\n",
      "## Result/Impact\n",
      "Successfully extracted 4,680 food recall records spanning from 2011 to 2022, creating a comprehensive dataset of 12+ CSV files that consolidate detailed information including company names, recall classifications, affected audiences, and distribution details. This structured dataset provides a foundation for downstream analysis and enables data-driven insights into food safety trends across the Canadian market.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 10: AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_2\n",
      "ğŸ”— URL: https://tinyurl.com/4xktcfsm\n",
      "ğŸ’» Tech Stack: Python, Pandas, Plotly, Matplotlib\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "\n",
      "## Purpose\n",
      "This project evaluates different charting approaches for communicating multi-dimensional AI model performance data. It compares grouped bar charts, and bubble charts to demonstrate how appropriate chart selection significantly improves the clarity and accessibility of model-to-model and benchmark-to-benchmark performance comparisons.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, Plotly, Matplotlib\n",
      "\n",
      "## Technical Highlights\n",
      "The project implements a comparative analysis of three visualization approaches to handle three-dimensional benchmark data (models, benchmarks, and performance scores). Grouped bar charts segment models side-by-side within each benchmark but struggle with visual clutter and comparison difficulty across categories. Grouped bar charts compound this problem by making cross-category comparisons difficult and misrepresenting relative performance values. In contrast, bubble charts leverage color gradients and marker sizing to encode all three dimensions simultaneously, enabling direct visual comparison of model performance across benchmarks without cognitive overhead. The implementation applies advanced Plotly configurations including sequential color scales (RdBu_r and RdYlBu_r), interactive hover templates with contextual details, and heatmap visualizations that further enhance pattern recognition across benchmark-model matrices.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Demonstrates expertise in data visualization design principles, information architecture, and the ability to evaluate trade-offs between different chart typologies. Shows proficiency in selecting visualization techniques based on data structure and analytical requirements, with a focus on reducing cognitive load and improving stakeholder comprehension of complex comparative datasets.\n",
      "\n",
      "## Result/Impact\n",
      "Establishes bubble charts as the superior visualization method for multi-dimensional benchmark analysis, enabling rapid identification of model performance patterns and competitive advantages without requiring multiple chart interpretations. The visual clarity of encoded bubble size and color produces more actionable insights than traditional grouped bar approaches.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 11: Supermarket Sales Dashboard\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_12\n",
      "ğŸ”— URL: https://tinyurl.com/55967ndn\n",
      "ğŸ’» Tech Stack: Power BI\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Supermarket Sales Dashboard\n",
      "\n",
      "## Purpose\n",
      "Transforms raw transactional retail data into actionable business intelligence through an interactive visual analytics platform. Enables data-driven decision-making for inventory management, sales analysis, and customer purchasing patterns across product categories.\n",
      "\n",
      "## Tech Stack\n",
      "Power BI\n",
      "\n",
      "## Technical Highlights\n",
      "The project demonstrates comprehensive data visualization and analytical dashboard design, converting a large-scale transactional dataset (228,267+ invoice line items spanning multiple years) into an intuitive, interactive reporting interface. The dashboard implements multi-dimensional analysis across temporal, product, and customer dimensions with drill-down capabilities. Key analytical components include sales metrics aggregation, quantity and product categorization (dry items vs. chiller items), and temporal trend analysis. The solution showcases effective data modeling and DAX calculations to deliver real-time insights into retail performance metrics and customer behavior patterns.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Advanced data visualization design and business intelligence tool expertise with Power BI. Demonstrates proficiency in translating raw transactional data into strategic insights through interactive dashboards, data aggregation, and KPI tracking for retail analytics.\n",
      "\n",
      "## Result/Impact\n",
      "Successfully created an interactive analytics platform enabling stakeholders to visualize and analyze comprehensive sales data across multiple dimensions, facilitating faster decision-making and business performance monitoring.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 12: Optimizing Credit Card Rewards: Cash-Back vs. BMO Air Miles\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_18\n",
      "ğŸ”— URL: https://tinyurl.com/46vdv4wv\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Matplotlib, Seaborn, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Optimizing Credit Card Rewards: Cash-Back vs. BMO Air Miles\n",
      "\n",
      "## Purpose\n",
      "Evaluate whether switching from a cash-back credit card to a BMO Air Miles rewards card yields higher net value given real personal spending patterns, and provide a data-driven recommendation.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, NumPy, Matplotlib, Seaborn, Jupyter Notebook.\n",
      "\n",
      "## Technical Highlights\n",
      "Implemented a reproducible pipeline that ingests personal expense data, cleans mixed-format currency and missing values, and standardizes temporal fields for time-based aggregation. Modeled tiered rewards using floor-division logic to accurately reflect how miles are awarded across standard and BMO reward schemes, and computed alternative reward scenarios (standard, BMO, BMO World Elite) including annual-fee adjustments. Aggregated spending by year, merchant (e.g., Safeway), and payment method to isolate miles-earning transactions, and used visualizations and grouped statistics to surface purchase frequency and variance that drive rewards outcomes.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Data engineering and cleaning of real-world financial spreadsheets, quantitative reward-modeling and scenario analysis, exploratory data analysis and visualization, and translating analytical results into actionable financial recommendations.\n",
      "\n",
      "## Result/Impact\n",
      "The analysis found the BMO Air Miles World Elite card would have yielded approximately $300 in rewards for 2021â€”about $111 more than the current Scotiabank cash-back cardâ€”while 2022 results were mixed (estimated ~$167 vs ~$181), highlighting the importance of category-specific spending and temporal accumulation when choosing rewards products. This evidence-based assessment supports an informed card-switch decision and identifies high-impact spending categories (e.g., grocery purchases at Safeway) to maximize returns.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 13: Breakfast Cereals Nutritional Analysis Dashboard\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_11\n",
      "ğŸ”— URL: https://tinyurl.com/52b3kr8t\n",
      "ğŸ’» Tech Stack: Tableau\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Breakfast Cereals Nutritional Analysis Dashboard\n",
      "\n",
      "## Purpose\n",
      "This interactive dashboard evaluates the nutritional profiles of popular breakfast cereals against Canada's health guidelines, helping consumers make informed dietary choices. The project demonstrates the ability to translate raw nutritional data into actionable business intelligence for health-conscious decision making.\n",
      "\n",
      "## Tech Stack\n",
      "Tableau\n",
      "\n",
      "## Technical Highlights\n",
      "The dashboard leverages Tableau's advanced visualization capabilities to compare nutritional metrics across multiple cereal brands while referencing evidence-based health standards. The design implements interactive filtering and drill-down functionality, allowing users to explore correlations between nutritional components and health classifications. The workbook architecture demonstrates best practices in data organization, calculated fields, and dashboard layout optimization for clarity and user engagement. The visualization transforms structured nutritional datasets into an intuitive interface that supports rapid insights and comparative analysis.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Proficiency in data visualization design, dashboard development, and translating health/nutrition domain knowledge into interactive analytical tools. Strong understanding of user experience design principles to create accessible, professional analytics interfaces that communicate complex nutritional data effectively.\n",
      "\n",
      "## Result/Impact\n",
      "This project demonstrates the ability to create production-ready business intelligence solutions that were deployed on Tableau Public, making the analysis accessible to a broad audience. The interactive dashboard enables end-users to independently explore and evaluate nutritional information, extending the impact beyond static reporting.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 14: Food Safety Analytics Dashboard: Visualizing 12 Years of Canadian Regulatory Recalls\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_14\n",
      "ğŸ”— URL: https://tinyurl.com/mf246dkf\n",
      "ğŸ’» Tech Stack: Python, Power BI, Pandas, NumPy, Regex, Matplotlib, Seaborn, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Food Safety Analytics Dashboard: Visualizing 12 Years of Canadian Regulatory Recalls\n",
      "\n",
      "## Purpose\n",
      "This project analyzes over a decade of food safety recalls from the Canadian Food Inspection Agency to identify patterns, trends, and key risk factors in food safety incidents. By processing and visualizing historical recall data, the analysis provides actionable insights into the types of hazards, affected products, and geographic distribution of recalls across Canada. This is the second part of a two-part project following data extraction and web scraping. The first part is **Government of Canada Food Recalls Data Extraction and Web Scraping Pipeline**\n",
      "\n",
      "## Tech Stack\n",
      "Python, Power BI, Pandas, NumPy, Regex, Matplotlib, Seaborn, Jupyter Notebook\n",
      "\n",
      "## Technical Highlights\n",
      "The project implements advanced data extraction and normalization techniques using custom regex functions to parse unstructured government data and extract structured fields including hazard types, product categories, and distribution patterns. A sophisticated multi-step cleaning pipeline handles data inconsistencies, duplicate values, and misaligned columns through conditional replacement logic and slice-based indexing. The solution includes data denormalization using pandas explode() operations to handle multi-value fields (separated by commas), transforming hierarchical recall classifications into analytical structures. A reusable generalization function applies pattern matching across multiple columns to standardize variations in provincial names, ingredient terminology, and hazard classifications, enabling consistent analysis across 3,893 recalls spanning 2011 to 2022.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Demonstrates expertise in data cleaning, transformation, and exploratory data analysis on large unstructured datasets. Proficiency with regex pattern matching, pandas data manipulation, and custom functions for scalable text processing; combined with data visualization and business intelligence design for translating raw data into stakeholder-facing insights.\n",
      "\n",
      "## Result/Impact\n",
      "Successfully processed and cleaned 3,893 food safety recalls with standardized classification across hazard types, geographic distribution, and product categories. Generated interactive Power BI dashboards and time-series visualizations revealing trends in food safety incidents, enabling data-driven insights into microbiological contamination, allergen exposures, and extraneous material incidents.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 15: Market Basket Analysis for Customer Personalization\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_16\n",
      "ğŸ”— URL: https://tinyurl.com/26p7p38y\n",
      "ğŸ’» Tech Stack: Python, Pandas, Seaborn, Matplotlib, MLxtend, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Market Basket Analysis for Customer Personalization\n",
      "\n",
      "## Purpose\n",
      "This project applies association analysis to discover purchasing patterns and product affinities in e-commerce transactions. By identifying which items are frequently bought together, the analysis enables data-driven product recommendations and targeted cross-selling strategies to enhance customer engagement and sales across different geographic markets.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, Seaborn, Matplotlib, MLxtend, Jupyter Notebook\n",
      "\n",
      "## Technical Highlights\n",
      "The project implements the Apriori algorithm to generate frequent itemsets from transactional data, establishing support thresholds to identify items purchased together with statistical significance. Data preprocessing includes comprehensive cleaningâ€”removing outliers, handling null values, and standardizing country codesâ€”followed by one-hot encoding of product descriptions to prepare data for association rule mining. The analysis employs multiple filtering metrics including support, confidence, and lift to extract meaningful associations, with country-specific analysis demonstrating how purchasing behaviors vary geographically. Exploratory data analysis uncovers temporal demand patterns, revealing seasonality with peak sales in November and significant demand drops in specific months, which can inform inventory and marketing strategies.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Demonstrates expertise in data cleaning and validation, exploratory data analysis, and statistical pattern recognition using market basket analysis algorithms. Shows proficiency in applying machine learning techniques for actionable business insights, including feature engineering and multi-dimensional data aggregation.\n",
      "\n",
      "## Result/Impact\n",
      "The analysis identifies distinct purchasing patterns across different countriesâ€”such as Dutch customers preferring bundled lunchbox purchases and German customers favoring specific product combinations like plastersâ€”enabling personalized product recommendations tailored by geographic market. These insights support targeted cross-selling strategies and optimized product placement to increase average transaction value.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 16: Air Quality Index ETL to ML Prediction Pipeline\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_5\n",
      "ğŸ”— URL: https://tinyurl.com/3xdwhxes\n",
      "ğŸ’» Tech Stack: Python, Ploomber, DuckDB, MotherDuck, OpenAQ API, ARIMA, Pandas, Streamlit, Statsmodels, Jupyter Notebooks, Docker, GitHub Actions\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Air Quality Index ETL to ML Prediction Pipeline\n",
      "\n",
      "## Purpose\n",
      "This project develops an end-to-end ETL pipeline that automatically collects air quality measurements from the OpenAQ API and applies time-series forecasting to predict the Air Quality Index. The solution addresses the need for reliable air quality predictions by combining data engineering with statistical machine learning techniques.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Ploomber, DuckDB, MotherDuck, OpenAQ API, ARIMA, Pandas, Streamlit, Statsmodels, Jupyter Notebooks, Docker, GitHub Actions\n",
      "\n",
      "## Technical Highlights\n",
      "The project implements a production-grade data pipeline with automated hourly data ingestion via GitHub Actions, storing air quality measurements from real-world sensors in a cloud-managed data warehouse. The core machine learning component uses ARIMA (AutoRegressive Integrated Moving Average) for time-series forecasting, which performs stationarity testing and differencing to capture temporal dependencies in seasonal air quality patterns. The system integrates multiple layers: data extraction with error handling and validation, transformation with timezone conversion and outlier filtering, cloud data persistence via MotherDuck, and a user-facing Streamlit application deployed on Ploomber Cloud that enables interactive predictions with AQI classification bucketing (Good, Satisfactory, Moderate, Poor, Very Poor, Severe).\n",
      "\n",
      "## Skills Demonstrated\n",
      "Demonstrates expertise in orchestrating complex ETL workflows using modern workflow management tools, API integration with robust error handling, and time-series analysis with statistical methods. Additionally showcases cloud database architecture, containerization for deployment, and full-stack data product development from pipeline to production-ready web interface.\n",
      "\n",
      "## Result/Impact\n",
      "The system successfully processes real-time air quality data with automated scheduling, enabling stakeholders to access predictive AQI forecasts through an interactive web application. The ARIMA model captures temporal patterns in multi-parameter sensor data for accurate short-term air quality predictions.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 17: Food Safety Analytics: Environmental Monitoring Program for Food Safety Compliance\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_9\n",
      "ğŸ”— URL: https://tinyurl.com/52yv69e3\n",
      "ğŸ’» Tech Stack: Microsoft Excel, Microsoft Forms, Microsoft SharePoint, Pivot Tables\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Food Safety Analytics: Environmental Monitoring Program for Food Safety Compliance\n",
      "\n",
      "## Purpose\n",
      "Developed a comprehensive environmental monitoring program (EMP) to record, track, and analyze microbiological swabs across a 45,000 sq. ft. food manufacturing facility. The system ensures validation of food safety programs against pathogens while maintaining full traceability and supporting GFSI certifications (SQF, FSSC 22000, HACCP).\n",
      "\n",
      "## Tech Stack\n",
      "Microsoft Excel, Microsoft Forms, Microsoft SharePoint, Pivot Tables\n",
      "\n",
      "## Technical Highlights\n",
      "Built a multi-layered data architecture with automated data collection via Microsoft Forms that flows directly into Excel, eliminating manual entry errors and ensuring real-time traceability. Implemented a sophisticated concatenation algorithm that generates standardized sample nomenclature based on swab type, location, date, and zone identifiersâ€”critical for tracking samples across 261+ monthly swabs. Engineered a statistical binning approach for CFU/swab analysis, grouping results into 100 CFU/swab ranges to handle high variability in pathogenic indicator organisms (aerobic count, yeast, mold). Leveraged pivot tables and pivot charts to transform raw microbiological data into actionable insights across multiple production zones.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Demonstrates expertise in designing end-to-end data systems for quality assurance, including data collection automation, standardization protocols, and advanced statistical analysis techniques for microbiological data. Shows proficiency in data cleaning, normalization, and analysis of complex food safety parameters with high variability, combined with knowledge of regulatory compliance requirements for food manufacturing.\n",
      "\n",
      "## Result/Impact\n",
      "Revealed that sanitation practices improved through the monitoring period with decreasing indicator organism counts in subsequent months. Identified that Zone 4 maintained E. coli and total coliform counts below detectable limits, validating effective hand washing practices as part of Good Manufacturing Practices. Provided data-driven evidence showing Listeria-positive results concentrated in drain locations, enabling targeted sanitation procedure optimization.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 18: Wage Gap BC Minimum vs. Living Wage 2025\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_0\n",
      "ğŸ”— URL: https://tinyurl.com/yjy6vdfr\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Wage Gap BC Minimum vs. Living Wage 2025\n",
      "\n",
      "## Purpose\n",
      "Analyzes the economic gap between British Columbia's minimum wage and living wage to highlight the challenge faced by minimum wage workers. This project uses strategic data visualization to communicate a critical social and economic issue affecting workers in BC.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "\n",
      "## Technical Highlights\n",
      "Implements a slope chart visualization techniqueâ€”a sophisticated data storytelling approach that effectively communicates the magnitude of wage disparity between two data points. The project demonstrates advanced matplotlib customization including custom axis formatters, multi-layer point plotting, strategic color palette selection, and high-resolution image export at 300 DPI. The visualization employs professional design principles such as visual hierarchy through point sizing, color differentiation to distinguish wage types, and supporting reference lines to enhance comparative analysis. Shows expertise in data manipulation using Pandas and NumPy for structured data preparation, combined with production-ready code practices that balance aesthetic appeal with analytical clarity.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Advanced data visualization design and narrative communication, with proficiency in translating numerical data into compelling visual stories. Demonstrates expertise in choosing appropriate visualization types for specific analytical goals, implementing professional chart styling, and optimizing visual design for clarity and impact. Strong capability in creating data-driven narratives that communicate complex economic information to diverse audiences while maintaining analytical rigor.\n",
      "\n",
      "## Result/Impact\n",
      "Quantified the wage gap at $9.20 CAD per hour, representing a 51% shortfall where living wage exceeds minimum wage. Generated a professional, publication-quality slope chart visualization that effectively illustrates economic disparity and serves as a powerful communication tool for stakeholder engagement and data-driven discourse.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 19: NOC Wage Finder: Interactive NOC-Based Salary Explorer\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_6\n",
      "ğŸ”— URL: https://tinyurl.com/4yu9pep9\n",
      "ğŸ’» Tech Stack: Python, Pandas, Streamlit, Plotly, Selenium, BeautifulSoup, Numpy, String Matching (difflib)\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# NOC Wage Finder: Interactive NOC-Based Salary Explorer\n",
      "\n",
      "## Purpose\n",
      "Provides Canadian job seekers, newcomers, and international students with reliable, province-specific wage data for occupations classified under the National Occupational Classification system. The application enables informed career and relocation decisions by consolidating wage information from multiple government sources into an accessible, searchable interface.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, Streamlit, Plotly, Selenium, BeautifulSoup, Numpy, String Matching (difflib)\n",
      "\n",
      "## Technical Highlights\n",
      "The project implements a multi-stage data pipeline that combines web scraping with sophisticated data reconciliation techniques. Selenium and BeautifulSoup are used to extract NOC codes and occupational classifications from government websites, processing paginated tables across 60+ pages. A custom string matching algorithm using Python's difflib library identifies equivalent job titles across NOC 2016 and 2021 classification systems, resolving discrepancies caused by nomenclature changes with configurable similarity thresholds. The data integration merges wage statistics from multiple government surveys (Census, Labour Force Survey, Employment Insurance) with NOC codes, transforming the consolidated dataset into geographic visualizations using Plotly's Choropleth maps within a Streamlit web application.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Demonstrated expertise in full-stack data engineering including web scraping automation, data cleaning and validation, exploratory data analysis, and relational data joining strategies. Proficient in building interactive data applications with responsive user interfaces, geospatial visualization design, and end-to-end data pipeline orchestration from source collection through visualization delivery.\n",
      "\n",
      "## Result/Impact\n",
      "Successfully deployed a live web application serving real wage data across all Canadian provinces and occupations, transforming disparate government data sources into a unified, searchable platform. The application provides accessible labor market intelligence to support career planning decisions for international audiences.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 20: Google Diet Search Trends 2020 - Canada Geographic Analytics Dashboard\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_10\n",
      "ğŸ”— URL: https://tinyurl.com/mrxvv8un\n",
      "ğŸ’» Tech Stack: Tableau\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Google Diet Search Trends 2020 - Canada Geographic Analytics Dashboard\n",
      "\n",
      "## Purpose\n",
      "This project analyzes the most searched diet trends across Canadian regions in 2020 using Google Trends data, transforming raw search interest patterns into interactive visualizations. The dashboard provides insights into consumer interest in various diet types including Sirtfood, GERD, Candida, Keto, Plant-based, Anti-inflammatory, Fatty-liver, DASH, and Alkaline diets.\n",
      "\n",
      "## Tech Stack\n",
      "Tableau\n",
      "\n",
      "## Technical Highlights\n",
      "The project aggregates multi-year Google Trends data (2019-2020) across 10 different diet categories, structuring it for both temporal trend analysis and geographic subregion comparisons. The data pipeline processes interest metrics over time and by location, enabling dual-view analytics that reveal seasonal patterns and regional preferences. The interactive Tableau dashboard integrates multiple datasets with filtering capabilities, allowing users to explore diet interest trends dynamically across the Canadian market. The visualization architecture separates time-series analysis from geospatial breakdowns, optimizing query performance and user exploration patterns.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Demonstrated expertise in data aggregation and structuring from external sources, creating interactive business intelligence dashboards, and translating raw trend data into actionable consumer insights. Proficiency in designing user-friendly analytics interfaces that balance comprehensiveness with accessibility for stakeholder decision-making.\n",
      "\n",
      "## Result/Impact\n",
      "The dashboard enables data-driven identification of emerging diet trends and regional preferences, supporting marketing and business strategy decisions in the health and wellness sector. The interactive visualization framework facilitates rapid exploration of multi-dimensional trend data across ten diet categories and multiple Canadian regions.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROJECT 21: Optimizing Ad Campaign: 62% Reduction in Ad Spend\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_7\n",
      "ğŸ”— URL: https://tinyurl.com/ms8vv7my\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Seaborn, Jupyter Notebook, Power BI\n",
      "ğŸ“¦ Chunk Type: full_content\n",
      "ğŸ·ï¸  Section Type: project_full\n",
      "\n",
      "ğŸ“„ Full Content\n",
      "# Optimizing Ad Campaign: 62% Reduction in Ad Spend\n",
      "\n",
      "## Purpose\n",
      "This project analyzes the return on ad spend (ROAS) performance across an advertising campaign to identify underperforming keywords and revenue optimization opportunities. The analysis provides data-driven recommendations to eliminate low-ROI keywords and improve overall campaign profitability.\n",
      "\n",
      "## Tech Stack\n",
      "Python, Pandas, NumPy, Seaborn, Jupyter Notebook, Power BI\n",
      "\n",
      "## Technical Highlights\n",
      "The analysis implements comprehensive data preprocessing, cleaning missing values and standardizing data formats from raw campaign data containing multiple keyword performance attributes. Advanced filtering and statistical analysis segments keywords by ROAS thresholds to isolate high-performing versus underperforming groups, with median-based stratification identifying top-tier performers. Word frequency analysis uses tokenization and value counting on high-performing keywords to identify common word patterns, generating actionable keyword composition recommendations. The solution provides both statistical summaries and interactive Power BI visualizations for business stakeholder communication.\n",
      "\n",
      "## Skills Demonstrated\n",
      "Data cleaning and transformation, exploratory data analysis, statistical analysis and filtering, feature engineering through text processing, and business intelligence visualization. Demonstrates ability to translate raw data into actionable business insights with clear ROI impact.\n",
      "\n",
      "## Result/Impact\n",
      "Identified $1,623 in annual advertising spend savings opportunity by removing keywords with ROAS below 1 (62% reduction in inefficient spend). Determined that 63% of underperforming keywords are automatically generated, enabling platform optimization recommendations. Provided specific keyword composition patterns (dairy, cheese, cream, free combinations) associated with higher profitability for targeted keyword experimentation.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ğŸ“Š Summary of All Full Project Chunks (21 total):\n",
      "\n",
      "1. Customer Review Sentiment Analysis\n",
      "   Tech: Python, Streamlit, Transformers, PyTorch, NLTK, Pandas, NumPy, Matplotlib, Seaborn, Scipy\n",
      "\n",
      "2. Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "   Tech: Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "\n",
      "3. Sales Forecasting with ML XGBoost Feature Importance for Retail Optimization\n",
      "   Tech: Python, Jupyter Notebook, Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn\n",
      "\n",
      "4. CERN Career Preferences Analysis: A Data Visualization\n",
      "   Tech: Python, Pandas, Seaborn, Matplotlib\n",
      "\n",
      "5. Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "   Tech: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "\n",
      "6. Creating SQL Database of Save-On-Foods products extracted using Web Scraping API\n",
      "   Tech: Python, Jupyter Notebook, Pandas, Requests, Numpy, JSON, SQLAlchemy, SQLite, Postman (web scraping API request prototyping)\n",
      "\n",
      "7. Alternative Protein Industry Market Analysis and Segmentation\n",
      "   Tech: Python, Pandas, NumPy, Scikit-Learn, Seaborn, Matplotlib, Plotly, GeoPandas, KModes, Data Storytelling\n",
      "\n",
      "8. University World Rankings 2023\n",
      "   Tech: Python, Pandas, NumPy, Jupyter Notebook, JSON, Postman (web scraping API request prototyping), Flourish\n",
      "\n",
      "9. Government of Canada Food Recalls Data Extraction and Web Scraping Pipeline\n",
      "   Tech: Python, Selenium, Beautiful Soup, Pandas, Jupyter Notebook, Chrome WebDriver\n",
      "\n",
      "10. AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "   Tech: Python, Pandas, Plotly, Matplotlib\n",
      "\n",
      "11. Supermarket Sales Dashboard\n",
      "   Tech: Power BI\n",
      "\n",
      "12. Optimizing Credit Card Rewards: Cash-Back vs. BMO Air Miles\n",
      "   Tech: Python, Pandas, NumPy, Matplotlib, Seaborn, Jupyter Notebook\n",
      "\n",
      "13. Breakfast Cereals Nutritional Analysis Dashboard\n",
      "   Tech: Tableau\n",
      "\n",
      "14. Food Safety Analytics Dashboard: Visualizing 12 Years of Canadian Regulatory Recalls\n",
      "   Tech: Python, Power BI, Pandas, NumPy, Regex, Matplotlib, Seaborn, Jupyter Notebook\n",
      "\n",
      "15. Market Basket Analysis for Customer Personalization\n",
      "   Tech: Python, Pandas, Seaborn, Matplotlib, MLxtend, Jupyter Notebook\n",
      "\n",
      "16. Air Quality Index ETL to ML Prediction Pipeline\n",
      "   Tech: Python, Ploomber, DuckDB, MotherDuck, OpenAQ API, ARIMA, Pandas, Streamlit, Statsmodels, Jupyter Notebooks, Docker, GitHub Actions\n",
      "\n",
      "17. Food Safety Analytics: Environmental Monitoring Program for Food Safety Compliance\n",
      "   Tech: Microsoft Excel, Microsoft Forms, Microsoft SharePoint, Pivot Tables\n",
      "\n",
      "18. Wage Gap BC Minimum vs. Living Wage 2025\n",
      "   Tech: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "\n",
      "19. NOC Wage Finder: Interactive NOC-Based Salary Explorer\n",
      "   Tech: Python, Pandas, Streamlit, Plotly, Selenium, BeautifulSoup, Numpy, String Matching (difflib)\n",
      "\n",
      "20. Google Diet Search Trends 2020 - Canada Geographic Analytics Dashboard\n",
      "   Tech: Tableau\n",
      "\n",
      "21. Optimizing Ad Campaign: 62% Reduction in Ad Spend\n",
      "   Tech: Python, Pandas, NumPy, Seaborn, Jupyter Notebook, Power BI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for full content chunks\n",
    "full_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_full\"))]\n",
    ")\n",
    "\n",
    "full_records, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    scroll_filter=full_filter,\n",
    "    limit=1000,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“š Full Project Content Chunks (project_full)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"ğŸ’¡ These chunks contain ALL sections for complete project context\\n\")\n",
    "\n",
    "for i, record in enumerate(full_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PROJECT {i}: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ†” Project ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ“¦ Chunk Type: {metadata.get('chunk_type', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Full Content\")\n",
    "    print(payload.get('content', 'N/A'))\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Summary of all full projects\n",
    "print(f\"\\nğŸ“Š Summary of All Full Project Chunks ({len(full_records)} total):\\n\")\n",
    "for i, record in enumerate(full_records, 1):\n",
    "    metadata = record.payload.get('metadata', {})\n",
    "    print(f\"{i}. {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"   Tech: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da1d51",
   "metadata": {},
   "source": [
    "### View Full Project Content (project_full chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a4d4200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Technical Summary Chunks (project_technical)\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ These chunks contain Tech Stack + Technical Highlights for fast matching\n",
      "\n",
      "================================================================================\n",
      "PROJECT 1: Optimizing Ad Campaign: 62% Reduction in Ad Spend\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_7\n",
      "ğŸ”— URL: https://tinyurl.com/ms8vv7my\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Seaborn, Jupyter Notebook, Power BI\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Optimizing Ad Campaign: 62% Reduction in Ad Spend\n",
      "\n",
      "Technologies: Python, Pandas, NumPy, Seaborn, Jupyter Notebook, Power BI\n",
      "\n",
      "Technical Work:\n",
      "The analysis implements comprehensive data preprocessing, cleaning missing values and standardizing data formats from raw campaign data containing mul...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 2: Air Quality Index ETL to ML Prediction Pipeline\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_5\n",
      "ğŸ”— URL: https://tinyurl.com/3xdwhxes\n",
      "ğŸ’» Tech Stack: Python, Ploomber, DuckDB, MotherDuck, OpenAQ API, ARIMA, Pandas, Streamlit, Statsmodels, Jupyter Notebooks, Docker, GitHub Actions\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Air Quality Index ETL to ML Prediction Pipeline\n",
      "\n",
      "Technologies: Python, Ploomber, DuckDB, MotherDuck, OpenAQ API, ARIMA, Pandas, Streamlit, Statsmodels, Jupyter Notebooks, Docker, GitHub Actions\n",
      "\n",
      "Technical Work:\n",
      "The project implements a production-grade data pipeline with automated hourly da...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 3: Breakfast Cereals Nutritional Analysis Dashboard\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_11\n",
      "ğŸ”— URL: https://tinyurl.com/52b3kr8t\n",
      "ğŸ’» Tech Stack: Tableau\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Breakfast Cereals Nutritional Analysis Dashboard\n",
      "\n",
      "Technologies: Tableau\n",
      "\n",
      "Technical Work:\n",
      "The dashboard leverages Tableau's advanced visualization capabilities to compare nutritional metrics across multiple cereal brands while referencing evidence-based health standards. The design implement...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 4: Alternative Protein Industry Market Analysis and Segmentation\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_4\n",
      "ğŸ”— URL: https://tinyurl.com/44ehkutz\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Scikit-Learn, Seaborn, Matplotlib, Plotly, GeoPandas, KModes, Data Storytelling\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Alternative Protein Industry Market Analysis and Segmentation\n",
      "\n",
      "Technologies: Python, Pandas, NumPy, Scikit-Learn, Seaborn, Matplotlib, Plotly, GeoPandas, KModes, Data Storytelling\n",
      "\n",
      "Technical Work:\n",
      "Implemented comprehensive exploratory data analysis including automated profiling and multi-st...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 5: NOC Wage Finder: Interactive NOC-Based Salary Explorer\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_6\n",
      "ğŸ”— URL: https://tinyurl.com/4yu9pep9\n",
      "ğŸ’» Tech Stack: Python, Pandas, Streamlit, Plotly, Selenium, BeautifulSoup, Numpy, String Matching (difflib)\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: NOC Wage Finder: Interactive NOC-Based Salary Explorer\n",
      "\n",
      "Technologies: Python, Pandas, Streamlit, Plotly, Selenium, BeautifulSoup, Numpy, String Matching (difflib)\n",
      "\n",
      "Technical Work:\n",
      "The project implements a multi-stage data pipeline that combines web scraping with sophisticated data reconcili...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 6: Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_1\n",
      "ğŸ”— URL: https://tinyurl.com/2wxtb6af\n",
      "ğŸ’» Tech Stack: Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "\n",
      "Technologies: Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "\n",
      "Technical Work:\n",
      "The project implements a sophisticated data transformation pipeline that processes raw CSV data with custom cleaning logic, including comm...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 7: CERN Career Preferences Analysis: A Data Visualization\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_3\n",
      "ğŸ”— URL: https://tinyurl.com/hnexp2r8\n",
      "ğŸ’» Tech Stack: Python, Pandas, Seaborn, Matplotlib\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: CERN Career Preferences Analysis: A Data Visualization\n",
      "\n",
      "Technologies: Python, Pandas, Seaborn, Matplotlib\n",
      "\n",
      "Technical Work:\n",
      "The project demonstrates proficiency in data transformation and visualization with pandas for dataset organization and sorting. Custom styling techniques are applied th...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 8: Food Safety Analytics Dashboard: Visualizing 12 Years of Canadian Regulatory Recalls\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_14\n",
      "ğŸ”— URL: https://tinyurl.com/mf246dkf\n",
      "ğŸ’» Tech Stack: Python, Power BI, Pandas, NumPy, Regex, Matplotlib, Seaborn, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Food Safety Analytics Dashboard: Visualizing 12 Years of Canadian Regulatory Recalls\n",
      "\n",
      "Technologies: Python, Power BI, Pandas, NumPy, Regex, Matplotlib, Seaborn, Jupyter Notebook\n",
      "\n",
      "Technical Work:\n",
      "The project implements advanced data extraction and normalization techniques using custom regex ...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 9: Sales Forecasting with ML XGBoost Feature Importance for Retail Optimization\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_17\n",
      "ğŸ”— URL: https://tinyurl.com/38urcn9d\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Sales Forecasting with ML XGBoost Feature Importance for Retail Optimization\n",
      "\n",
      "Technologies: Python, Jupyter Notebook, Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn\n",
      "\n",
      "Technical Work:\n",
      "Implemented an end-to-end machine learning pipeline encompassing data cleaning, exploratory data a...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 10: Optimizing Credit Card Rewards: Cash-Back vs. BMO Air Miles\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_18\n",
      "ğŸ”— URL: https://tinyurl.com/46vdv4wv\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Matplotlib, Seaborn, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Optimizing Credit Card Rewards: Cash-Back vs. BMO Air Miles\n",
      "\n",
      "Technologies: Python, Pandas, NumPy, Matplotlib, Seaborn, Jupyter Notebook.\n",
      "\n",
      "Technical Work:\n",
      "Implemented a reproducible pipeline that ingests personal expense data, cleans mixed-format currency and missing values, and standardizes...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 11: Google Diet Search Trends 2020 - Canada Geographic Analytics Dashboard\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_10\n",
      "ğŸ”— URL: https://tinyurl.com/mrxvv8un\n",
      "ğŸ’» Tech Stack: Tableau\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Google Diet Search Trends 2020 - Canada Geographic Analytics Dashboard\n",
      "\n",
      "Technologies: Tableau\n",
      "\n",
      "Technical Work:\n",
      "The project aggregates multi-year Google Trends data (2019-2020) across 10 different diet categories, structuring it for both temporal trend analysis and geographic subregion compa...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 12: Government of Canada Food Recalls Data Extraction and Web Scraping Pipeline\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_13\n",
      "ğŸ”— URL: https://tinyurl.com/5n6mfjm3\n",
      "ğŸ’» Tech Stack: Python, Selenium, Beautiful Soup, Pandas, Jupyter Notebook, Chrome WebDriver\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Government of Canada Food Recalls Data Extraction and Web Scraping Pipeline\n",
      "\n",
      "Technologies: Python, Selenium, Beautiful Soup, Pandas, Jupyter Notebook, Chrome WebDriver\n",
      "\n",
      "Technical Work:\n",
      "The solution implements a sophisticated two-stage web scraping architecture that handles dynamic HTML cont...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 13: Wage Gap BC Minimum vs. Living Wage 2025\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_0\n",
      "ğŸ”— URL: https://tinyurl.com/yjy6vdfr\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Wage Gap BC Minimum vs. Living Wage 2025\n",
      "\n",
      "Technologies: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "\n",
      "Technical Work:\n",
      "Implements a slope chart visualization techniqueâ€”a sophisticated data storytelling approach that effectively communicates the magnitude of wage disparity bet...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 14: University World Rankings 2023\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_15\n",
      "ğŸ”— URL: https://tinyurl.com/4uwrz5tk\n",
      "ğŸ’» Tech Stack: Python, Pandas, NumPy, Jupyter Notebook, JSON, Postman (web scraping API request prototyping), Flourish\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: University World Rankings 2023\n",
      "\n",
      "Technologies: Python, Pandas, NumPy, Jupyter Notebook, JSON, Postman (web scraping API request prototyping), Flourish\n",
      "\n",
      "Technical Work:\n",
      "The project demonstrates robust data engineering practices by extracting ranking data from a proprietary API through network...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 15: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_19\n",
      "ğŸ”— URL: https://tinyurl.com/wzjb5mfv\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "\n",
      "Technologies: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export).\n",
      "\n",
      "Technical Work:\n",
      "Cleaned and normalized the raw Amazon order export, including sensitive-data removal, numeric coercion, and datetime pa...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 16: Customer Review Sentiment Analysis\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_8\n",
      "ğŸ”— URL: https://tinyurl.com/3baruh6x\n",
      "ğŸ’» Tech Stack: Python, Streamlit, Transformers, PyTorch, NLTK, Pandas, NumPy, Matplotlib, Seaborn, Scipy\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Customer Review Sentiment Analysis\n",
      "\n",
      "Technologies: Python, Streamlit, Transformers, PyTorch, NLTK, Pandas, NumPy, Matplotlib, Seaborn, Scipy\n",
      "\n",
      "Technical Work:\n",
      "The project implements a comparative analysis of two distinct sentiment analysis approaches: a traditional VADER (Valence Aware Dictio...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 17: Food Safety Analytics: Environmental Monitoring Program for Food Safety Compliance\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_9\n",
      "ğŸ”— URL: https://tinyurl.com/52yv69e3\n",
      "ğŸ’» Tech Stack: Microsoft Excel, Microsoft Forms, Microsoft SharePoint, Pivot Tables\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Food Safety Analytics: Environmental Monitoring Program for Food Safety Compliance\n",
      "\n",
      "Technologies: Microsoft Excel, Microsoft Forms, Microsoft SharePoint, Pivot Tables\n",
      "\n",
      "Technical Work:\n",
      "Built a multi-layered data architecture with automated data collection via Microsoft Forms that flows direc...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 18: Creating SQL Database of Save-On-Foods products extracted using Web Scraping API\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_20\n",
      "ğŸ”— URL: https://tinyurl.com/39xdn7p8\n",
      "ğŸ’» Tech Stack: Python, Jupyter Notebook, Pandas, Requests, Numpy, JSON, SQLAlchemy, SQLite, Postman (web scraping API request prototyping)\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Creating SQL Database of Save-On-Foods products extracted using Web Scraping API\n",
      "\n",
      "Technologies: Python, Jupyter Notebook, Pandas, Requests, Numpy, JSON, SQLAlchemy, SQLite, Postman (web scraping API request prototyping).\n",
      "\n",
      "Technical Work:\n",
      "The solution identifies the vendor API via browser de...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 19: Market Basket Analysis for Customer Personalization\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_16\n",
      "ğŸ”— URL: https://tinyurl.com/26p7p38y\n",
      "ğŸ’» Tech Stack: Python, Pandas, Seaborn, Matplotlib, MLxtend, Jupyter Notebook\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Market Basket Analysis for Customer Personalization\n",
      "\n",
      "Technologies: Python, Pandas, Seaborn, Matplotlib, MLxtend, Jupyter Notebook\n",
      "\n",
      "Technical Work:\n",
      "The project implements the Apriori algorithm to generate frequent itemsets from transactional data, establishing support thresholds to identify ...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 20: AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_2\n",
      "ğŸ”— URL: https://tinyurl.com/4xktcfsm\n",
      "ğŸ’» Tech Stack: Python, Pandas, Plotly, Matplotlib\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "\n",
      "Technologies: Python, Pandas, Plotly, Matplotlib\n",
      "\n",
      "Technical Work:\n",
      "The project implements a comparative analysis of three visualization approaches to handle three-dimensional benchmark data (models, benchmarks, and performanc...\n",
      "\n",
      "================================================================================\n",
      "PROJECT 21: Supermarket Sales Dashboard\n",
      "================================================================================\n",
      "ğŸ†” Project ID: project_12\n",
      "ğŸ”— URL: https://tinyurl.com/55967ndn\n",
      "ğŸ’» Tech Stack: Power BI\n",
      "ğŸ“¦ Chunk Type: technical_summary\n",
      "ğŸ·ï¸  Section Type: project_technical\n",
      "\n",
      "ğŸ“„ Content Preview (first 300 chars):\n",
      "   Project: Supermarket Sales Dashboard\n",
      "\n",
      "Technologies: Power BI\n",
      "\n",
      "Technical Work:\n",
      "The project demonstrates comprehensive data visualization and analytical dashboard design, converting a large-scale transactional dataset (228,267+ invoice line items spanning multiple years) into an intuitive, interactive...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for technical summary chunks\n",
    "tech_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"project_technical\"))]\n",
    ")\n",
    "\n",
    "tech_records, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    scroll_filter=tech_filter,\n",
    "    limit=100,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ”§ Technical Summary Chunks (project_technical)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"ğŸ’¡ These chunks contain Tech Stack + Technical Highlights for fast matching\\n\")\n",
    "\n",
    "for i, record in enumerate(tech_records, 1):\n",
    "    payload = record.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PROJECT {i}: {metadata.get('project_title', 'N/A')}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ†” Project ID: {metadata.get('project_id', 'N/A')}\")\n",
    "    print(f\"ğŸ”— URL: {metadata.get('project_url', 'N/A')}\")\n",
    "    print(f\"ğŸ’» Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "    print(f\"ğŸ“¦ Chunk Type: {metadata.get('chunk_type', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    print(f\"\\nğŸ“„ Content Preview (first 300 chars):\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:300]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41969762",
   "metadata": {},
   "source": [
    "### View Technical Summaries (project_technical chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6099a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Projects Collection Overview\n",
      "================================================================================\n",
      "\n",
      "Total chunks in collection: 42\n",
      "Total unique projects: 21\n",
      "\n",
      "ğŸ’¡ Each project has 2 chunks: technical_summary + full_content\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¦ Project: Wage Gap BC Minimum vs. Living Wage 2025\n",
      "   ID: project_0\n",
      "   URL: https://tinyurl.com/yjy6vdfr\n",
      "   Tech Stack: Python, Pandas, NumPy, Seaborn, Matplotlib, Jupyter Notebook\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Canadian Grain Production Analysis: Visualizing Data with Optimal Color Scales\n",
      "   ID: project_1\n",
      "   URL: https://tinyurl.com/2wxtb6af\n",
      "   Tech Stack: Python, Pandas, Plotly, Matplotlib, NumPy\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Google Diet Search Trends 2020 - Canada Geographic Analytics Dashboard\n",
      "   ID: project_10\n",
      "   URL: https://tinyurl.com/mrxvv8un\n",
      "   Tech Stack: Tableau\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Breakfast Cereals Nutritional Analysis Dashboard\n",
      "   ID: project_11\n",
      "   URL: https://tinyurl.com/52b3kr8t\n",
      "   Tech Stack: Tableau\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Supermarket Sales Dashboard\n",
      "   ID: project_12\n",
      "   URL: https://tinyurl.com/55967ndn\n",
      "   Tech Stack: Power BI\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Government of Canada Food Recalls Data Extraction and Web Scraping Pipeline\n",
      "   ID: project_13\n",
      "   URL: https://tinyurl.com/5n6mfjm3\n",
      "   Tech Stack: Python, Selenium, Beautiful Soup, Pandas, Jupyter Notebook, Chrome WebDriver\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Food Safety Analytics Dashboard: Visualizing 12 Years of Canadian Regulatory Recalls\n",
      "   ID: project_14\n",
      "   URL: https://tinyurl.com/mf246dkf\n",
      "   Tech Stack: Python, Power BI, Pandas, NumPy, Regex, Matplotlib, Seaborn, Jupyter Notebook\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: University World Rankings 2023\n",
      "   ID: project_15\n",
      "   URL: https://tinyurl.com/4uwrz5tk\n",
      "   Tech Stack: Python, Pandas, NumPy, Jupyter Notebook, JSON, Postman (web scraping API request prototyping), Flourish\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Market Basket Analysis for Customer Personalization\n",
      "   ID: project_16\n",
      "   URL: https://tinyurl.com/26p7p38y\n",
      "   Tech Stack: Python, Pandas, Seaborn, Matplotlib, MLxtend, Jupyter Notebook\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Sales Forecasting with ML XGBoost Feature Importance for Retail Optimization\n",
      "   ID: project_17\n",
      "   URL: https://tinyurl.com/38urcn9d\n",
      "   Tech Stack: Python, Jupyter Notebook, Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Optimizing Credit Card Rewards: Cash-Back vs. BMO Air Miles\n",
      "   ID: project_18\n",
      "   URL: https://tinyurl.com/46vdv4wv\n",
      "   Tech Stack: Python, Pandas, NumPy, Matplotlib, Seaborn, Jupyter Notebook\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Renew Amazon Prime? A Cost-Benefit Analysis\n",
      "   ID: project_19\n",
      "   URL: https://tinyurl.com/wzjb5mfv\n",
      "   Tech Stack: Python, Jupyter Notebook, pandas, numpy, seaborn, matplotlib, CSV data (personal order export)\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: AI Model Performance Visualization: Grouped Bar vs. Bubble Charts\n",
      "   ID: project_2\n",
      "   URL: https://tinyurl.com/4xktcfsm\n",
      "   Tech Stack: Python, Pandas, Plotly, Matplotlib\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Creating SQL Database of Save-On-Foods products extracted using Web Scraping API\n",
      "   ID: project_20\n",
      "   URL: https://tinyurl.com/39xdn7p8\n",
      "   Tech Stack: Python, Jupyter Notebook, Pandas, Requests, Numpy, JSON, SQLAlchemy, SQLite, Postman (web scraping API request prototyping)\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: CERN Career Preferences Analysis: A Data Visualization\n",
      "   ID: project_3\n",
      "   URL: https://tinyurl.com/hnexp2r8\n",
      "   Tech Stack: Python, Pandas, Seaborn, Matplotlib\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Alternative Protein Industry Market Analysis and Segmentation\n",
      "   ID: project_4\n",
      "   URL: https://tinyurl.com/44ehkutz\n",
      "   Tech Stack: Python, Pandas, NumPy, Scikit-Learn, Seaborn, Matplotlib, Plotly, GeoPandas, KModes, Data Storytelling\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Air Quality Index ETL to ML Prediction Pipeline\n",
      "   ID: project_5\n",
      "   URL: https://tinyurl.com/3xdwhxes\n",
      "   Tech Stack: Python, Ploomber, DuckDB, MotherDuck, OpenAQ API, ARIMA, Pandas, Streamlit, Statsmodels, Jupyter Notebooks, Docker, GitHub Actions\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: NOC Wage Finder: Interactive NOC-Based Salary Explorer\n",
      "   ID: project_6\n",
      "   URL: https://tinyurl.com/4yu9pep9\n",
      "   Tech Stack: Python, Pandas, Streamlit, Plotly, Selenium, BeautifulSoup, Numpy, String Matching (difflib)\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Optimizing Ad Campaign: 62% Reduction in Ad Spend\n",
      "   ID: project_7\n",
      "   URL: https://tinyurl.com/ms8vv7my\n",
      "   Tech Stack: Python, Pandas, NumPy, Seaborn, Jupyter Notebook, Power BI\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Customer Review Sentiment Analysis\n",
      "   ID: project_8\n",
      "   URL: https://tinyurl.com/3baruh6x\n",
      "   Tech Stack: Python, Streamlit, Transformers, PyTorch, NLTK, Pandas, NumPy, Matplotlib, Seaborn, Scipy\n",
      "   Chunks: 2 (technical + full)\n",
      "\n",
      "ğŸ“¦ Project: Food Safety Analytics: Environmental Monitoring Program for Food Safety Compliance\n",
      "   ID: project_9\n",
      "   URL: https://tinyurl.com/52yv69e3\n",
      "   Tech Stack: Microsoft Excel, Microsoft Forms, Microsoft SharePoint, Pivot Tables\n",
      "   Chunks: 2 (technical + full)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View all projects in the collection\n",
    "all_projects, _ = client.scroll(\n",
    "    collection_name=projects_collection,\n",
    "    limit=100,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“‚ Projects Collection Overview\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Total chunks in collection: {len(all_projects)}\")\n",
    "\n",
    "# Group by project_id\n",
    "from collections import defaultdict\n",
    "projects_by_id = defaultdict(list)\n",
    "for record in all_projects:\n",
    "    project_id = record.payload.get('metadata', {}).get('project_id')\n",
    "    projects_by_id[project_id].append(record)\n",
    "\n",
    "print(f\"Total unique projects: {len(projects_by_id)}\")\n",
    "print(f\"\\nğŸ’¡ Each project has 2 chunks: technical_summary + full_content\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Display each project's chunks\n",
    "for project_id, chunks in sorted(projects_by_id.items()):\n",
    "    tech_chunk = next((c for c in chunks if c.payload.get('section_type') == 'project_technical'), None)\n",
    "    full_chunk = next((c for c in chunks if c.payload.get('section_type') == 'project_full'), None)\n",
    "    \n",
    "    if tech_chunk:\n",
    "        metadata = tech_chunk.payload.get('metadata', {})\n",
    "        print(f\"ğŸ“¦ Project: {metadata.get('project_title', 'N/A')}\")\n",
    "        print(f\"   ID: {project_id}\")\n",
    "        print(f\"   URL: {metadata.get('project_url', 'N/A')}\")\n",
    "        print(f\"   Tech Stack: {', '.join(metadata.get('tech_stack', []))}\")\n",
    "        print(f\"   Chunks: {len(chunks)} (technical + full)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1499a58",
   "metadata": {},
   "source": [
    "### Hierarchical Chunking Strategy\n",
    "\n",
    "The `projects` collection uses a **hierarchical chunking strategy** with two chunk types per project:\n",
    "\n",
    "1. **`project_technical`**: Tech Stack + Technical Highlights (for filtering/matching)\n",
    "2. **`project_full`**: Complete project with all sections (Purpose, Tech Stack, Technical Highlights, Skills Demonstrated, Result/Impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f9e2eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Semantic Search Query: 'Python data analysis ETL pipeline machine learning'\n",
      "   Query vector dimensions: 1536\n",
      "   Searching in: resume_data collection\n",
      "\n",
      "ğŸ“Š Top 4 Results (by semantic similarity):\n",
      "\n",
      "================================================================================\n",
      "RESULT 1 - Similarity Score: 0.6346\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Analyst: Built an ETL pipeline integrating five data sources totaling over 1M records using SQL and Python, automating ingestion and cleaning and saving 8 hours weekly in data preparation.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Rubicon Organics\n",
      "   Position: Data Analyst\n",
      "\n",
      "================================================================================\n",
      "RESULT 2 - Similarity Score: 0.5505\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Scientist II: Extracted and processed millions of import/export transactions by building web-scraping collectors and a PySpark ETL pipeline to load cleaned data into a Microsoft Fabric lakehouse.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Canadian Food Inspection Agency\n",
      "   Position: Data Scientist II\n",
      "\n",
      "================================================================================\n",
      "RESULT 3 - Similarity Score: 0.5154\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Scientist II: Automated data categorization, reducing data cleaning time by over 15 hours per week, by integrating LLM-based classification into the ETL pipeline.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Canadian Food Inspection Agency\n",
      "   Position: Data Scientist II\n",
      "\n",
      "================================================================================\n",
      "RESULT 4 - Similarity Score: 0.5121\n",
      "================================================================================\n",
      "ğŸ“„ Content: Data Scientist II: Implemented daily automated data refreshes, replacing weekly manual CSV exports, by gaining direct data-warehouse access and building an ETL pipeline using SQL and Microsoft Fabric to store data in a Lakehouse.\n",
      "ğŸ·ï¸  Section Type: work_experience\n",
      "   Company: Canadian Food Inspection Agency\n",
      "   Position: Data Scientist II\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a query for Python-related achievements\n",
    "query_text = \"Python data analysis ETL pipeline machine learning\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "print(f\"ğŸ” Semantic Search Query: '{query_text}'\")\n",
    "print(f\"   Query vector dimensions: {len(query_vector)}\")\n",
    "print(f\"   Searching in: {resume_collection} collection\")\n",
    "\n",
    "# Search with vector similarity using query_points (newer API)\n",
    "results = client.query_points(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    query=query_vector,\n",
    "    limit=5,\n",
    "    score_threshold=0.5  # Only return results with similarity > 0.5\n",
    ").points\n",
    "\n",
    "print(f\"\\nğŸ“Š Top {len(results)} Results (by semantic similarity):\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"RESULT {i} - Similarity Score: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“„ Content: {payload.get('content', 'N/A')}\")\n",
    "    print(f\"ğŸ·ï¸  Section Type: {payload.get('section_type', 'N/A')}\")\n",
    "    if payload.get('section_type') == 'work_experience':\n",
    "        print(f\"   Company: {metadata.get('company', 'N/A')}\")\n",
    "        print(f\"   Position: {metadata.get('position', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe440754",
   "metadata": {},
   "source": [
    "### Search for Personality Traits Matching Job Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114100a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Job Requirements Query: 'analytical thinking problem-solving collaboration strategic innovative team player'\n",
      "   Searching in: personality collection (NEW!)\n",
      "\n",
      "âœ… Retrieved 8 results from personality collection\n",
      "\n",
      "ğŸ“Š Top 5 Personality Trait Chunks by Semantic Similarity:\n",
      "\n",
      "ğŸ’¡ Benefits of simplified chunking:\n",
      "   - Pure semantic search without complex filtering\n",
      "   - Fixed-size chunks maintain consistent context windows\n",
      "   - Faster search (smaller collection)\n",
      "\n",
      "================================================================================\n",
      "TRAIT 1 - Similarity: 0.5586\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 0\n",
      "ğŸ“ Characters: 0 - 400\n",
      "ğŸ“„ Content:\n",
      "   # Personality Traits\n",
      "I'm very analytical, highly curious and constantly seek to improve systems. I approach life with a strategic mindset, always looking several steps ahead and planning for contingencies. I value autonomy but also enjoy collaborating with others. This allows me to tackle complex problems with confidence and innovation. I hold high standards for myself and others, always striving\n",
      "\n",
      "================================================================================\n",
      "TRAIT 2 - Similarity: 0.4968\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 3\n",
      "ğŸ“ Characters: 900 - 1300\n",
      "ğŸ“„ Content:\n",
      "   also attending to crucial details, which makes me a valuable asset in any organization.\n",
      "\n",
      "## Strengths\n",
      "### Innovative Mindset\n",
      "My ability to see possibilities others overlook often helps me find smarter solutions and effective improvements at work.\n",
      "\n",
      "### Independent Worker\n",
      "My talent for working productively on my own allows me to manage tasks effectively without the need for constant direction or sup\n",
      "\n",
      "================================================================================\n",
      "TRAIT 3 - Similarity: 0.4910\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 4\n",
      "ğŸ“ Characters: 1200 - 1600\n",
      "ğŸ“„ Content:\n",
      "   ively on my own allows me to manage tasks effectively without the need for constant direction or supervision.\n",
      "\n",
      "### Conceptual Thinking\n",
      "I effortlessly grasp abstract, complex ideas, making me particularly suited to roles that require strategic analysis and long-term planning.\n",
      "\n",
      "### Continuous Improvement\n",
      "I naturally focus on refining work processes and spotting inefficiencies, consistently improving\n",
      "\n",
      "================================================================================\n",
      "TRAIT 4 - Similarity: 0.4732\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 2\n",
      "ğŸ“ Characters: 600 - 1000\n",
      "ğŸ“„ Content:\n",
      "   rences\n",
      "I thrive in environments that challenge me intellectually and provide opportunities for growth. I seek roles where I can implement innovative ideas and apply strategic thinking and problem-solvingâ€”often in fields like science, technology, or business strategy. I can see the big picture while also attending to crucial details, which makes me a valuable asset in any organization.\n",
      "\n",
      "## Strength\n",
      "\n",
      "================================================================================\n",
      "TRAIT 5 - Similarity: 0.4607\n",
      "================================================================================\n",
      "ğŸ“ Chunk Index: 5\n",
      "ğŸ“ Characters: 1500 - 1900\n",
      "ğŸ“„ Content:\n",
      "   ent\n",
      "I naturally focus on refining work processes and spotting inefficiencies, consistently improving project outcomes wherever I go.\n",
      "\n",
      "### Objective Judgment\n",
      "My capacity to make impartial decisions based on facts rather than favoritism or personal bias earns respect and trust from my colleagues.\n",
      "\n",
      "### Reliable Performance\n",
      "When entrusted with critical tasks, I consistently deliver precise, high-quali\n",
      "\n",
      "\n",
      "ğŸ’¡ These traits would be deduplicated (removing 100-char overlaps) and injected into the cover letter prompt!\n"
     ]
    }
   ],
   "source": [
    "# Simulate a job analysis with soft skills and keywords\n",
    "job_analysis = {\n",
    "    'soft_skills': ['analytical thinking', 'problem-solving', 'collaboration'],\n",
    "    'keywords': ['strategic', 'innovative', 'team player']\n",
    "}\n",
    "\n",
    "# Build query (same logic as retrieve_personality_traits)\n",
    "query_parts = job_analysis.get('soft_skills', []) + job_analysis.get('keywords', [])\n",
    "query_text = ' '.join(query_parts)\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "print(f\"ğŸ” Job Requirements Query: '{query_text}'\")\n",
    "print(f\"   Searching in: {personality_collection} collection (NEW!)\\n\")\n",
    "\n",
    "# Search the PERSONALITY collection directly (no filtering needed!)\n",
    "all_results = client.query_points(\n",
    "    collection_name=personality_collection,  # â† Query personality collection directly!\n",
    "    query=query_vector,\n",
    "    limit=10\n",
    ").points\n",
    "\n",
    "print(f\"âœ… Retrieved {len(all_results)} results from personality collection\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 5 Personality Trait Chunks by Semantic Similarity:\\n\")\n",
    "print(f\"ğŸ’¡ Benefits of simplified chunking:\")\n",
    "print(f\"   - Pure semantic search without complex filtering\")\n",
    "print(f\"   - Fixed-size chunks maintain consistent context windows\")\n",
    "print(f\"   - Faster search (smaller collection)\\n\")\n",
    "\n",
    "for i, result in enumerate(all_results[:5], 1):  # Top 5\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAIT {i} - Similarity: {result.score:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“ Chunk Index: {metadata.get('chunk_index', 'N/A')}\")\n",
    "    print(f\"ğŸ“ Characters: {metadata.get('char_start', 'N/A')} - {metadata.get('char_end', 'N/A')}\")\n",
    "    print(f\"ğŸ“„ Content:\\n   {payload.get('content', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nğŸ’¡ These traits would be deduplicated (removing 100-char overlaps) and injected into the cover letter prompt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b67c13",
   "metadata": {},
   "source": [
    "### Semantic Search with Section Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "096b45de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: 'data science machine learning SQL Python dashboard visualization'\n",
      "ğŸ“¦ Collection: resume_data\n",
      "ğŸ¯ Filter: section_type = 'work_experience'\n",
      "\n",
      "ğŸ“Š Top 5 Work Achievements:\n",
      "\n",
      "1. [Score: 0.5813] Canadian Food Inspection Agency - Data Scientist\n",
      "   Data Scientist: Standardized descriptive and statistical reporting in Power BI, reducing report-gene...\n",
      "\n",
      "2. [Score: 0.5107] Rubicon Organics - Data Analyst\n",
      "   Data Analyst: Built an ETL pipeline integrating five data sources totaling over 1M records using SQL...\n",
      "\n",
      "3. [Score: 0.5098] Rubicon Organics - Data Analyst\n",
      "   Data Analyst: Built three Power BI dashboards for sales and marketing by collaborating with stakehol...\n",
      "\n",
      "4. [Score: 0.4946] Canadian Food Inspection Agency - Data Scientist II\n",
      "   Data Scientist II: Automated forecasting and reduced manual effort by 40 hours per month by deployin...\n",
      "\n",
      "5. [Score: 0.4918] Canadian Food Inspection Agency - Data Scientist II\n",
      "   Data Scientist II: Implemented daily automated data refreshes, replacing weekly manual CSV exports, ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for data science achievements ONLY in work experience (resume_data collection)\n",
    "query_text = \"data science machine learning SQL Python dashboard visualization\"\n",
    "query_vector = embedder.embed_query(query_text)\n",
    "\n",
    "# Apply filter to only search work_experience\n",
    "work_filter = Filter(\n",
    "    must=[FieldCondition(key=\"section_type\", match=MatchValue(value=\"work_experience\"))]\n",
    ")\n",
    "\n",
    "results = client.query_points(\n",
    "    collection_name=resume_collection,  # â† Query resume_data collection\n",
    "    query=query_vector,\n",
    "    query_filter=work_filter,  # â† Apply filter during search\n",
    "    limit=5\n",
    ").points\n",
    "\n",
    "print(f\"ğŸ” Query: '{query_text}'\")\n",
    "print(f\"ğŸ“¦ Collection: {resume_collection}\")\n",
    "print(f\"ğŸ¯ Filter: section_type = 'work_experience'\")\n",
    "print(f\"\\nğŸ“Š Top {len(results)} Work Achievements:\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    payload = result.payload\n",
    "    metadata = payload.get('metadata', {})\n",
    "    \n",
    "    print(f\"{i}. [Score: {result.score:.4f}] {metadata.get('company', 'N/A')} - {metadata.get('position', 'N/A')}\")\n",
    "    print(f\"   {payload.get('content', 'N/A')[:100]}...\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Resume_Claude_SDK_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
